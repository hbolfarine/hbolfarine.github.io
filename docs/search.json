[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Henrique Bolfarine",
    "section": "",
    "text": "Welcome to my homepage! I’m a lecturer of statistics and data science at the McCombs School of Business at the University of Texas at Austin. I obtained my Ph.D. in Statistics at the Institute of Mathematics and Statistics of the University of São Paulo, advised by Professors Hedibert Lopes and Carlos Carvalho, and worked as a postdoctoral researcher at the McCombs School of Business, advised by Professor Carlos Carvalho."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Research",
    "section": "",
    "text": "“Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models.” H. Bolfarine, H.F. Lopes, & C.M. Carvalho. Working Paper"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Henrique Bolfarine",
    "section": "Education",
    "text": "Education\nUniversidade de São Paulo (USP), São Paulo\nPhD in Statistics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Henrique Bolfarine",
    "section": "",
    "text": "Welcome to my homepage! I’m a lecturer of statistics and data science at the McCombs School of Business at the University of Texas at Austin. I obtained my Ph.D. in Statistics at the Institute of Mathematics and Statistics of the University of São Paulo, advised by Professors Hedibert Lopes and Carlos Carvalho, and worked as a postdoctoral researcher at the McCombs School of Business, advised by Professor Carlos Carvalho."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "STA 235 - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables\nclass 03 - Interactions\nclass 04 - Regression Assumptions\nclass 05 - Class 05 - Modeling nonlinear relationships\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 Honors - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables and Interactions\nclass 03 - Regression assumptions and Outliers\nclass 04 - Modeling nonlinear relationships\nclass 05 - Time Series Decomposition and Autoregression\nclass 06 - Model Selection\nclass 07 - Logistic Regression\nclass 08 - Basic Causal Inference\nclass 09 - Randomized Control Trials\nclass 10 - Natural Experiments - Diff-in-Diff\nclass 11 - Natural Experiments - RDD\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Application\n\n\n\n\n\nSTA 235 - Data Science for Business Application"
  },
  {
    "objectID": "teaching.html#teaching",
    "href": "teaching.html#teaching",
    "title": "Teaching",
    "section": "",
    "text": "Modifcation"
  },
  {
    "objectID": "class_01.html#what-is-data-science",
    "href": "class_01.html#what-is-data-science",
    "title": "Data Science for Business Applications",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\nIn Data Science we use the available data to obtain:"
  },
  {
    "objectID": "class_01.html#data-science-tasks",
    "href": "class_01.html#data-science-tasks",
    "title": "Data Science for Business Applications",
    "section": "Data Science tasks",
    "text": "Data Science tasks\n\nDescription: Can we classify our customers into different segments? (simple task)\nPrediction: What is the probability of a shopper coming back to our website? (kind of a simple task)\nCausal Inference: What is the effect of increasing our advertising budget on our total revenue? (difficult task)"
  },
  {
    "objectID": "class_01.html#simple-and-multiple-regression",
    "href": "class_01.html#simple-and-multiple-regression",
    "title": "Data Science for Business Applications",
    "section": "Simple and Multiple Regression",
    "text": "Simple and Multiple Regression\n\nLinear Regression will be the most important tool for solving these Data Science tasks.\nBasically, in the linear regression model, we are explaining the relation between different variables by a line (that’s where the linear comes from.)\nMany fancy methods are generalizations or extensions of Linear Regression!\nIn this class, we will do a quick review on linear regression."
  },
  {
    "objectID": "class_01.html#cookie-example",
    "href": "class_01.html#cookie-example",
    "title": "Data Science for Business Applications",
    "section": "Cookie Example",
    "text": "Cookie Example\n\n\n\nSuppose we have some data, and we want to understand how happiness changes in relation to the number of cookies eaten.\nTo do so, we summarize the relation between these two variables through a line.\nThis line is the linear regression line.\nLet’s see how this works!\n\n\n\n\n\ncookies\nhappiness\n\n\n\n\n1\n0.1\n\n\n2\n2\n\n\n3\n1\n\n\n4\n2.5\n\n\n5\n3\n\n\n6\n1.3\n\n\n7\n1.9\n\n\n8\n2.4\n\n\n9\n1.8\n\n\n10\n3"
  },
  {
    "objectID": "class_01.html#cookie-example-1",
    "href": "class_01.html#cookie-example-1",
    "title": "Data Science for Business Applications",
    "section": "Cookie Example",
    "text": "Cookie Example\n\n\n\nThe regression line is defined by two values, the intercept and the slope.\nThe intercept is where the line intercepts the happiness axis.\nThe slope relates to the inclination of the line.\nIf the slope is positive the line has a upward direction.\nIf the slope is negative the line has a downward direction.\nThe regression line is a model of the relation between happiness and the number of cookies eaten."
  },
  {
    "objectID": "class_01.html#some-questions-on-regression",
    "href": "class_01.html#some-questions-on-regression",
    "title": "Data Science for Business Applications",
    "section": "Some questions on regression",
    "text": "Some questions on regression\n\nFrom the regression line, what is the relationship between happiness and the number of cookies eaten?\nAre there other factors other than the number of cookies that might affect the happiness level?\nFrom this model, can we conclude that eating cookies alone causes happiness?\nWhy not look only at the correlation between happiness and the number of cookies?\nHow do we obtain the intercept and the slope?\nDoes this result apply to the entire population?"
  },
  {
    "objectID": "class_01.html#regressions-details",
    "href": "class_01.html#regressions-details",
    "title": "Data Science for Business Applications",
    "section": "Regressions Details",
    "text": "Regressions Details\n\nThe Linear Regression model is represented by the formula: \\[\nY = \\beta_0 + \\beta_1\\cdot X_1 + \\beta_2\\cdot X_2 + e\n\\]\nMultiple Regression means we have two or more \\(X\\)’s.\nLet’s break down this model into its essential parts."
  },
  {
    "objectID": "class_01.html#essential-parts-of-a-regression",
    "href": "class_01.html#essential-parts-of-a-regression",
    "title": "Data Science for Business Applications",
    "section": "Essential Parts of a Regression",
    "text": "Essential Parts of a Regression\n\n\\(Y\\) - Outcome Variable, Response Variable, Dependent Variable (Thing you want to explain or predict)\n\\(X\\) - Explanatory Variable, Predictor Variable, Independent Variable, Covariate (Thing you use to explain or predict \\(Y\\))\n\\(\\beta\\)’s - Coefficients, Parameters (How \\(Y\\) changes numerically in relation to \\(X\\))\n\\(e\\) - Residual, Noise (Things we didn’t account in our model)"
  },
  {
    "objectID": "class_01.html#two-purposes-of-regression",
    "href": "class_01.html#two-purposes-of-regression",
    "title": "Data Science for Business Applications",
    "section": "Two Purposes of Regression",
    "text": "Two Purposes of Regression"
  },
  {
    "objectID": "class_01.html#back-to-the-cookie-example",
    "href": "class_01.html#back-to-the-cookie-example",
    "title": "Data Science for Business Applications",
    "section": "Back to the Cookie example",
    "text": "Back to the Cookie example\n\nBy writing the cookie example as a model where the variable happiness is the response variable and the number of cookies is the predictor, we have \\[\n\\texttt{happiness} = \\beta_0 + \\beta_1\\cdot\\texttt{cookies} + e\n\\]\nhappiness is the response variable (\\(Y\\)).\ncookies is the predictor variable (\\(X\\)).\n\\(\\beta_0\\) is the intercept.\n\\(\\beta_1\\) is the slope associate to the cookies variable.\n\\(e\\) are the unknown factors that might explain the relation between cookies and happiness.\nThe challenge now is to estimate the parameters \\(\\beta_0\\), and \\(\\beta_1\\)."
  },
  {
    "objectID": "class_01.html#how-do-we-estimate-the-coefficients-in-a-regression",
    "href": "class_01.html#how-do-we-estimate-the-coefficients-in-a-regression",
    "title": "Data Science for Business Applications",
    "section": "How do we estimate the coefficients in a regression?",
    "text": "How do we estimate the coefficients in a regression?\n\nA very useful strategy is use what is called the Ordinary Least Squares (OLS).\nThe method is called ordinary least squares because the algorithm selects the coefficients (\\(\\beta\\)’s) that minimize the sum of the squares of the errors in our sample.\nSo the data in this case is fundamental.\nWe use R to learn \\(\\beta\\).\nThe estimated coefficients are referred to as \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "class_01.html#lets-get-into-some-data",
    "href": "class_01.html#lets-get-into-some-data",
    "title": "Data Science for Business Applications",
    "section": "Let’s get into some data",
    "text": "Let’s get into some data\n\nExample: Movie data Set (movie_1990_data.csv)\nWe will create a model that explains and predicts the movie revenue in terms of the budget.\nThere are 1,368 different movies in the data, with 22 different attributes.\nThis means that the data contains 1,368 lines and 22 columns.\nWe are interested in two attributes, the movie budget, and movie revenue.\nMovie budget is in the predictor variable - Adj_Budget.\nMovie revenue is in the response variable - Adj_Revenue.\nThe units in this case are important (both are in millions of dollars).\nLet’s visualize the relation between these two variables."
  },
  {
    "objectID": "class_01.html#section",
    "href": "class_01.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "First we load the library tidyverse, then we use the ggplot function to make plot between Adj_Budget and Adj_Revenue\n\n\n# Load library\nlibrary(tidyverse)\n\n# Create plot\nggplot(movie_1990_data) +\n  geom_point(aes(x = Adj_Budget, y = Adj_Revenue))"
  },
  {
    "objectID": "class_01.html#section-1",
    "href": "class_01.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We encode the model below in R.\n\\[\n\\texttt{Adj_Revenue} = \\beta_0 + \\beta_1\\cdot\\texttt{Adj_Budget} + e\n\\]\n\n# The model\n# Revenue = intercept + slope*Budget + e\nlm1 &lt;- lm(Adj_Revenue ~ Adj_Budget, data = movie_1990_data)\nsummary(lm1)\n\n\nCall:\nlm(formula = Adj_Revenue ~ Adj_Budget, data = movie_1990_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-262.40  -38.01  -16.39   19.24  619.23 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 22.66095    3.15001   7.194 1.03e-12 ***\nAdj_Budget   1.11043    0.03738  29.709  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 79.78 on 1366 degrees of freedom\nMultiple R-squared:  0.3925,    Adjusted R-squared:  0.3921 \nF-statistic: 882.6 on 1 and 1366 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_01.html#lets-interpret-the-output",
    "href": "class_01.html#lets-interpret-the-output",
    "title": "Data Science for Business Applications",
    "section": "Let’s interpret the output",
    "text": "Let’s interpret the output\n\nFrom the coefficients, \\(\\widehat\\beta_0 = 22.7\\), \\(\\widehat\\beta_1 = 1.11\\) we have the updated model: \\[\n\\widehat{\\texttt{Adj_Revenue}} = 22.7 + 1.11\\cdot\\texttt{Adj_Budget}\n\\]\nWhen there’s a hat, it means that the values were generated from the data.\n\\(e\\) (noise) vanishes because we eliminated the unknown factors and concentrated the effect on what we observe.\nNow we have the residuals, that is the distance between the points in the data and the regression model.\nThe question now is: how can we interpret this result?"
  },
  {
    "objectID": "class_01.html#explanation",
    "href": "class_01.html#explanation",
    "title": "Data Science for Business Applications",
    "section": "Explanation",
    "text": "Explanation\n\nIntercept: By setting the movie budget to zero, we have that the average revenue is equal to $22.7 million dollars.\nSlope: For one unit change in the movie budget, that is, millions, there will be an increase of $1.11 million dollars in the movie’s revenue."
  },
  {
    "objectID": "class_01.html#section-2",
    "href": "class_01.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Let’s visualize this model.\n\n# Create plot with regression line\nggplot(movie_1990_data) +\n  geom_point(aes(x = Adj_Budget, y = Adj_Revenue)) +\n  geom_smooth(aes(x = Adj_Budget, y = Adj_Revenue), method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model",
    "href": "class_01.html#statistical-significance-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nAre these coefficients statistically significant?\nCan we extrapolate the results to the larger population?\nWe can answer these questions by looking at the confidence interval (CI).\n\n\n# We use the confint() function to get the confidence interval\nconfint(lm1)\n\n                2.5 %    97.5 %\n(Intercept) 16.481572 28.840332\nAdj_Budget   1.037112  1.183756"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-1",
    "href": "class_01.html#statistical-significance-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\nFrom the confidence interval we have that:\n\n\n\nThe value of the intercept is statistically different from zero since zero is not between the lower and upper values of the interval .\nThe value of the slope is statistically different from zero since zero is not between the lower and upper values of the interval .\n\n\n\nWith 95% confidence, the value of the intercept at the population level is between 16.5 and 28.8 million dollars .\nWith 95% confidence, the value of the slope at the population level is between 1.04 and 28.8 million dollars ."
  },
  {
    "objectID": "class_01.html#predictions",
    "href": "class_01.html#predictions",
    "title": "Data Science for Business Applications",
    "section": "Predictions",
    "text": "Predictions\n\nSuppose we want to predict the revenue of a movie , knowing that the revenue was $25 million.\nWe can use our estimated model to make the prediction.\nInput the value into the adjusted budget, resulting in: \\[\n\\widehat{\\texttt{Adj_Revenue}} = 22.7 + 1.11\\cdot 25 = 50.45\n\\]\nThe average predicted revenue of a $25 million dollar budget movie is $50 million dollars.\nWe can also do this using the predict function in R\n\n\n# We use the predict() function to get predictions\npredict(lm1, list(Adj_Budget = 25))\n\n       1 \n50.42179"
  },
  {
    "objectID": "class_01.html#predictions-1",
    "href": "class_01.html#predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Predictions",
    "text": "Predictions\n\nHow good are these predictions?\nWe can use the residual standard error (RSE), which is a result of the summary function.\n\\(\\texttt{Residual standard error: 79.78}\\),\nThese mean that our predictions will be off by approximately 79.78.\nWith 95% confidence, the prediction of the revenue will be 50.42 plus and minus \\(2\\times 79.78 = 159.56\\).\nQuite a big variation."
  },
  {
    "objectID": "class_01.html#adding-more-variables",
    "href": "class_01.html#adding-more-variables",
    "title": "Data Science for Business Applications",
    "section": "Adding more variables",
    "text": "Adding more variables\n\nWe can add more variables in our model.\nWe will add the variable imdbRating which encodes the different IMDB ratings in the data (1-10).\nThe resulting model now is \\[\n\\texttt{Adj_Revenue} = \\beta_0 + \\beta_1\\cdot\\texttt{Adj_Budget} + \\beta_2\\cdot\\texttt{imdbRating} + e\n\\]\nYou can observe that we have an extra slope in the equation.\nThis will have an impact in how we interpret the model.\nNext we encode this model in R"
  },
  {
    "objectID": "class_01.html#explore-the-data-beauty",
    "href": "class_01.html#explore-the-data-beauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: beauty",
    "text": "Explore the data: beauty"
  },
  {
    "objectID": "class_01.html#relation-bewteen-variables",
    "href": "class_01.html#relation-bewteen-variables",
    "title": "Data Science for Business Applications",
    "section": "Relation bewteen variables",
    "text": "Relation bewteen variables\n\n\nLet’s interpret the model:\n\n\n\\[\n\\texttt{Adj_Revenue} = -136.507 + 1.091 \\cdot\\texttt{Adj_Budget} + 24.099\\cdot\\texttt{imdbRating}\n\\]"
  },
  {
    "objectID": "class_01.html#correlation",
    "href": "class_01.html#correlation",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\nRegressions are super useful…\nBut you need to know how to interpret them.\nBe sure not to overstate your claims!\nRemember the magic words for interpretation"
  },
  {
    "objectID": "class_01.html#correlation-1",
    "href": "class_01.html#correlation-1",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "class_01.html#correlation-between-variables",
    "href": "class_01.html#correlation-between-variables",
    "title": "Data Science for Business Applications",
    "section": "Correlation between variables",
    "text": "Correlation between variables\n\nLet’s calculate the correlation between evaluation and beauty\n\n\ncor(profs$eval, profs$beauty)\n\n[1] 0.1890391\n\n\n\nHow can we interpret this?\nInstead of trying to interpret the correlation, we can build a model that reveals the relationship between the professor’s evaluation and their beauty score."
  },
  {
    "objectID": "class_01.html#simple-regression-model",
    "href": "class_01.html#simple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Simple regression model",
    "text": "Simple regression model\n\nmodel &lt;- lm(eval ~ beauty, data=profs)\nsummary(model)\n\n\nCall:\nlm(formula = eval ~ beauty, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.99827    0.02535 157.727  &lt; 2e-16 ***\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05"
  },
  {
    "objectID": "class_01.html#interpreting-the-model",
    "href": "class_01.html#interpreting-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\neval is the response variable (\\(Y\\));\nbeauty is the predicton variable (\\(X\\))\nSimple regression uses the best fit line to give us a linear equation to predict \\(Y\\) from \\(X\\):\n\n\\[\n\\text{eval} = 3.998 + 0.133\\cdot \\text{beauty}\n\\]\n\nWe can predict the evaluation score for someone based on their beauty score just by plugging into the equation.\nWhat do the coefficients mean?"
  },
  {
    "objectID": "class_01.html#interpreting-the-model-1",
    "href": "class_01.html#interpreting-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model"
  },
  {
    "objectID": "class_01.html#interpreting-the-model-2",
    "href": "class_01.html#interpreting-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\nWe can predict the evaluation score for someone based on their beauty score just by plugging into the equation.\nWhat do the coefficients mean?\nBut what does the population that it was drawn from look like?"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-2",
    "href": "class_01.html#statistical-significance-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nLet’s analyse the confidence interval\n\n\n# We use the confint() function to get the confidence interval\nconfint(lm2)\n\n                  2.5 %      97.5 %\n(Intercept) -165.245169 -107.768758\nAdj_Budget     1.020706    1.161364\nimdbRating    19.841475   28.357234\n\n\n\nAll coefficients are statistically significant."
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-3",
    "href": "class_01.html#statistical-significance-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nThe population regression line (the best fit line in the population) is \\[Y = \\beta_0 + \\beta_1 X + \\text{noise}\\]\nUsually we don’t have access to the entire population, so we can’t know this\nThe noise term represents what we haven’t accounted for in our model"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-4",
    "href": "class_01.html#statistical-significance-of-the-model-4",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nOur regression equation is the best fit line in the sample, or \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\]\nThis is what we get from our sample data\nThe sample intercept and slope \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\)\nare our best estimates for the population intercept\nand slope \\(\\beta_0\\) and \\(\\beta_1\\)\nBut we need to get a sense of how close \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\) are to \\(\\beta_0\\) and \\(\\beta_1\\)!"
  },
  {
    "objectID": "class_01.html#confidence-intervals",
    "href": "class_01.html#confidence-intervals",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nLet’s form confidence intervals for the slope and intercept to get a sense of the uncertainty in our estimates:\n\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) 3.94845765 4.0480866\nbeauty      0.06976869 0.1962342"
  },
  {
    "objectID": "class_01.html#confidence-intervals-1",
    "href": "class_01.html#confidence-intervals-1",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\\(\\beta_1\\): We are 95% confident that the incremental impact of each additional beauty point is between \\(0.07\\) and \\(0.20\\) student evaluation points.\n\\(\\beta_0\\): We are 95% confident that the average student evaluation score for average-looking professors (beauty = 0) is between \\(3.95\\) and \\(4.05\\)"
  },
  {
    "objectID": "class_01.html#confidence-intervals-for-predictions",
    "href": "class_01.html#confidence-intervals-for-predictions",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\nConfidence for a given prediction\n\n\npredict(model, list(beauty=1), interval=\"prediction\")\n\n       fit      lwr      upr\n1 4.131274 3.056375 5.206172\n\n\n\nWe are 95% confident that a single professor with a beauty score of 1 will get rated between 3.06 and 5.21."
  },
  {
    "objectID": "class_01.html#confidence-intervals-for-predictions-1",
    "href": "class_01.html#confidence-intervals-for-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\nConfidence for the average prediction\n\n\npredict(model, list(beauty=1), interval=\"confidence\")\n\n       fit      lwr      upr\n1 4.131274 4.050776 4.211771\n\n\n\nWe are 95% confident that the will be between\n4.05 and 4.21"
  },
  {
    "objectID": "class_01.html#residuals",
    "href": "class_01.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nEach instructor has a residual: the difference between their actual and predicted scores (the prediction error)."
  },
  {
    "objectID": "class_01.html#residuals-1",
    "href": "class_01.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nThis instructor has a positive residual—their evaluation score was higher than expected given their beauty:"
  },
  {
    "objectID": "class_01.html#residuals-2",
    "href": "class_01.html#residuals-2",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nThis instructor has a negative residual—their evaluation score was lower than expected given their beauty:"
  },
  {
    "objectID": "class_01.html#residuals-3",
    "href": "class_01.html#residuals-3",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nThe residuals are approximately Normally distributed with a mean of 0:"
  },
  {
    "objectID": "class_01.html#practical-significance-of-the-model",
    "href": "class_01.html#practical-significance-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Practical significance of the model",
    "text": "Practical significance of the model\n\nWe know that there is a statistically significant relationship between beauty and eval, so we can be highly confident they are indeed related in the larger population. But it that relationship meaningful?\nThe of \\(0.545\\) tells us that the standard deviation of the residuals is about \\(0.545\\) points"
  },
  {
    "objectID": "class_01.html#practical-significance-of-the-model-1",
    "href": "class_01.html#practical-significance-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Practical significance of the model",
    "text": "Practical significance of the model\n\nThat means that 95% of the residuals are less than \\(2\\cdot 0.545=1.09\\) (since 95% of a Normal distribution is within \\(\\pm 2\\) SD of the mean)\nIn other words, 95% of the time when using beauty to predict evaluation scores, we’ll be off by\nless than 1.09 point"
  },
  {
    "objectID": "class_01.html#practical-significance-of-the-model-2",
    "href": "class_01.html#practical-significance-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Practical significance of the model",
    "text": "Practical significance of the model\n\nThe \\(R^2\\) of \\(0.0357\\) tells us that about 4% of the variation between professors in student evaluations can be explained by their beauty\nLook at the difference in \\(\\widehat{\\text{eval}}\\) between someone super-hot (beauty = 2) and super-not (beauty = \\(-1.5\\)); the difference in their predicted student evaluations is \\(3.5 \\cdot 0.133 = 0.466\\)\nWe have to use our subjective judgment to decide whether these indicate a meaningful (practically significant) relationship"
  },
  {
    "objectID": "class_01.html#adding-more-variables-1",
    "href": "class_01.html#adding-more-variables-1",
    "title": "Data Science for Business Applications",
    "section": "Adding more variables",
    "text": "Adding more variables\n\n# The model\n# Revenue = intercept + slope*Budget + slope*Rating + e\nlm2 &lt;- lm(Adj_Revenue ~ Adj_Budget + imdbRating, data = movie_1990_data)\nsummary(lm2)\n\n\nCall:\nlm(formula = Adj_Revenue ~ Adj_Budget + imdbRating, data = movie_1990_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-256.79  -41.25  -14.97   26.55  598.53 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -136.50696   14.64962  -9.318   &lt;2e-16 ***\nAdj_Budget     1.09103    0.03585  30.433   &lt;2e-16 ***\nimdbRating    24.09935    2.17050  11.103   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.43 on 1365 degrees of freedom\nMultiple R-squared:  0.4428,    Adjusted R-squared:  0.442 \nF-statistic: 542.5 on 2 and 1365 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_01.html#visualizing-the-model",
    "href": "class_01.html#visualizing-the-model",
    "title": "Data Science for Business Applications",
    "section": "Visualizing the model",
    "text": "Visualizing the model"
  },
  {
    "objectID": "class_01.html#explanation-1",
    "href": "class_01.html#explanation-1",
    "title": "Data Science for Business Applications",
    "section": "Explanation",
    "text": "Explanation\n\n\nLet’s interpret the model:\n\n\n\\[\n\\texttt{Adj_Revenue} = -136.507 + 1.091 \\cdot\\texttt{Adj_Budget} + 24.099\\cdot\\texttt{imdbRating}\n\\]\n\nIntercept: By setting the movie budget and the rating to zero, we have that the average revenue is equal to $-137 million dollars.\nSlope Budget: For movies with the same fixed rating, for one unit change in the movie budget, there will be an increase of $1.091 million dollars in the movie’s revenue.\nSlope Rating: For movies with the same fixed budget, for one unit change in the movie rating, there will be an increase of $24.1 million dollars in the movie’s revenue."
  },
  {
    "objectID": "class_01.html#what-about-the-predictions",
    "href": "class_01.html#what-about-the-predictions",
    "title": "Data Science for Business Applications",
    "section": "What about the predictions?",
    "text": "What about the predictions?\n\nWe can observe from the summary that the residual standard error when down to \\(\\texttt{76.43}\\).\nWhich will result in more accurate predictions.\nSuppose we have a movie 25 million dollar budget and the movie has a IMDb rating of 5.4 what is the prediction for this movie?\nWhat about the 95% prediction interval for this prediction?\n\n\n# We use the confint() function to get the confidence interval\npredict(lm2, list(Adj_Budget = 25, imdbRating = 5.4))\n\n       1 \n20.90542 \n\n\n\nWhat about the 95% prediction interval for this prediction?\nupper bound: \\(20.90542 + 2\\times 76.43 = 173.7654\\)\nlower bound: \\(20.90542 - 2\\times 76.43 = -131.9546\\)"
  },
  {
    "objectID": "class_01.html#whats-next",
    "href": "class_01.html#whats-next",
    "title": "Data Science for Business Applications",
    "section": "What’s next?",
    "text": "What’s next?\n\nWe’ll include categorical variables.\nInteractions between categorical and numerical variables."
  },
  {
    "objectID": "class_02.html",
    "href": "class_02.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Recall what are the goals of the linear regression model"
  },
  {
    "objectID": "class_02.html#regresison-model-goals",
    "href": "class_02.html#regresison-model-goals",
    "title": "Data Science for Business Applications",
    "section": "Regresison model goals",
    "text": "Regresison model goals\n\n\nRecall what are the goals of the linear regression model"
  },
  {
    "objectID": "class_02.html#data-science-tasks",
    "href": "class_02.html#data-science-tasks",
    "title": "Data Science for Business Applications",
    "section": "Data Science tasks",
    "text": "Data Science tasks\n\nWe found that adding more predictors to linear models increases their accuracy and explanatory power.\nWhat if we want to add instead of Quantative predictors, Qualitative predictors?\nCategorical or Qualitative Variables split the data into different groups or levels.\nHow can we add these types of variables in the regression model?"
  },
  {
    "objectID": "class_02.html#lets-start-with-an-example",
    "href": "class_02.html#lets-start-with-an-example",
    "title": "Data Science for Business Applications",
    "section": "Let’s start with an example!",
    "text": "Let’s start with an example!\n\n\nExample: Cars dataset (cars_luxury.csv)\nData on 2,088 used cars in South California\nFor each car there are several predictors as:\nprice: Price of the car in dollars.\nmileage: Car mileage.\nluxury: If the car is a luxury car: “\\(\\texttt{yes}\\)”or “\\(\\texttt{no}\\)”\nbadge: Badge indicating if the car is considered some type of deal, that can be: “\\(\\texttt{Good Deal}\\)”, “\\(\\texttt{Great Deal}\\)”, “\\(\\texttt{No Badge}\\)” or “\\(\\texttt{Fair Price}\\)”.\n(and others)"
  },
  {
    "objectID": "class_02.html#luxury-and-price",
    "href": "class_02.html#luxury-and-price",
    "title": "Data Science for Business Applications",
    "section": "Luxury and price",
    "text": "Luxury and price\n\n\nBefore we start our analysis, let’s see if there’s a difference between the used price of luxury and not luxury cars.\n\n\n\n\nlibrary(tidyverse)\nggplot(cars_luxury, aes(x = luxury, y = price)) +\n  geom_boxplot()"
  },
  {
    "objectID": "class_02.html#regression-model",
    "href": "class_02.html#regression-model",
    "title": "Data Science for Business Applications",
    "section": "Regression model",
    "text": "Regression model\n\nIt’s interesting to incorporate the categorical variable luxury, into the multiple regression model.\nWe want to see the impact of mileage on price controlling for the type of car. If it’s a luxury or not.\nThe resulting model is equal to: \\[\n\\texttt{price} = \\beta_0 + \\beta_1\\texttt{mileage} + \\beta_2\\texttt{luxury} + e\n\\]\nHow can we assess the categorical variable luxury?"
  },
  {
    "objectID": "class_02.html#price-in-terms-of-mileage-and-luxury",
    "href": "class_02.html#price-in-terms-of-mileage-and-luxury",
    "title": "Data Science for Business Applications",
    "section": "Price in terms of mileage and luxury",
    "text": "Price in terms of mileage and luxury\n\n\nLet’s plot the relation between mileage, price and luxury\n\n\n\nggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n  geom_point()"
  },
  {
    "objectID": "class_02.html#dummy-variable",
    "href": "class_02.html#dummy-variable",
    "title": "Data Science for Business Applications",
    "section": "Dummy variable",
    "text": "Dummy variable\n\nluxury is a categorical variable (\\(\\texttt{\"yes\"}\\) or \\(\\texttt{\"no\"}\\) in this data set).\nThis variable contains two groups or two levels.\nRecode luxury into a quantitative variable where \\(\\texttt{1 = \"yes\"}\\), \\(\\texttt{0 = \"no\"}\\).\nThis quantitative variable is known as dummy variable.\nR does this for us.\nR will choose the alphabetically first category as the 0 level."
  },
  {
    "objectID": "class_02.html#regression-model-1",
    "href": "class_02.html#regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Regression model",
    "text": "Regression model\n\n# Remove scientific notation \noptions(scipen = 999)\n\n# Regression Model\nlm1 = lm(price ~ mileage + luxury, data = cars_luxury)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ mileage + luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24018  -6204  -1919   3727  78453 \n\nCoefficients:\n                Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 25422.756210   508.681485   49.98 &lt;0.0000000000000002 ***\nmileage        -0.185784     0.008688  -21.39 &lt;0.0000000000000002 ***\nluxuryyes   12986.388662   569.304402   22.81 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11010 on 2085 degrees of freedom\nMultiple R-squared:  0.3439,    Adjusted R-squared:  0.3433 \nF-statistic: 546.4 on 2 and 2085 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nHow can we interpret these numbers?"
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model",
    "href": "class_02.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\nEstimated model:\n\n\n\\[\n\\texttt{price} = 25,423 - 0.19\\times \\texttt{mileage} + 12,986 \\times \\texttt{luxury}\n\\]\n\nHow can we interpret the coefficients?\nintercept: For a car with zero mileage and luxury = \\(\\texttt{\"no\"}\\) = 0, the average selling price is equal to US$ 25,423.\nmileage: For a fixed type of car, for each extra increase in mileage (in miles), there will be a decrease of US$ 0.19 in the price of the car.\nluxury: For cars with the same mileage, the added price of being a luxury car (luxury = \\(\\texttt{\"yes\"}\\) = 1) is US$ 12,986.\nImportant: When we add a categorical variable to the regression model, the intercept is also referred to as the baseline. The effect of the categorical variable is also known as the offset."
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-1",
    "href": "class_02.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nBy adding a categorical variable, we can also interpret this as different regression models depending on the number of groups.\nTo do so we add the effect of the categorical variable to the intercept.\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\texttt{price} &= 25,423 - 0.19\\times \\texttt{mileage} + 12,986 \\times (1) \\\\\n             &= (25,423 + 12,986) - 0.19\\times \\texttt{mileage} \\\\\n             &= 38,409 - 0.19\\times \\texttt{mileage} \\\\\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"no\"}\\) = 0 \\[\n\\begin{align}\n\\texttt{price} &= 25,423 - 0.19\\times \\texttt{mileage} + 12,986 \\times (0) \\\\\n             &= 25,423 - 0.19\\times \\texttt{mileage} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "class_02.html#section",
    "href": "class_02.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We can even visualize these two models in a plot\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_line(aes(y=predict(lm1)))"
  },
  {
    "objectID": "class_02.html#significance-and-predictions",
    "href": "class_02.html#significance-and-predictions",
    "title": "Data Science for Business Applications",
    "section": "Significance and Predictions",
    "text": "Significance and Predictions\n\nIs the price of a luxury cars statistically different of a non-luxury car?\n\n\nconfint(lm1)\n\n                    2.5 %        97.5 %\n(Intercept) 24425.1797220 26420.3326977\nmileage        -0.2028214    -0.1687463\nluxuryyes   11869.9244244 14102.8528996\n\n\n\nYes, with 95% confidence we can conclude that the price of a luxury car is different from a non luxury one.\nWhat is estimated price of luxury vehicle that has as mileage of 50000.\n\n\npredict(lm1, list(mileage = 50000, luxury = \"yes\"))\n\n       1 \n29119.95 \n\n\n\nThe estimated price of a 50,000-mile luxury car will be US$ 29,120."
  },
  {
    "objectID": "class_02.html#more-than-two-groups",
    "href": "class_02.html#more-than-two-groups",
    "title": "Data Science for Business Applications",
    "section": "More than two groups",
    "text": "More than two groups\n\nSuppose, instead of controlling the fact that a car is a luxury car or not, we want to observe the effect of badge on price.\nThe variable badge contains for groups or levels: “\\(\\texttt{Good Deal}\\)”, “\\(\\texttt{Great Deal}\\)”, “\\(\\texttt{No Badge}\\)” or “\\(\\texttt{Fair Price}\\)”.\nIs there a difference in the price of the car depending on what type of badge it holds?"
  },
  {
    "objectID": "class_02.html#section-1",
    "href": "class_02.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Let’s plot the relation between mileage, price and badge\n\n\nggplot(cars_luxury, aes(x = mileage, y = price, col = badge)) +\n  geom_point()"
  },
  {
    "objectID": "class_02.html#regressions-model",
    "href": "class_02.html#regressions-model",
    "title": "Data Science for Business Applications",
    "section": "Regressions model",
    "text": "Regressions model\n\n\nRun the model: \\(\\texttt{price} = \\beta_0 + \\beta_1\\times \\texttt{mileage} + \\beta_2 \\times \\texttt{badge} + e\\)\n\n\nlm2 = lm(price ~ mileage + badge, data = cars_luxury)\nsummary(lm2)\n\n\nCall:\nlm(formula = price ~ mileage + badge, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-19961  -6981  -2395   3629  82508 \n\nCoefficients:\n                    Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)     35931.481715  1140.032009  31.518 &lt; 0.0000000000000002 ***\nmileage            -0.209568     0.009527 -21.997 &lt; 0.0000000000000002 ***\nbadgeGood Deal  -3556.561624  1057.385699  -3.364             0.000783 ***\nbadgeGreat Deal -8988.415770  1062.334934  -8.461 &lt; 0.0000000000000002 ***\nbadgeNo Badge   -9930.896296  1143.713386  -8.683 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11860 on 2083 degrees of freedom\nMultiple R-squared:  0.2388,    Adjusted R-squared:  0.2374 \nF-statistic: 163.4 on 4 and 2083 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-2",
    "href": "class_02.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\nintercept (baseline): For a car with zero mileage and with a fair price badge, the average selling price is equal to US$ 35,932 (\\(\\texttt{Good Deal} = 0\\),\\(\\texttt{Great Deal} = 0\\), \\(\\texttt{No Badge} = 0\\)).\nmileage: For a car with a fixed badge, for each extra increase in mileage (in miles), there will be a decrease of US$ 0.21 in the price of the car.\n\\(\\texttt{Good Deal} = 1\\), remainig levels equal to zero: For cars with the same mileage, there will be a decrease in their price if they have a good deal badge of US$ 3,557 compared to the baseline, that is, cars with a fair price badge.\n\\(\\texttt{Great Deal}  = 1\\), remainig levels equal to zero: For cars with the same mileage, there will be a decrease in their price if they have a great deal badge of US$ 8,988 compared to the baseline, that is, cars with a fair price badge.\n\\(\\texttt{No Badge}  = 1\\), remainig levels equal to zero: For cars with the same mileage, there will be a decrease in their price if they have no badge of US$ 8,988 compared to the baseline, that is, cars with a fair price badge."
  },
  {
    "objectID": "class_02.html#section-2",
    "href": "class_02.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We have now 4 different models, one for each badge catgory.\nWe have four different intecepts."
  },
  {
    "objectID": "class_02.html#interactions",
    "href": "class_02.html#interactions",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nWe observed that there was a significant difference between the price of luxury and non-luxury cars.\nIs there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction."
  },
  {
    "objectID": "class_02.html#interactions-1",
    "href": "class_02.html#interactions-1",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\texttt{price} = \\beta_0 + \\beta_1\\texttt{mileage} + \\beta_2\\texttt{luxury} + \\beta_3 (\\texttt{luxury} \\times \\texttt{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\texttt{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\texttt{price} = \\beta_0 + \\beta_1\\texttt{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\texttt{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\texttt{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\texttt{mileage} + e\n\\]"
  },
  {
    "objectID": "class_02.html#regression-model-2",
    "href": "class_02.html#regression-model-2",
    "title": "Data Science for Business Applications",
    "section": "Regression Model",
    "text": "Regression Model\n\n\nLet’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-3",
    "href": "class_02.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nHow do we interpret this model?\nintercept (baseline), luxury = \\(\\texttt{\"no\"}\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\\texttt{\"no\"}\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\\texttt{\"yes\"}\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars."
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-4",
    "href": "class_02.html#interpretation-of-the-model-4",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nWe also have the following interpretation:\nluxury = \\(\\texttt{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\texttt{price} &= 23,894 - 0.15\\times \\texttt{mileage} + 19,772 (0) - 0.16\\times \\texttt{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\texttt{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\texttt{price} &= 23,894 - 0.15\\times \\texttt{mileage} + 19,772 (1) - 0.16\\times \\texttt{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\texttt{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\texttt{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope."
  },
  {
    "objectID": "class_02.html#section-3",
    "href": "class_02.html#section-3",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_02.html#significance-and-predictions-1",
    "href": "class_02.html#significance-and-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Significance and Predictions",
    "text": "Significance and Predictions\n\nDo luxury cars depreciate faster than non-luxury cars?\n\n\nconfint(lm3)\n\n                          2.5 %        97.5 %\n(Intercept)       22824.7212986 24962.4814687\nmileage              -0.1735141    -0.1358795\nluxuryyes         17629.8713295 21914.9959940\nmileage:luxuryyes    -0.1975365    -0.1133770\n\n\n\nYes, with 95% confidence we can conclude that the price of a luxury car depreciates faster than a non-luxury one.\nWhat is estimated price of luxury vehicle that has as mileage of 50000.\n\n\npredict(lm3, list(mileage = 50000, luxury = \"yes\"))\n\n       1 \n28158.36 \n\n\n\nThe estimated price of a 50,000-mile luxury car will be US$ 28,158.\nThere was also a decrese in the RSE of this model compared to the model without the interaction. From \\(\\texttt{11860}\\) to \\(\\texttt{10,880}\\)."
  },
  {
    "objectID": "class_02.html#conlusion",
    "href": "class_02.html#conlusion",
    "title": "Data Science for Business Applications",
    "section": "Conlusion",
    "text": "Conlusion\n\nInteractions make a model more complex to analyze and explain, so it’s only worth doing so when you get better interpretation and more accurate predictions.\nWe can have interactions between different kinds of variables. Between categorical variables, numerical and categorical and, numerical and numerical.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them"
  },
  {
    "objectID": "class_03.html",
    "href": "class_03.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Linear models are useful:\n\nPrediction - given a new observations\nExplanatory power - which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "class_03.html#regression-assumptions-and-potential-problems",
    "href": "class_03.html#regression-assumptions-and-potential-problems",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "class_03.html#regression-assumptions-and-potential-problems-1",
    "href": "class_03.html#regression-assumptions-and-potential-problems-1",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "class_03.html#multiple-regression-assumptions",
    "href": "class_03.html#multiple-regression-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression assumptions",
    "text": "Multiple regression assumptions\nWe need four things to be true for regression to work properly:\n\nLinearity: \\(Y\\) is a linear function of the \\(X\\)’s (except for the prediction errors).\nIndependence: The prediction errors are independent.\nNormality: The prediction errors are normally distributed.\nEqual Variance: The variance of \\(Y\\) is the same for any value of \\(X\\) (“homoscedasticity”)."
  },
  {
    "objectID": "class_03.html#non-linearity",
    "href": "class_03.html#non-linearity",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity",
    "text": "Non-Linearity\n\n\nWhat we would expect to observe in a regression where there is a linear relation?\n\n\n\nlibrary(tidyverse)\nggplot(linear_data, aes(x=X, y=Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "class_03.html#residuals",
    "href": "class_03.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nLet’s plot the residuals \\(r_i\\), such that \\[r_i = y_i − \\widehat{y}_i\\] where \\(\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\) vs \\(x_i\\)\nHopefully identify non-linear relationships\nWe are looking for patterns or trends in the residuals"
  },
  {
    "objectID": "class_03.html#residuals-1",
    "href": "class_03.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\n\nPlot of the residuals\nHow can these residuals be useful for us?"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plots",
    "href": "class_03.html#regression-diagnostic-plots",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plots",
    "text": "Regression diagnostic plots\nWe’ll use regression diagnostic plots to help us evaluate some of the assumptions.\nThe residuals vs fitted graph plots:\n\nResiduals on the \\(Y\\)-axis\nFitted values (predicted \\(Y\\) values) on the \\(X\\)-axis\n\nThis graph effectively subtracts out the linear trend between \\(Y\\) and the \\(X\\)’s, so we want to see no trend left in this graph."
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot",
    "href": "class_03.html#regression-diagnostic-plot",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check non-linearity we focus on the Residual vs. Fitted plot\n\n\n\nlibrary(ggfortify)\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-1",
    "href": "class_03.html#regression-diagnostic-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nFrom the Residual vs. Fitted plot, we can observe that since the residuals are evenly distributed around zero in relation to the fitted values, we have that the linear regression model is a good fit for this data.\nThis means that we are learning the linear representation contained in this data."
  },
  {
    "objectID": "class_03.html#non-linearity-example",
    "href": "class_03.html#non-linearity-example",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\n\nWhat we would expect to observe if the relation is non linear?\n\n\n\nggplot(nonlinear_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "class_03.html#non-linearity-example-1",
    "href": "class_03.html#non-linearity-example-1",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nLet’s look at the residuals for this model\n\n\n\nLet’s check the residual plot"
  },
  {
    "objectID": "class_03.html#non-linearity-example-2",
    "href": "class_03.html#non-linearity-example-2",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nlm2 = lm(Y ~ X, data = nonlinear_data)\nautoplot(lm2)"
  },
  {
    "objectID": "class_03.html#non-linearity-example-3",
    "href": "class_03.html#non-linearity-example-3",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nFrom the Residual vs. Fitted, we can observe that the residuals are not evenly distributed around zero.\nThis indicates that for lower and higher values of \\(x_i\\) our model is overpredicting and underpredicting in the mid values.\nWhat are the implications in this case?\nWorse predictions"
  },
  {
    "objectID": "class_03.html#independence",
    "href": "class_03.html#independence",
    "title": "Data Science for Business Applications",
    "section": "Independence",
    "text": "Independence\n\nIndependence means that knowing the prediction error for one observation doesn’t tell you anything about the error for another observation\nData collected over time are usually not independent\nWe can’t use regression diagnostics to decide the independence\nWe have to measure the autocorrelation of the residuals\nWe’ll get back to autocorrelation when we discuss Time Series models"
  },
  {
    "objectID": "class_03.html#normality-assumption",
    "href": "class_03.html#normality-assumption",
    "title": "Data Science for Business Applications",
    "section": "Normality assumption",
    "text": "Normality assumption\n\nWhen we’ve been interpreting residual standard error (RSE) , we’ve used the following interpretation:\n95% of our predictions will be accurate to within plus or minus \\(2\\times RSE\\).\nIn order for this to be true, the residuals have to be Normally distributed"
  },
  {
    "objectID": "class_03.html#normality-example",
    "href": "class_03.html#normality-example",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\n\nWe can check the distribution of the residuals\n\n\n\nlinear_data = linear_data %&gt;% \n  mutate(resid = residuals(lm1))\n\nggplot(linear_data, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 0.2)"
  },
  {
    "objectID": "class_03.html#normality-example-1",
    "href": "class_03.html#normality-example-1",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nBut how can we judge if the residuals follows a Normal distribution?\nThe key is to look at the Normal Q-Q plot, which compares the distribution of our residuals to a perfect Normal distribution.\nIf the dots line up along an (approximately) straight line, then the Normality assumption is satisfied."
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-2",
    "href": "class_03.html#regression-diagnostic-plot-2",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check for Normality we focus on the Normal Q-Q plot\n\n\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)\n\n\n\nIn this case the normality assumptions seem to be met"
  },
  {
    "objectID": "class_03.html#normality-example-2",
    "href": "class_03.html#normality-example-2",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nLet’s look at different data.\nIn this case the data has non Normal errors."
  },
  {
    "objectID": "class_03.html#normality-example-3",
    "href": "class_03.html#normality-example-3",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\n\nHistogram of the residuals (right skewed)\n\n\n\nlm3 = lm(Y ~ X, data = non_normal)\n\nnon_normal = non_normal %&gt;% \n  mutate(resid = residuals(lm3))\n\nggplot(non_normal, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 1)"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-3",
    "href": "class_03.html#regression-diagnostic-plot-3",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nautoplot(lm3)"
  },
  {
    "objectID": "class_03.html#interpretation-of-the-plot",
    "href": "class_03.html#interpretation-of-the-plot",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Normal Q-Q plot, we can observe that the residuals are not following the line that indicates the Normal quantiles\nThis means that our model results in non-normal residuals\nThis affects statistical tests, and confidence intervals"
  },
  {
    "objectID": "class_03.html#equal-variance",
    "href": "class_03.html#equal-variance",
    "title": "Data Science for Business Applications",
    "section": "Equal variance",
    "text": "Equal variance\n\nEqual variance is also known as “homoscedasticity”\nThe variance of \\(Y\\) should be about the same at any \\(X\\) value (or combination of values for the \\(X\\)’s).\nIn other words, the vertical spread of the points should be the same anywhere along the \\(X\\)-axis.\nIf there’s no equal variance then we might have heteroskedasticity.\nLower precision, estimates are further from the correct population value."
  },
  {
    "objectID": "class_03.html#equal-variance-example",
    "href": "class_03.html#equal-variance-example",
    "title": "Data Science for Business Applications",
    "section": "Equal variance example",
    "text": "Equal variance example\n\nThe vertical spread of the points is larger along the right side of the graph\n\n\nggplot(heter_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-4",
    "href": "class_03.html#regression-diagnostic-plot-4",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check for homoscedasticity we focus on the Scale-Location plot\n\n\n\nlm4 = lm(Y ~ X, data = heter_data)\nautoplot(lm4)"
  },
  {
    "objectID": "class_03.html#interpretation-of-the-plot-1",
    "href": "class_03.html#interpretation-of-the-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Sacle-Location plot, we can observe that the residuals have a fan shape, indicating that there is heteroscedacity in the data.\nThis resulted in lower precision; thus, estimates are further from the correct population value."
  },
  {
    "objectID": "class_03.html#influential-observations",
    "href": "class_03.html#influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Influential observations",
    "text": "Influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "class_03.html#diagnostics-plot",
    "href": "class_03.html#diagnostics-plot",
    "title": "Data Science for Business Applications",
    "section": "Diagnostics Plot",
    "text": "Diagnostics Plot\n\nWe can observe the point with high residual on the Residual vs. Leverage plot\n\n\nlm5 = lm(Y ~ X, data = outlier_residual)\nautoplot(lm5)"
  },
  {
    "objectID": "class_03.html#high-leverage",
    "href": "class_03.html#high-leverage",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can also have points with high leverage - when a point in \\(X\\) is distant from the average on \\(X\\)\n\n\n\nThis point has low residual but high leverage. RSE = 0.2956"
  },
  {
    "objectID": "class_03.html#high-leverage-1",
    "href": "class_03.html#high-leverage-1",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can observe the point with high leverage on the Residual vs. Leverage plot\n\n\nlm6 = lm(Y ~ X, data = outlier_leverage)\nautoplot(lm6)"
  },
  {
    "objectID": "class_03.html#points-with-high-influence",
    "href": "class_03.html#points-with-high-influence",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nPoints with high leverage and high residuals are known as influential points\n\n\n\nThis point has high residual but high leverage. RSE = 0.8281"
  },
  {
    "objectID": "class_03.html#points-with-high-influence-1",
    "href": "class_03.html#points-with-high-influence-1",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWe can observe the point with high influence on the Residual vs. Leverage plot\n\n\nlm7 = lm(Y ~ X, data = outlier_influence)\nautoplot(lm7)"
  },
  {
    "objectID": "class_03.html#points-with-high-influence-2",
    "href": "class_03.html#points-with-high-influence-2",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWhen a case has a very unusual \\(X\\) value, it has leverage — the potential to have a big impact on the regression line\nIf the case is in line with the overall trend of the regression line, it won’t be a problem\nBut when that case also has a \\(Y\\) (high residual) value that is out of line\nWe need both a large residual and high leverage for an observation to be influential\nWe should be worried about these points\nThey affect the coefficients and predictions"
  },
  {
    "objectID": "class_03.html#regression-assumptions-and-outliers",
    "href": "class_03.html#regression-assumptions-and-outliers",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Outliers",
    "text": "Regression Assumptions, and Outliers\nLinear models are useful:\n\nPrediction - given a new observations\nExplanatory power - which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "class_03.html#outliers-and-influential-observations",
    "href": "class_03.html#outliers-and-influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Outliers and influential observations",
    "text": "Outliers and influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "class_04.html#polynomial-models",
    "href": "class_04.html#polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nThe data set utilities contains information on the utility bills for a house in Minnesota. We’ll focus on two variables:\n\ndailyspend is the average amount of money spent on utilities (e.g. heating) for each day during the month\ntemp is the average temperature outside for that month"
  },
  {
    "objectID": "class_04.html#polynomial-models-1",
    "href": "class_04.html#polynomial-models-1",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWhat problems do you see here?\n\nlibrary(tidyverse)\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point()"
  },
  {
    "objectID": "class_04.html#polynomial-models-2",
    "href": "class_04.html#polynomial-models-2",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm1 &lt;- lm(dailyspend ~ temp, data=utilities) \nsummary(lm1)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59   &lt;2e-16 ***\ntemp        -0.096432   0.003911  -24.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 2.2e-16\n\n\n\nLet’s interpret this relation\nFor one unit increase in temperature (Fahrenheit), there will be a 10-cent decrease in spending"
  },
  {
    "objectID": "class_04.html#polynomial-models-3",
    "href": "class_04.html#polynomial-models-3",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlibrary(ggfortify)\nautoplot(lm1)\n\n\n\nLinearity and homoscedasticity are violated"
  },
  {
    "objectID": "class_04.html#polynomial-models-4",
    "href": "class_04.html#polynomial-models-4",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe’ll use polynomial regression to fix problems\nIf a polynomial curve (e.g., quadratic, cubic, etc) would be a better fit for the data than a line, we can fit a curve to the data.\nThe way we do this is by adding \\(X^2\\) to the model as a second predictor variable.\nThis can “fix” the linearity problem because now \\(Y\\) is a linear function of \\(X\\) and \\(X^2\\), resulting in: \\[\nY = \\beta_0 + \\beta_1\\cdot X + \\beta\\cdot X^2 + e\n\\]"
  },
  {
    "objectID": "class_04.html#polynomial-models-5",
    "href": "class_04.html#polynomial-models-5",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe add the term I(temp^2) in the regression equation:\n\n\nlm2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities) \nsummary(lm2)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239  &lt; 2e-16 ***\ntemp        -0.2115553  0.0191046 -11.074  &lt; 2e-16 ***\nI(temp^2)    0.0012476  0.0002037   6.124 1.33e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe have that the new term is evaluated as an extra variable."
  },
  {
    "objectID": "class_04.html#polynomial-models-6",
    "href": "class_04.html#polynomial-models-6",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWriting out the equation: \\[\n\\widehat{\\texttt{dailyspend}} = 9.4723 −0.2116\\cdot \\texttt{temp} + 0.0012\\cdot \\texttt{temp}^2\n\\] The effect of the extra variable is statistically significant:\n\nconfint(lm2)\n\n                    2.5 %       97.5 %\n(Intercept)  8.6981381712 10.246438869\ntemp        -0.2494014032 -0.173709160\nI(temp^2)    0.0008440041  0.001651114\n\n\n\nThe residual standard error of the polynomial model is \\(\\texttt{0.75}\\).\nThe residual standard error of the linear model is \\(\\texttt{0.87}\\)."
  },
  {
    "objectID": "class_04.html#polynomial-models-7",
    "href": "class_04.html#polynomial-models-7",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nAdding an \\(X^2\\) term fits a parabola to the data (orange line)\n\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm1)), col = \"lightblue\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")"
  },
  {
    "objectID": "class_04.html#polynomial-models-8",
    "href": "class_04.html#polynomial-models-8",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nIt solves the linearity problem\n\nautoplot(lm2)"
  },
  {
    "objectID": "class_04.html#polynomial-models-9",
    "href": "class_04.html#polynomial-models-9",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWhat about a higher-order polynomial?\nWe could fit a cubic curve by adding an \\(X^3\\) term\nMaking the polynomial higher order will decrease the RSE\nWhy not go nuts and fit a 7th degree polynomial?\n\n\n\n\nDegree\nname\nRSE\n\n\n\n\n1\nlinear\n0.866\n\n\n2\nquadratic\n0.754\n\n\n3\ncubic\n0.755\n\n\n4\nquartic\n0.755\n\n\n5\nquintic\n0.758\n\n\n6\n\n0.761\n\n\n7\n\n0.761"
  },
  {
    "objectID": "class_04.html#polynomial-models-10",
    "href": "class_04.html#polynomial-models-10",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ poly(temp,7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "class_04.html#building-polynomial-models",
    "href": "class_04.html#building-polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Building polynomial models",
    "text": "Building polynomial models\nStart simple: only add higher-degree terms to the extent it gives you a substantial decrease in the RSE, or satisfies an assumption hold that wasn’t satisfied before\n\nYou must include lower-order terms: e.g., if you add \\(X^3\\), you must also include \\(X\\) and \\(X^2\\)\nBe careful about overfitting when adding higher-order terms!\nBe particularly careful about extrapolating beyond the range of the data!\nMind-bender: We can think about an \\(X^2\\) term as an interaction of \\(X\\) with itself: in a parabola, the slope depends on the value of \\(X\\)"
  },
  {
    "objectID": "class_04.html#the-log-transformation",
    "href": "class_04.html#the-log-transformation",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nWe saw that we can use transformations to fix problems\nSometimes, a violation of regression assumptions can be fixed by transforming one or the other of the variables (or both).\nWhen we transform a variable, we have to also transform our interpretation of the equation."
  },
  {
    "objectID": "class_04.html#the-log-transformation-1",
    "href": "class_04.html#the-log-transformation-1",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\nThe log transformation is frequently useful in regression, because many nonlinear relationships are naturally exponential.\n\n\\(\\log_b x=y\\) when \\(b^y=x\\)\nFor example, \\(\\log_{10} 1000 = 3\\), \\(\\log_{10}100 = 2\\), and \\(\\log_{10}10 = 1\\)"
  },
  {
    "objectID": "class_04.html#the-log-transformation-2",
    "href": "class_04.html#the-log-transformation-2",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!)\nSkewed data is also a good candidate for log"
  },
  {
    "objectID": "class_04.html#moores-law",
    "href": "class_04.html#moores-law",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nMoore’s Law was a prediction made by Gordon Moore in 1965 (!) that the number of transistors on computer chips would double every 2 years\nThis implies exponential growth, so a linear model won’t fit well (and neither will any polynomial)\n\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_04.html#moores-law-1",
    "href": "class_04.html#moores-law-1",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nlm_moore = lm(Transistor.count ~ Date.of.introduction, data = moores)\nautoplot(lm_moore)\n\n\n\nA linear model is a spectacular fail"
  },
  {
    "objectID": "class_04.html#modeling-exponential-growth",
    "href": "class_04.html#modeling-exponential-growth",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\nIf \\(Y = ae^{bX}\\), then\n\\[\\log(Y) = \\log(a)+ bX\\]\n\nIn other words, \\(\\log(Y)\\) is a linear function of \\(X\\) when \\(Y\\) is an exponential function of \\(X\\)\nSo if we think \\(Y\\) is an exponential function of \\(X\\), predict \\(\\log(Y)\\) as a linear function of \\(X\\)"
  },
  {
    "objectID": "class_04.html#modeling-exponential-growth-1",
    "href": "class_04.html#modeling-exponential-growth-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nTransistors does NOT have a linear relationship with year\n\\(\\log(\\texttt{Transistors})\\) does have a linear relationship with year\n\n\nggplot(moores, aes(x = Date.of.introduction, y = log(Transistor.count))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_04.html#log-linear-model",
    "href": "class_04.html#log-linear-model",
    "title": "Data Science for Business Applications",
    "section": "Log-linear Model",
    "text": "Log-linear Model\nLet’s run the regression model\n\noptions(scipen = 999)\nlm_moore = lm(log(Transistor.count) ~ Date.of.introduction, data = moores)\nsummary(lm_moore)\n\n\nCall:\nlm(formula = log(Transistor.count) ~ Date.of.introduction, data = moores)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1299 -0.3338  0.1767  0.5230  2.0626 \n\nCoefficients:\n                        Estimate  Std. Error t value            Pr(&gt;|t|)    \n(Intercept)          -681.212056   15.958165  -42.69 &lt;0.0000000000000002 ***\nDate.of.introduction    0.349154    0.007981   43.75 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.054 on 99 degrees of freedom\nMultiple R-squared:  0.9508,    Adjusted R-squared:  0.9503 \nF-statistic:  1914 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "class_04.html#modeling-exponential-growth-2",
    "href": "class_04.html#modeling-exponential-growth-2",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nautoplot(lm_moore)"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model",
    "href": "class_04.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\nOur model is \\[\\widehat{\\log(\\texttt{Transistors})} = −681.21 + 0.35 \\cdot \\texttt{Year}\\]\nTwo interpretations of the slope coefficient:\n\nEvery year, the predicted log of transistors goes up by 0.35\nMore useful: Every year, the predicted number of transistors goes up by 35%\nA constant percentage increase every year is exponential growth!"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model-1",
    "href": "class_04.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nMaking predictions using the log-linear model\nWhen making predictions, we have to remember that our equation gives us predictions for \\(\\log(\\texttt{Transistors})\\), not Transistors!\n\nExample: To make a prediction for the number of transistors in 2022: \\[\n\\log(\\texttt{Transistors}) = −681.21 + 0.35(2022) = 26.49\n\\] But our prediction is not 26.49:\n\\(e^{\\log(\\texttt{Transistors})} = e^{26.49} = 319,492,616,196\\)"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model-2",
    "href": "class_04.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_line(aes(x = Date.of.introduction, y = exp(predict(lm_moore))), col = \"orange\")"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model-3",
    "href": "class_04.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\n\n\n\n\nModel\nEquation\nInterpretation\n\n\n\n\nLinear\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies \\(\\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-linear\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies ≈ \\(100 \\cdot \\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)\n\n\nLinear-log\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(0.01 \\cdot \\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-log\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(\\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)"
  },
  {
    "objectID": "class_04.html#conclusion",
    "href": "class_04.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhen is the log transformation useful?\nYou can transform \\(X \\rightarrow \\log(X)\\), \\(Y \\rightarrow \\log(Y)\\), or both\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!), try transforming it with a log\nIn this case, Transistors is skewed right so it is a good candidate for log\nYou may need to do a little bit of trial and error to see what works best\nOther transformations are possible!"
  },
  {
    "objectID": "class_05.html#basic-time-series-concepts",
    "href": "class_05.html#basic-time-series-concepts",
    "title": "Data Science for Business Applications",
    "section": "Basic time series concepts",
    "text": "Basic time series concepts\n\nApple quarterly revenue (Billions of dollars)\nGoal: What is the pattern here, and how can we forecast future earnings?\n\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "class_05.html#what-are-time-series",
    "href": "class_05.html#what-are-time-series",
    "title": "Data Science for Business Applications",
    "section": "What are time series?",
    "text": "What are time series?\n\nData where the cases represent time: data collected every day, month, year, etc.\nTime series are important for both explaining how variables change over time and forecasting the future\nExamples of time series data:\nGoogle’s closing daily stock price every day in 2020\nInventory levels of each item at a retail store at the end of every week in 2020\nNumber of new COVID cases in the US each day since the start of the pandemic\nApple’s quarterly revenue since 2009"
  },
  {
    "objectID": "class_05.html#anatomy-of-a-time-series",
    "href": "class_05.html#anatomy-of-a-time-series",
    "title": "Data Science for Business Applications",
    "section": "Anatomy of a time series",
    "text": "Anatomy of a time series\nSome notation:\n\n\\(t = 1,2,3,...\\), time index\n\\(Y_t\\), is the value: of the variable of interest at time \\(t\\)\n\\(Y_t\\) may be composed of one or more components:\nTrend\nSeasonal\nCyclical\nRandom"
  },
  {
    "objectID": "class_05.html#trend-component",
    "href": "class_05.html#trend-component",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nA trend is persistent upwards or downwards movement in the data (not necessarily linear)."
  },
  {
    "objectID": "class_05.html#trend-component-1",
    "href": "class_05.html#trend-component-1",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nExample: Moore’s Law (accelerating increase of transistor count)\nExample: US population over time\nA time series with no trend is called stationary."
  },
  {
    "objectID": "class_05.html#seasonal-component",
    "href": "class_05.html#seasonal-component",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nSeasonal fluctuation occurs when predictable up or down movements occur over a regular interval."
  },
  {
    "objectID": "class_05.html#seasonal-component-1",
    "href": "class_05.html#seasonal-component-1",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nThe ups and downs must occur over a regular interval (e.g., every month, or every year)\nExample: Highway traffic volume is highest during rush hour every day\nExample: Supermarket sales may be highest every month right after common paydays like the 15th and 30th"
  },
  {
    "objectID": "class_05.html#cyclic-component",
    "href": "class_05.html#cyclic-component",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nCyclic fluctuations occur at unpredictable intervals, e.g. due to changing business or economic conditions."
  },
  {
    "objectID": "class_05.html#cyclic-component-1",
    "href": "class_05.html#cyclic-component-1",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nIn contrast to seasonal fluctuations, cyclic fluctuations do not occur at regular, predictable intervals\nIt may be possible to predict cyclic components based on some other (non-time) variable\nExample: Restaurant sales dropped dramatically in 2020 due to COVID, as people ate out less\nExample: Sales of bell bottoms rose in the 60s and 70s, declined by the 80s, and then had a resurgence in the 90s"
  },
  {
    "objectID": "class_05.html#remaindererror-component",
    "href": "class_05.html#remaindererror-component",
    "title": "Data Science for Business Applications",
    "section": "Remainder/Error component",
    "text": "Remainder/Error component\n\nAny real time series will always have random noise as well, which can’t be predicted or forecast."
  },
  {
    "objectID": "class_05.html#time-series-components",
    "href": "class_05.html#time-series-components",
    "title": "Data Science for Business Applications",
    "section": "Time Series Components",
    "text": "Time Series Components\n\nWhich component(s) you see in each of these time series?"
  },
  {
    "objectID": "class_05.html#putting-these-together",
    "href": "class_05.html#putting-these-together",
    "title": "Data Science for Business Applications",
    "section": "Putting these together",
    "text": "Putting these together\nReal time series will usually include a combination of these four components. We will model the time series \\(Y_t\\) either additively:\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\] Or multiplicatively: \\[\nY_t = \\text{Trend}\\cdot\\text{Seasonal}\\cdot\\text{Random}= T_t \\cdot S_t \\cdot E_t\n\\] * (\\(E_t\\) consists of both the cyclic and error components, as both are unpredictable.) This model can be rewritten as a log model: \\[\n\\log{Y_t} = \\log(T_t) + \\log(S_t) + \\log(E_t)\n\\]"
  },
  {
    "objectID": "class_05.html#additive-models",
    "href": "class_05.html#additive-models",
    "title": "Data Science for Business Applications",
    "section": "Additive models",
    "text": "Additive models\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\]\n\nMost appropriate when seasonal fluctuations are consistent (do not increase or decrease over time)\nThe trend component \\(T_t\\) is a function of t (e.g., linear or quadratic)\nThe seasonal component \\(S_t\\) is a set of dummy variable representing “seasons”\nSo we can estimate additive models using regular regression"
  },
  {
    "objectID": "class_05.html#additive-decomposition",
    "href": "class_05.html#additive-decomposition",
    "title": "Data Science for Business Applications",
    "section": "Additive decomposition",
    "text": "Additive decomposition\n\nRun a regression predicting \\(Y\\) as a function of:\n\n\n\\(t\\), \\(t^2\\), \\(\\log(t)\\) etc (the trend component \\(T_t\\))\nDummy variables for the seasons (the seasonal component \\(S_t\\))\n\n\nTo make a prediction for \\(Y\\), plug into the model!\nThe residuals of this model correspond to the error component \\(E_t\\)"
  },
  {
    "objectID": "class_05.html#apple-quarterly-revenue",
    "href": "class_05.html#apple-quarterly-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple quarterly revenue",
    "text": "Apple quarterly revenue\n\nWhat components do you see here?\n\n\nlibrary(tidyverse)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "class_05.html#fitting-additive-model",
    "href": "class_05.html#fitting-additive-model",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nlm_additive = lm(Revenue ~ Period + Quarter, data=apple) \nsummary(lm_additive)\n\n\nCall:\nlm(formula = Revenue ~ Period + Quarter, data = apple)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -5.135   1.280   4.923  17.928 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  33.93619    2.74731  12.353  &lt; 2e-16 ***\nPeriod        1.41324    0.05917  23.884  &lt; 2e-16 ***\nQuarterQ2   -20.62657    2.89298  -7.130 2.31e-09 ***\nQuarterQ3   -27.44818    2.89480  -9.482 3.62e-13 ***\nQuarterQ4   -24.20276    2.89298  -8.366 2.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.921 on 55 degrees of freedom\nMultiple R-squared:  0.9269,    Adjusted R-squared:  0.9216 \nF-statistic: 174.4 on 4 and 55 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_05.html#interpretation-of-the-model",
    "href": "class_05.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nThe trend that we can infer from the variable Period indicates a positive growth in revenue of US$ 1.4 billion for each increase in the periods.\nThe seasonal from the Quarter component indicates:\n\n\nQ2’s are expected to be $20.7 worse than Q1’s\nQ3’s are expected to be $27.4 worse than Q1’s\nQ4’s are expected to be $24.2 worse than Q1’s\nQ3’s are significantly worse than Q1’s\n\n\nThese effects are statistically significant (confint(lm_additive))\nThe RSE from this model is US$ 7.921 billions of dollars.\nHow can we interpret these results?"
  },
  {
    "objectID": "class_05.html#fitting-additive-model-1",
    "href": "class_05.html#fitting-additive-model-1",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line() +\n  geom_line(aes(x = Time, y = predict(lm_additive)), col = \"orange\")"
  },
  {
    "objectID": "class_05.html#fitting-additive-model-2",
    "href": "class_05.html#fitting-additive-model-2",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nWhat does the final model predict from the Quarter component indicates: for Apple in 2024 Q3?\n\n\npredict(lm_additive, list(Period = 61, Quarter = \"Q3\"), interval = \"prediction\")\n\n       fit      lwr     upr\n1 92.69571 75.86745 109.524\n\n\n\nThe actual revenue was US$ 85.78 billions\nWhat does the final model predict from the Quarter component indicates: for Apple in 2030 Q1? (Should we trust that prediction?)"
  },
  {
    "objectID": "class_05.html#fitting-additive-model-3",
    "href": "class_05.html#fitting-additive-model-3",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nThe residuals from this model show the “detrended and deasonalized” data (but there’s still some trend left!):\nWe hadn’t yet dealt with the time dependence\n\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line(aes(x = Time, y = residuals(lm_additive)))"
  },
  {
    "objectID": "class_05.html#autorgression-model",
    "href": "class_05.html#autorgression-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression model",
    "text": "Autorgression model\n\nHow we deal with the time dependence ? Key idea: Instead of predicting \\(Y_t\\) as a function of \\(t\\) (or other variables), predict \\(Y_t\\) as a function of \\(Y_{t-1}\\): \\[\nY_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\n\\]\n\\(Y_{t-1}\\) is called the “1st lag” of \\(Y\\)\nThis is called autoregressive (AR) because it predicts the values of a time series based on previous values\nThe model above is an AR(1) model\nWe can have AR(\\(p\\)) models, with lag \\(p\\)"
  },
  {
    "objectID": "class_05.html#autocorrelation",
    "href": "class_05.html#autocorrelation",
    "title": "Data Science for Business Applications",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation, is the correlation of \\(Y_t\\) with each of its lags \\(Y_t, Y_{t−1},\\dots\\) \\[\nCor(Y_t, Y_{t−1}), Cor(Y_t, Y_{t−2}),\\dots\n\\]\nWe also have the autocorrelation of the residuals, \\(r_t\\)’s, which indicates that there’s a strong indication that the independence assumption is violated \\[\nCor(r_t, r_{t−1}), Cor(r_t, r_{t−2}),\\dots\n\\]"
  },
  {
    "objectID": "class_05.html#ozone-example",
    "href": "class_05.html#ozone-example",
    "title": "Data Science for Business Applications",
    "section": "Ozone example",
    "text": "Ozone example\n\nCreating an AR(1) model: Daily ozone levels in Houston\n\n\nggplot(ozone, aes(x = day, y = ozone)) + \n  geom_line()"
  },
  {
    "objectID": "class_05.html#acf-plot",
    "href": "class_05.html#acf-plot",
    "title": "Data Science for Business Applications",
    "section": "ACF plot",
    "text": "ACF plot\n\nVisualizing the autocorrelation function (ACF)\n\n\nacf(ozone$ozone)\n\n\n\nAutocorrelations outside of the dashed blue lines are statistically significant."
  },
  {
    "objectID": "class_05.html#autorgression-of-the-model",
    "href": "class_05.html#autorgression-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression of the model",
    "text": "Autorgression of the model\n\nWe use the lag function to create the lagged observations\n\n\nozone &lt;- ozone %&gt;% \n  mutate(lag1=lag(ozone)) \nozone.model = lm(ozone ~ lag1, data=ozone) \nsummary(ozone.model)\n\n\nCall:\nlm(formula = ozone ~ lag1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.192  -3.464  -1.108   2.679  16.679 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.87446    1.06976   6.426 2.76e-09 ***\nlag1         0.40419    0.08381   4.823 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.999 on 120 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1624,    Adjusted R-squared:  0.1554 \nF-statistic: 23.26 on 1 and 120 DF,  p-value: 4.197e-06"
  },
  {
    "objectID": "class_05.html#assumptions-of-an-ar1-model",
    "href": "class_05.html#assumptions-of-an-ar1-model",
    "title": "Data Science for Business Applications",
    "section": "Assumptions of an AR(1) model",
    "text": "Assumptions of an AR(1) model\n\nLinearity, Normality, Equal Variance: Check using residual plot (linearity + homoscedasticity), Q-Q plot (normality), scale/location (homoscedasticity) like any other regression model\nIndependence: Since this is a time series, we can actually check this by looking at the autocorrelation of the residuals (we want no significant autocorrelation)"
  },
  {
    "objectID": "class_05.html#autoplot",
    "href": "class_05.html#autoplot",
    "title": "Data Science for Business Applications",
    "section": "Autoplot",
    "text": "Autoplot\n\nLinearity, Normality, Equal Variance\n\n\nautoplot(ozone.model)"
  },
  {
    "objectID": "class_05.html#acf-of-the-residuals",
    "href": "class_05.html#acf-of-the-residuals",
    "title": "Data Science for Business Applications",
    "section": "ACF of the residuals",
    "text": "ACF of the residuals\n\nacf(ozone.model$residuals)\n\n\n\nWe expect 5% of autocorrelations to be significant just by chance, so having just 1 out of the 20 lags flagged as significant indicates we are OK on independence!"
  },
  {
    "objectID": "class_05.html#making-predictions-in-time-series",
    "href": "class_05.html#making-predictions-in-time-series",
    "title": "Data Science for Business Applications",
    "section": "Making predictions in time series",
    "text": "Making predictions in time series\n\n\n\n\n\n\n\n\nType\nModel\nPredicted \\(Y_t\\)\n\n\n\n\nWhite noise\n\\(Y_t = e_t\\)\n\\(0\\)\n\n\nRandom sample\n\\(Y_t = \\beta_0 + e_t\\)\n\\(\\widehat{\\beta}_0\\) (or average \\(Y\\))\n\n\nRandom walk\n\\(Y_t = \\beta_0 + Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + Y_{t-1}\\)\n\n\nGeneral AR(1)\n\\(Y_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 Y_{t-1}\\)\n\n\n\n\nUnit root occurs when \\(\\beta_1 = 1\\). This means:\nThe series is a random walk.\nThere’s no mean reversion, and any shocks will have a permanent effect.\nWhen \\(\\beta_1 = 1\\), the model is non-stationary, meaning the series tends to “drift” without stabilizing around a fixed mean.\nIf \\(|\\beta_1| &lt; 1\\), the series is mean-reverting, and shocks are temporary."
  },
  {
    "objectID": "class_05.html#statistical-analysis",
    "href": "class_05.html#statistical-analysis",
    "title": "Data Science for Business Applications",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nconfint(ozone.model)\n\n                2.5 %    97.5 %\n(Intercept) 4.7564110 8.9925161\nlag1        0.2382561 0.5701286\n\n\n\nThe coefficient \\(\\widehat{\\beta}_1\\) is associated with the variable lag1.\nIn this case, for the larger population, with 95% confidence, \\(\\widehat{\\beta}_1\\) lies between 0.24 and 0.57.\nThis means that \\(|\\beta_1| &lt; 1\\), indicating that the series is mean-reverting."
  },
  {
    "objectID": "class_05.html#apple-revenue-acf-plot",
    "href": "class_05.html#apple-revenue-acf-plot",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the additive model."
  },
  {
    "objectID": "class_05.html#apple-revenue",
    "href": "class_05.html#apple-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nCombining decomposition and autoregression in a multiplicative model\n\n\\[\n\\log(\\texttt{Revenue}_t) = \\log(\\texttt{Period}_t) + \\texttt{Quarter}_t + \\log(\\texttt{Revenue}_{t-1})\n\\]\n\nWe need to create the lag variable.\nIt will have only one lag, and thus is an AR(1) model.\n\n\napple = apple %&gt;% \n  mutate(lag1 = lag(Revenue))"
  },
  {
    "objectID": "class_05.html#apple-revenue-1",
    "href": "class_05.html#apple-revenue-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nlog_apple = lm(log(Revenue) ~ log(Period) + Quarter + log(lag1), data = apple)\nsummary(log_apple)\n\n\nCall:\nlm(formula = log(Revenue) ~ log(Period) + Quarter + log(lag1), \n    data = apple)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.204851 -0.056602  0.005991  0.066084  0.193337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.14400    0.17945   6.375 4.56e-08 ***\nlog(Period)  0.20622    0.06918   2.981  0.00433 ** \nQuarterQ2   -0.53559    0.04911 -10.906 3.72e-15 ***\nQuarterQ3   -0.47076    0.03397 -13.859  &lt; 2e-16 ***\nQuarterQ4   -0.31872    0.03346  -9.526 4.47e-13 ***\nlog(lag1)    0.63410    0.10109   6.273 6.65e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09013 on 53 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9751,    Adjusted R-squared:  0.9728 \nF-statistic: 415.4 on 5 and 53 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_05.html#apple-revenue-predictions",
    "href": "class_05.html#apple-revenue-predictions",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nPredictions of multiplicative model"
  },
  {
    "objectID": "class_05.html#apple-revenue-predictions-1",
    "href": "class_05.html#apple-revenue-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nConfidence interval of the multiplicative model\n\n\nconfint(log_apple)\n\n                  2.5 %     97.5 %\n(Intercept)  0.78406737  1.5039420\nlog(Period)  0.06746219  0.3449861\nQuarterQ2   -0.63409896 -0.4370871\nQuarterQ3   -0.53888914 -0.4026276\nQuarterQ4   -0.38583509 -0.2516142\nlog(lag1)    0.43133359  0.8368601\n\n\n\nThe slope associated with lag is statistically significant, and its value is between minus and plus one; we have that this is a mean-reverting time series.\nWe also have a better fit (here we feed lag1 with prediction from the previous period, US$ 90.75 billions):\n\n\n exp(predict(log_apple, list(Period = 61, Quarter = \"Q3\", lag1 = 90.75), interval = \"prediction\"))\n\n       fit      lwr      upr\n1 79.80492 66.06926 96.39618\n\n\n\nThe confidence interval for the forecast is narrower, and the difference between what we observe and predict is smaller."
  },
  {
    "objectID": "class_05.html#apple-revenue-acf-plot-1",
    "href": "class_05.html#apple-revenue-acf-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the multiplicative model.\n\n\n\nThe independent assumptions look better, but it might be necessary to add more lags."
  },
  {
    "objectID": "class_05.html#time-series-strategy",
    "href": "class_05.html#time-series-strategy",
    "title": "Data Science for Business Applications",
    "section": "Time Series Strategy",
    "text": "Time Series Strategy\nTo building a time series model:\n\nStart with a an additive or multiplicative model with trend and seasonal components. (Plot your data! If the seasonal variation increases or decreases over time you’ll want a multiplicative model.)\nExamine the usual diagnostic plots, and plot your residuals as a function of time. Do you need a (different) nonlinear time trend? A transformation of \\(Y\\)?\nCheck your residuals for autocorrelation. If it’s present, add appropriate lag terms to your model."
  },
  {
    "objectID": "class_06.html#introduction-to-prediction",
    "href": "class_06.html#introduction-to-prediction",
    "title": "Data Science for Business Applications",
    "section": "Introduction to prediction",
    "text": "Introduction to prediction\n\nSo far, we have been focusing mostly on trying to explain the effects from the predictors \\(X\\) through the coefficients.\nUntil now, our focus was on the soundness of our model in relation to statistical significance and how well our model was fitting the data (Regression Assumptions).\nToday, we will focus on making models that return Estimate/predict outcomes with high accuracy without extrapolating, with previously unseen data."
  },
  {
    "objectID": "class_06.html#inference-and-prediction",
    "href": "class_06.html#inference-and-prediction",
    "title": "Data Science for Business Applications",
    "section": "Inference and Prediction",
    "text": "Inference and Prediction\n\nInference \\(\\rightarrow\\) focus on the predictor\nInterpretability of model\nPrediction \\(\\rightarrow\\) focus on outcome variable\nAccuracy of model"
  },
  {
    "objectID": "class_06.html#bias-vs.-variance",
    "href": "class_06.html#bias-vs.-variance",
    "title": "Data Science for Business Applications",
    "section": "Bias vs. Variance",
    "text": "Bias vs. Variance\n\nBias vs Variance trade-off\nVariance: The amount by which the function \\(f\\) would change if we estimated it using a different training dataset\nBias: Error introduced by approximating a real-life problem with a model\nMore flexible models have a higher variance and a lower bias\nLess flexible models have a lower variance but a higher bias\nValidation set approach: Training and testing data\nBalance between flexibility and accuracy"
  },
  {
    "objectID": "class_06.html#bias-vs.-variance-1",
    "href": "class_06.html#bias-vs.-variance-1",
    "title": "Data Science for Business Applications",
    "section": "Bias vs. Variance",
    "text": "Bias vs. Variance\n\nWhen explaining, bias is usually greater than variance\nIn prediction, we care about both\nMeasures of accuracy will have both bias and variance"
  },
  {
    "objectID": "class_06.html#measures-of-accuracy",
    "href": "class_06.html#measures-of-accuracy",
    "title": "Data Science for Business Applications",
    "section": "Measures of accuracy",
    "text": "Measures of accuracy\n\nHow do we measure accuracy?\nMean Squared Error (MSE): Can be decomposed into variance and bias terms \\[\n\\text{MSE} = \\text{Var} + \\text{Bias}^2 + \\text{Irreducible Error}\n\\] where MSE is equal to \\[\nMSE = \\frac{1}{n} \\sum_{i = 1}^n(y_i-\\widehat{y}_i)^2\n\\]\nRoot Mean Squared Error (RMSE): Measured in the same units as the outcome \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n\\]\nOther measures: Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC)"
  },
  {
    "objectID": "class_06.html#is-flexibility-always-better",
    "href": "class_06.html#is-flexibility-always-better",
    "title": "Data Science for Business Applications",
    "section": "Is flexibility always better?",
    "text": "Is flexibility always better?"
  },
  {
    "objectID": "class_06.html#measures-of-accuracy-1",
    "href": "class_06.html#measures-of-accuracy-1",
    "title": "Data Science for Business Applications",
    "section": "Measures of accuracy",
    "text": "Measures of accuracy\n\nModels with increasing flexibility (linear, cubic, spline).\nThink of a spline as a polynomial model with a high degree.\nRMSE decreases with flexibility in the training data.\nThe spline overfits the training data since the RMSE of the testing data is large."
  },
  {
    "objectID": "class_06.html#what-is-churn",
    "href": "class_06.html#what-is-churn",
    "title": "Data Science for Business Applications",
    "section": "What is churn?",
    "text": "What is churn?\n\n\n\nChurn: Measure of how many customers stop using your product (e.g. cancel a subscription).\nLess costly to keep a customer than bring a new one\nGoal: Prevent churn\nIdentify customer that are likely to cancel/quit/fail to renew"
  },
  {
    "objectID": "class_06.html#predicting-pre-churn",
    "href": "class_06.html#predicting-pre-churn",
    "title": "Data Science for Business Applications",
    "section": "Predicting “pre-churn”",
    "text": "Predicting “pre-churn”\n\nWe will predict “pre-churn”.\nAt a good measure for someone at risk of unsubscribing (“pre-churn”) is the times they’ve logged in the past week.\nWe are interested in the number of log ins in the variable logins.\nWe will predict logins from the other variable in the data.\nWe two candidates: Simple vs Complex"
  },
  {
    "objectID": "class_06.html#predicting-pre-churn-1",
    "href": "class_06.html#predicting-pre-churn-1",
    "title": "Data Science for Business Applications",
    "section": "Predicting “pre-churn”",
    "text": "Predicting “pre-churn”\n\nSimple Model: \\[\nlogins = \\beta_0 + \\beta_1 \\cdot Succession + \\beta_2 \\cdot city + \\epsilon\n\\]\nComplex Model: \\[\nlogins = \\beta_0 + \\beta_1 \\cdot Succession + \\beta_2 \\cdot age + \\beta_3 \\cdot age^2 + \\beta_4 \\cdot city + \\beta_5 \\cdot female + \\epsilon\n\\]\nCan we build more complex methods? Yes!\nFirst we will just analyse these two."
  },
  {
    "objectID": "class_06.html#create-validation-sets",
    "href": "class_06.html#create-validation-sets",
    "title": "Data Science for Business Applications",
    "section": "Create Validation Sets",
    "text": "Create Validation Sets\n\nCreate Training and Testing sets\nWe will use 75% of the data to train the data\nThe remaining part of the data, 25%, we reserve for testing\nThis split is done randomly\nTo do so we use the libraries modelr, and rsample\n\n\nlibrary(modelr) # for common model performance metrics\nlibrary(rsample)  # for creating train/test splits\n\nset.seed(100) #Always set seed for replication\nhbo_split =  initial_split(hbomax, prop=0.75)\nhbo_train = training(hbo_split)\nhbo_test  = testing(hbo_split)"
  },
  {
    "objectID": "class_06.html#rmse-in-training-and-testing-data",
    "href": "class_06.html#rmse-in-training-and-testing-data",
    "title": "Data Science for Business Applications",
    "section": "RMSE in training and testing data",
    "text": "RMSE in training and testing data\n\n# Simple Model\nlm_simple = lm(logins ~ succession + city, data = hbo_train)\n\n# Complex Model\nlm_complex = lm(logins ~ female + city + age + I(age^2) + succession, data = hbo_train)\n\n# Testing error for the simple model:\nrmse(lm_simple, hbo_test)\n\n[1] 2.075106\n\n# Testing error for the complex model:\nrmse(lm_complex, hbo_test)\n\n[1] 2.080211\n\n\n\nWhich model we should choose?\nThe model with the smallest out of sample error\nOut of sample means evaluation in the testing data"
  },
  {
    "objectID": "class_06.html#cross-validation",
    "href": "class_06.html#cross-validation",
    "title": "Data Science for Business Applications",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nTo avoid using only one training and testing dataset, we can iterate over k-fold division of our data:\n\n\n\nGrey: all of the data\nPink: Testing data\nYellow: Training data"
  },
  {
    "objectID": "class_06.html#cross-validation-1",
    "href": "class_06.html#cross-validation-1",
    "title": "Data Science for Business Applications",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nProcedure for k-fold cross-validation:\n\nDivide your data in k-folds (usually, \\(K = 5\\) or \\(K = 10\\)).\nUse as \\(k = 1\\) the testing data and \\(k = 2,3,\\dots, K\\) as the training data.\nCalculate the accuracy measure on the testing data, \\(RMSE_k\\).\nRepeat for each \\(k\\).\nAverage \\(RMSE_k\\) for all \\(k\\).\n\nMain advantage: Use the entire dataset for training AND testing."
  },
  {
    "objectID": "class_06.html#apple-quarterly-revenue",
    "href": "class_06.html#apple-quarterly-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple quarterly revenue",
    "text": "Apple quarterly revenue\n\nInstall the library caret\n\n\nlibrary(caret)\nset.seed(100)\ntrain.control = trainControl(method = \"cv\", number = 10)\n\nlm_simple = train(logins ~ succession + city, data = hbomax, method= \"lm\", trControl = train.control)\n\nlm_simple\n\nLinear Regression \n\n5000 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 4500, 4501, 4499, 4500, 4500, 4501, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  2.087314  0.6724741  1.639618\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "class_06.html#stepwise-selection",
    "href": "class_06.html#stepwise-selection",
    "title": "Data Science for Business Applications",
    "section": "Stepwise selection",
    "text": "Stepwise selection\n\nWe have seen how to choose between some given models. But what if we want to test all possible models?\nStepwise selection: Computationally-efficient algorithm to select a model based on the data we have (subset selection).\nAlgorithm for forward stepwise selection:\n\n\nStart with the null model, (no predictors)\nFor : (a) Consider all models that augment with one additional predictor. (b) Choose the best among these models and call it .\nSelect the single best model from using CV.\n\n\nBackwards stepwise follows the same procedure, but starts with the full model."
  },
  {
    "objectID": "class_06.html#stepwise-selection-and-cv",
    "href": "class_06.html#stepwise-selection-and-cv",
    "title": "Data Science for Business Applications",
    "section": "Stepwise selection and CV",
    "text": "Stepwise selection and CV\n\nset.seed(100)\n# Linear Regression with Forward Selection\n# Remove unsubscribe\ntrain.control = trainControl(method = \"cv\", number = 10) #set up a 10-fold cv\nlm.fwd = train(logins ~.- unsubscribe, data = hbomax, method = \"leapForward\", \n               tuneGrid = data.frame(nvmax = 1:5), trControl = train.control)\n\n\nlm.fwd$results\n\n  nvmax     RMSE    Rsquared      MAE     RMSESD  RsquaredSD      MAESD\n1     1 3.643876 0.001423859 3.168804 0.05856896 0.001837302 0.08173805\n2     2 3.643778 0.002541723 3.168174 0.06094142 0.003036447 0.08474783\n3     3 3.186594 0.206309738 2.719227 0.62445616 0.282844240 0.59591617\n4     4 2.580810 0.468546464 2.125469 0.62430763 0.278310925 0.59607716\n5     5 2.087951 0.672274342 1.640141 0.04906724 0.014296583 0.04888083\n\n\n\nWhich one would you choose out of the 5 models? Why?\nThe model with the smallest RMSE, which is model 5.\nCan we see this model?"
  },
  {
    "objectID": "class_06.html#stepwise-selection-and-cv-1",
    "href": "class_06.html#stepwise-selection-and-cv-1",
    "title": "Data Science for Business Applications",
    "section": "Stepwise selection and CV",
    "text": "Stepwise selection and CV\n\nAnd how does that model looks like:\n\n\nsummary(lm.fwd$finalModel)\n\nSubset selection object\n6 Variables  (and intercept)\n           Forced in Forced out\nX              FALSE      FALSE\nfemale         FALSE      FALSE\ncity           FALSE      FALSE\nage            FALSE      FALSE\nsuccession     FALSE      FALSE\nid             FALSE      FALSE\n1 subsets of each size up to 5\nSelection Algorithm: forward\n         X   id  female city age succession\n1  ( 1 ) \" \" \" \" \" \"    \" \"  \" \" \"*\"       \n2  ( 1 ) \" \" \" \" \" \"    \"*\"  \" \" \"*\"       \n3  ( 1 ) \" \" \" \" \" \"    \"*\"  \"*\" \"*\"       \n4  ( 1 ) \" \" \" \" \"*\"    \"*\"  \"*\" \"*\"       \n5  ( 1 ) \"*\" \" \" \"*\"    \"*\"  \"*\" \"*\"       \n\n\n\nThe selected model has the following variables:\nfemale, city, age, succession, id"
  },
  {
    "objectID": "class_06.html#conclusion",
    "href": "class_06.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn prediction, everything is going to be about:\nBias vs Variance\nImportance of validation sets\nWe now have methods to select models"
  },
  {
    "objectID": "class_06.html",
    "href": "class_06.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "So far, we have been focusing mostly on trying to explain the effects from the predictors \\(X\\) through the coefficients.\nUntil now, our focus was on the soundness of our model in relation to statistical significance and how well our model was fitting the data (Regression Assumptions).\nToday, we will focus on making models that return Estimate/predict outcomes with high accuracy without extrapolating, with previously unseen data."
  },
  {
    "objectID": "class_07.html#quick-review-of-our-class",
    "href": "class_07.html#quick-review-of-our-class",
    "title": "Data Science for Business Applications",
    "section": "Quick review of our Class",
    "text": "Quick review of our Class\nThis is what we covered in previous classes:\n\nSimple and Multiple Regression\nCategorical Variables and Interactions\nResidual Analysis\nNonlinear Transformations\nTime Series\nModel Selection\n\nToday we will introduce a new model."
  },
  {
    "objectID": "class_07.html#the-okcupid-data-set",
    "href": "class_07.html#the-okcupid-data-set",
    "title": "Data Science for Business Applications",
    "section": "The OkCupid data set",
    "text": "The OkCupid data set\n\n\n\nThe OkCupid data set contains information about 59826 profiles from users of the OkCupid online dating service.\nWe have data on user age, height, sex, income , sexual orientation, education level, body type , ethnicity, and more.\nLet’s see if we can predict the sex of the user based on their height. (In this data set, everyone is classified as male or female.)"
  },
  {
    "objectID": "class_07.html#lets-build-the-model",
    "href": "class_07.html#lets-build-the-model",
    "title": "Data Science for Business Applications",
    "section": "Let’s build the model",
    "text": "Let’s build the model\n\nWhat’s wrong with this regression?\n\n\\[\n\\widehat{\\text{sex}} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_{0} \\cdot \\text{height}\n\\]\n\nThe \\(Y\\) variable here is categorical (two levels—everyone in this data set is either labeled male or female), so regular linear regression won’t work here.\nBut what if we just do it anyway?"
  },
  {
    "objectID": "class_07.html#binary-variable",
    "href": "class_07.html#binary-variable",
    "title": "Data Science for Business Applications",
    "section": "Binary Variable",
    "text": "Binary Variable\n\nLet’s first create a dummy variable male to convert sex to a quantitative dummy variable:\n\n\nlibrary(tidyverse)\nokcupid = okcupid %&gt;% \n  mutate(male = ifelse(okcupid$sex == \"m\", 1, 0))\n\n\nWe could do this with 1 representing either male or female (it wouldn’t matter)."
  },
  {
    "objectID": "class_07.html#regular-linear-regression",
    "href": "class_07.html#regular-linear-regression",
    "title": "Data Science for Business Applications",
    "section": "Regular Linear Regression",
    "text": "Regular Linear Regression\n\nggplot(okcupid, aes(x=height, y = male)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se = F)\n\n\n\nA line is not a great fit to this data—it’s not even close to linear. And what does it mean to predict that male = 0.7 (or 1.2)?"
  },
  {
    "objectID": "class_07.html#logistic-regression",
    "href": "class_07.html#logistic-regression",
    "title": "Data Science for Business Applications",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nInstead of predicting whether someone is male, let’s predict the probability that they are male\nIn logistic regression, one level of \\(Y\\) is always called “success” and the other called “failure.” Since \\(Y = 1\\) for males, in our setup we have designated males as “success.” (You could also set \\(Y = 1\\) for females and call females “success.”)\nLet’s fit a curve that is always between 0 and 1."
  },
  {
    "objectID": "class_07.html#odds-and-probabilty",
    "href": "class_07.html#odds-and-probabilty",
    "title": "Data Science for Business Applications",
    "section": "Odds and Probabilty",
    "text": "Odds and Probabilty\n\nTo fit the Logistic regression model we need to know the difference between odds and probability and how they relate.\nWhen something has “even (1/1) odds,” the probability of success is 1/2.\nWhen something has “2/1 odds,” the probability of success is 2/3.\nWhen something has “3/2 odds,” the probability of success is 3/5.\nIn general, the odds of something happening are \\(p/(1 − p)\\).\nWhere \\(p\\) is the probability defined bewteen zero and one.\nYou can transform odds to probability: \\[\n\\text{Odds} = \\frac{3}{2} = \\frac{3/(3+2)}{2/(3+2)} = \\frac{3/5}{2/5}  = \\frac{p}{1-p}\n\\]\nIf the odds are between zero and one they are not in your favor, \\((1-p)&gt;p\\)\nLet’s the explore this relation!"
  },
  {
    "objectID": "class_07.html#probability-vs-odds-vs-log-odds",
    "href": "class_07.html#probability-vs-odds-vs-log-odds",
    "title": "Data Science for Business Applications",
    "section": "Probability vs odds vs log odds",
    "text": "Probability vs odds vs log odds\n\n\n\nProbability \\(p\\)\nOdds \\(p/(1 − p)\\)\nLog odds \\(\\log(p/(1 − p))\\)\n\n\n\n\n0\n0\n\\(-\\infty\\)\n\n\n0.25\n0.33\n−1.10\n\n\n0.5\n1\n0\n\n\n0.75\n3\n1.10\n\n\n0.8\n4\n1.39\n\n\n0.9\n9\n2.20\n\n\n0.95\n19\n2.94\n\n\n1\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n\nProbability is between zero and one.\nOdds are strictly positive (greater than zero).\nLog odds ranges the whole real line."
  },
  {
    "objectID": "class_07.html#the-logistic-regression-model",
    "href": "class_07.html#the-logistic-regression-model",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nLogistic regression models the log odds of success \\(p\\) as a linear function of \\(X\\): \\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot X + e\n\\]\nThis fits an “S-shaped” curve to the data\nWe’ll see what it looks like later\nBy making this choice, we have a series of benefits.\nLet’s try it!"
  },
  {
    "objectID": "class_07.html#the-logistic-regression-model-1",
    "href": "class_07.html#the-logistic-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nWe need a different function - glm() (generalized linear models)\n\n\nmodel &lt;- glm(male ~ height, data = okcupid, family = binomial)\nsummary(model)\n\n\nCall:\nglm(formula = male ~ height, family = binomial, data = okcupid)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -44.448609   0.357510  -124.3   &lt;2e-16 ***\nheight        0.661904   0.005293   125.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 80654  on 59825  degrees of freedom\nResidual deviance: 44637  on 59824  degrees of freedom\nAIC: 44641\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nHow ca we interpret this model?"
  },
  {
    "objectID": "class_07.html#the-logistic-regression-model-2",
    "href": "class_07.html#the-logistic-regression-model-2",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nThe logistic regression output tells us that our prediction is \\[\n\\log(\\text{odds}) = \\log\\left(\\frac{\\widehat{p(\\text{male})}}{1-\\widehat{p(\\text{male})}}\\right) = −44.45 + 0.66 \\cdot \\text{height}\n\\]\nTo get the probability we have to solve in terms \\(\\widehat{p(\\text{male})}\\)\nThe probability of being male given height: \\[\n\\widehat{p(\\text{male})} = \\frac{\\exp(−44.45 + 0.66 \\cdot \\text{height})}{1+ \\exp(−44.45 + 0.66 \\cdot \\text{height})}\n\\] where \\(\\exp()\\) is the exponential function \\(e^x\\)."
  },
  {
    "objectID": "class_07.html#lets-show-this",
    "href": "class_07.html#lets-show-this",
    "title": "Data Science for Business Applications",
    "section": "Let’s show this",
    "text": "Let’s show this\nLet \\(\\widehat{p} = \\widehat{p(\\text{male})}\\), and \\(\\exp(X\\widehat{\\beta}) = \\exp(−44.45 + 0.66 \\cdot \\text{height})\\):\n\\[\n\\begin{eqnarray}\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right) &=& X\\widehat{\\beta} \\\\\n\\exp\\left(\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\\right) &=& \\exp(X\\widehat{\\beta}) \\\\\n\\frac{\\widehat{p}}{1-\\widehat{p}} &=& \\exp(X\\widehat{\\beta})\\\\\n\\widehat{p} &=& \\exp(X\\widehat{\\beta})\\cdot (1-\\widehat{p})\\\\\n\\widehat{p} &=&\\exp(X\\widehat{\\beta}) - \\exp(X\\widehat{\\beta}) \\cdot \\widehat{p} \\\\\n\\widehat{p} &=& \\frac{\\exp(X\\widehat{\\beta})}{1 + \\exp(X\\widehat{\\beta})}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "class_07.html#visualizing-the-model",
    "href": "class_07.html#visualizing-the-model",
    "title": "Data Science for Business Applications",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\n\nHow to interpret this curve?\nThe blue line is \\(\\widehat{p(\\text{male})}\\), given the height."
  },
  {
    "objectID": "class_07.html#interpreting-the-coefficients",
    "href": "class_07.html#interpreting-the-coefficients",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\nOur prediction equation is:\n\\[\n\\log(\\text{odds}) = \\log\\left(\\frac{\\widehat{p(\\text{male})}}{1-\\widehat{p(\\text{male})}}\\right) = −44.45 + 0.66 \\cdot \\text{height}\n\\]\nLet’s start with some basic, but not particularly useful, interpretations:\n\nWhen height = 0, we predict that the log odds will be -44.45 , so the probability of male is predicted to be very close to 0%.\nWhen height increases by 1 inch, we predict that the log odds of being male will increase by 0.66.\nInstead of log odds is better to have the interpretation in odds."
  },
  {
    "objectID": "class_07.html#interpreting-the-coefficients-1",
    "href": "class_07.html#interpreting-the-coefficients-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nLet’s rewrite the prediction equation as:\nPredicted odds of male, \\(\\exp(−44.45 + 0.66 \\cdot \\text{height})\\).\nIncreasing height by 1 inch will multiply the odds by \\(\\exp(0.66) = 1.94\\); i.e., increase the odds by 94%.\nIn summary, \\[(\\exp(\\widehat{\\beta}) - 1)\\times 100 = \\text{percentage change in odds}.\\]\nIncreasing height by 2 inches will multiply the odds by \\(\\exp(2\\cdot0.66) = 3.76\\); i.e., increase the odds by 276%.\nOdds equal to 1 indicate an one-to-one chance."
  },
  {
    "objectID": "class_07.html#making-predictions",
    "href": "class_07.html#making-predictions",
    "title": "Data Science for Business Applications",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is the probability of being male given we have a height of 69.\n\n\npredict(model, list(height=69), type=\"response\")\n\n        1 \n0.7725447 \n\n\n\nUsing the probability equation in R:\n\n\\[\n\\widehat{p(\\text{male})} = \\frac{\\exp(−44.45 + 0.66 \\cdot 69)}{1+ \\exp(−44.45 + 0.66 \\cdot 69)} = 0.77\n\\]\n\nexp(-44.448609 + 0.661904*69)/(1+exp(-44.448609 + 0.661904*69))\n\n[1] 0.7725501"
  },
  {
    "objectID": "class_07.html#adding-more-predictors",
    "href": "class_07.html#adding-more-predictors",
    "title": "Data Science for Business Applications",
    "section": "Adding more predictors",
    "text": "Adding more predictors\n\nAdding another predictor: can we do better?\nJust like with a linear regression model, we can add additional predictors to the model.\nOur interpretation of the coefficients in multiple logistic regression is similar to multiple linear regression, in the sense that each coefficient represents the predicted effect of one \\(X\\) on \\(Y\\), holding the other \\(X\\) variables constant."
  },
  {
    "objectID": "class_07.html#how-good-is-our-model",
    "href": "class_07.html#how-good-is-our-model",
    "title": "Data Science for Business Applications",
    "section": "How good is our model?",
    "text": "How good is our model?\n\nUnfortunately, the typical root mean squared error, RSE metric isn’t available for logistic regression.\nHowever, there are many metrics that indicate model fit.\nBut: most of these metrics are difficult to interpret, so we’ll focus on something simpler to interpret and communicate."
  },
  {
    "objectID": "class_07.html#accuracy-of-the-model",
    "href": "class_07.html#accuracy-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\n\nWe could use our model to make a prediction of sex based on the probability.\nSuppose we say that our prediction is: \\[\n\\text{Prediction} = \\begin{cases}\n\\text{male}, & \\text{if $\\widehat{P(\\text{male})} \\geq 0.5$}, \\\\\n\\text{female}, & \\text{if $\\widehat{P(\\text{male})} &lt; 0.5$}. \\\\\n\\end{cases}\n\\]\nGiven a threshold of 0.5, now we can compute the fraction of individuals whose sex we correctly predicted.\nFor males and females.\nThis is known as the accuracy of the model."
  },
  {
    "objectID": "class_07.html#accuracy-of-the-model-1",
    "href": "class_07.html#accuracy-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\n\nWe can use the xtabs function to get the accuracy:\nWe add we the number of correctly predicted groups for both male and female, and divide by the total of observations.\n\n\nokcupid = okcupid %&gt;% \n  mutate(predict.sex = ifelse(predict(model, type=\"response\") &gt;= 0.5,\"m\",\"f\"))\nxtabs(~ predict.sex + sex,okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nCorrectly predicted that is female - 19466\nCorrectly predicted that is male - 30243\nTotal number of individuals in the sample - 59826\nThe accuracy is (19466 + 30243)/59826 = 0.831, or 83%"
  },
  {
    "objectID": "class_07.html#confusion-matrix",
    "href": "class_07.html#confusion-matrix",
    "title": "Data Science for Business Applications",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nThe table from xtabs is also called a Confusion matrix\n\n\n\n\n\nActual failure\nActual success\n\n\n\n\nModel predicts failure\nTrue negative\nFalse negative\n\n\nModel predicts success\nFalse positive\nTrue positive\n\n\n\n\nTrue positives: predicting male for someone that is male\nTrue negatives: predicting female for someone that is female\nFalse positives: predicting male for someone that is female\nFalse negatives: predicting female for someone that is male\nIf we had designated female as 1 and male as 0, these would have switched\nSo Accuracy = (True negative + True positive)/(Total cases)"
  },
  {
    "objectID": "class_07.html#accuracy-of-the-model-2",
    "href": "class_07.html#accuracy-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\nSuppose that the Amazon is trying to build a model to predict which costumers buy a certain product:\n\nSuppose that 0.01% of people are costumers of this product (the product is really expensive / high revenue)\nA “null” or “no-brainer” model that predicts that no one is a costumer will be 99.99% accurate.\nThe revenue coming from our model would be zero.\nBut the model could make two different kinds of prediction errors:\nFalse positive: predicting someone is a customer when they really are not\nFalse negative: predicting someone is not a customer when they really are\nThese two measures give us a better idea of the predictive power of our model."
  },
  {
    "objectID": "class_07.html#false-positive-rate",
    "href": "class_07.html#false-positive-rate",
    "title": "Data Science for Business Applications",
    "section": "False positive rate",
    "text": "False positive rate\nThe false positive rate is the proportion of actual failures where the model predicted success.\n\nxtabs(~ predict.sex + sex, okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nFalse Positives - predicting someone is a male when they really are female\nActual failure - number of cases that are female\nFalse positive rate = False positives/ Actual failure\nIn our model, the false positive rate is 4623/24089 = 0.19"
  },
  {
    "objectID": "class_07.html#false-negative-rate",
    "href": "class_07.html#false-negative-rate",
    "title": "Data Science for Business Applications",
    "section": "False negative rate",
    "text": "False negative rate\nThe false negative rate is the proportion of actual successes where the model predicted failure.\n\nxtabs(~ predict.sex + sex, okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nFalse Negatives - predicting someone is a female when they really are male\nActual success - number of cases that are male\nFalse negative rate = False negatives/ Actual success\nIn our model, the false positive rate is 5494/35737 = 0.15"
  },
  {
    "objectID": "class_07.html#changing-rates",
    "href": "class_07.html#changing-rates",
    "title": "Data Science for Business Applications",
    "section": "Changing rates",
    "text": "Changing rates\nHow do we reduce false positive/negative rates?\n\nInstead of using 50% as a cutoff probability to decide when to predict success, use a higher (or lower) probability.\nFor example, we could have the model predict that someone is male only if \\(\\widehat{p(\\text{male})} \\geq 0.8\\), instead of 0.5:\n\n\nokcupid = okcupid %&gt;% \n  mutate(predict.sex = ifelse(predict(model, type=\"response\") &gt;= 0.8,\"m\",\"f\"))\nxtabs(~ predict.sex + sex,okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   22650 12938 35588\n        m    1439 22799 24238\n        Sum 24089 35737 59826\n\n\n\nAccuracy = (21425+26753)/59826 = 0.76\nFalse positive rate = 2664/24089 = 0.06\nFalse negative rate = 8984/59826 = 0.36"
  },
  {
    "objectID": "class_07.html#prediction-trade-off",
    "href": "class_07.html#prediction-trade-off",
    "title": "Data Science for Business Applications",
    "section": "Prediction Trade off",
    "text": "Prediction Trade off\n\nWe can decrease the false positive rate, but at the expense of increasing the false negative rate.\nOr we can decrease the false negative rate, but at the expense of increasing the false positive rate.\nWe might choose a cutoff probability other than 50% based on our assessment of the relative costs of the two different kinds of errors."
  },
  {
    "objectID": "class_07.html#summary",
    "href": "class_07.html#summary",
    "title": "Data Science for Business Applications",
    "section": "Summary",
    "text": "Summary\nIn a logistic regression model, the response variable is binary, taking values of either zero or one.\n\nThe model estimates the log odds of the event associated with a response of one.\nThe model’s effects are interpreted in terms of odds.\nPredictions are expressed as probabilities.\nThe performance of the model is evaluated based on its prediction accuracy."
  },
  {
    "objectID": "class_07.html",
    "href": "class_07.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "This is what we covered in previous classes:\n\nSimple and Multiple Regression\nCategorical Variables and Interactions\nResidual Analysis\nNonlinear Transformations\nTime Series\nModel Selection\n\nToday we will introduce a new model."
  },
  {
    "objectID": "class_08.html#what-is-causal-inference",
    "href": "class_08.html#what-is-causal-inference",
    "title": "Data Science for Business Applications",
    "section": "What is Causal Inference?",
    "text": "What is Causal Inference?"
  },
  {
    "objectID": "class_08.html#early-key-ideas",
    "href": "class_08.html#early-key-ideas",
    "title": "Data Science for Business Applications",
    "section": "Early key ideas",
    "text": "Early key ideas\nInformal review on causes and effects\n\nFrancis Bacon (1561-1626) was talking about “control” and specifically “controlled experiments”.\nDavid Hume (1711-1776) was worried about “confounding”. Correlation is not causation.\nJohn Stuart Mill (1806-1873) Mill was focusing on “contrasts”.\nCan we measure causality from what we observe?"
  },
  {
    "objectID": "class_08.html#causality",
    "href": "class_08.html#causality",
    "title": "Data Science for Business Applications",
    "section": "Causality",
    "text": "Causality\nCauses of effects: Given an outcome, what were its causes?\n\nA patient has a headache. Why?\nA city experiences a crime wave. Why?\nThe stock market is down today. Why?\n\nEffects of causes: Given a cause, what was its effect?\n\nThe patient took an aspirin. Did it mitigate the severity or duration of their headache?\nWhat is the impact of police presence on crime rates?\nHow much is the coronovirus affecting the stock market?\n\nWe will focus exclusively on the latter – measuring effects – in order to avoid the ill-posedness of multiple causes. Inferring the effect of one thing on another thing."
  },
  {
    "objectID": "class_08.html#measuring-the-causal-effect",
    "href": "class_08.html#measuring-the-causal-effect",
    "title": "Data Science for Business Applications",
    "section": "Measuring the causal effect",
    "text": "Measuring the causal effect\nHow do measure this causal effect?\n\nCounterfactual comparison\n\nSuppose we are measuring the effect of college degree on income:\n\n\\(Z_i \\in \\{0,1\\}\\) is a binary variable indicating if a person went to college.\n\\(Y_{i}(1)\\) is the income of person \\(i\\) if they went to college, i.e., when \\(Z_i = 1\\).\n\\(Y_{i}(0)\\) is the income of person \\(i\\) if they did not go to college, i.e., when \\(Z_i = 0\\)\n\nThe causal effect of the college degree on income is: \\[\n\\text{causal effect of college on income} = Y_{i}(1)- Y_{i}(0)\n\\] - This framework is known as the Potential Outcomes approach to causal inference. - This framework is also known as the Neyman-Rubin model (1974)."
  },
  {
    "objectID": "class_08.html#potential-outcomes",
    "href": "class_08.html#potential-outcomes",
    "title": "Data Science for Business Applications",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\nIn the Potential Outcomes framework, we aim to compare each unit to the alternative reality, that is, the counterfactual, where they received the opposite treatment assignment from what they actually received.\nIn the previous case:\n\n\\(Y_{i}(1)\\) is the counterfactual of \\(Y_{i}(0)\\), and vice versa.\nBoth are potential outcomes of income given the treatment \\(Z_i\\).\nThe causal effect is the difference between the potential outcomes.\n\\(Y_{i}(1)\\) is referred to as the treatment group.\n\\(Y_{i}(0)\\) is referred to as the control group.\nThe observations \\(Y_i\\) can be rewritten in terms of the potential outcomes as: \\[Y_i = Z_i \\cdot Y_i(1) + (1 − Z_i) ⋅ Y_i(0)\\]\nWhat is the problem with this framework?"
  },
  {
    "objectID": "class_08.html#potential-outcomes-1",
    "href": "class_08.html#potential-outcomes-1",
    "title": "Data Science for Business Applications",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nHolland (1986) defines in “Statistics and Causal Inference” the “fundamental problem” of causal inference.\nTo determine the causal effect, we must observe both \\(Y_{i}(1)\\) and \\(Y_{i}(0)\\), but we only get to see one of the two!\nThis means that either person \\(i\\) went to college or they did not. We cannot have both scenarios happening at the same time.\nHow can we deal with this problem?"
  },
  {
    "objectID": "class_08.html#average-treatment-effect",
    "href": "class_08.html#average-treatment-effect",
    "title": "Data Science for Business Applications",
    "section": "Average treatment effect",
    "text": "Average treatment effect\n\nInstead of measuring the effect on an individual unit, we aim to estimate the average treatment effect (ATE) of going to college across several individuals in a population of interest. At the population level, we have: \\[\n\\text{Population ATE} = E[Y_{i}(1) - Y_{i}(0)]\n\\]\nHere, \\(E[X_i]\\) is the population average, or better known as the expected value of the random variable \\(X_i\\).\nBut this is on the population level we can’t infer nothing yet.\nAlso, we still have the problem of only observing one possible outcome \\(Y_{i}(1)\\) or \\(Y_{i}(0)\\)."
  },
  {
    "objectID": "class_08.html#estimating-ate",
    "href": "class_08.html#estimating-ate",
    "title": "Data Science for Business Applications",
    "section": "Estimating ATE",
    "text": "Estimating ATE\n\nWe can estimate the sample ATE as:\n\n\\[\n\\text{Sample ATE} = E[Y_{i}(1)] - E[Y_{i}(0)] + \\text{bias}\n\\]\n\nThis bias can be eliminated if we adopt certain assumptions, such as the independence of treatment assumption (also called ignorability), which ensures that the treatment assignment is independent of the potential outcomes.\nIn our example, this would mean that the potential outcomes, \\(Y_{i}(1)\\) and \\(Y_{i}(0)\\), are independent of whether a person chooses to go to college (i.e., the treatment assignment \\(Z\\))."
  },
  {
    "objectID": "class_08.html#sample-ate",
    "href": "class_08.html#sample-ate",
    "title": "Data Science for Business Applications",
    "section": "Sample ATE",
    "text": "Sample ATE\nSuppose we have \\(N\\) observations of incomes \\(Y_i\\) from individuals who did and did not go to college. \\(N_1\\) is the number of individuals who went to college, and \\(N_0\\) is the number of individuals who did not go to college, so \\(N = N_0 + N_1\\).\nFrom the sample ATE, we have: \\[E[Y_{i}(1)] = \\frac{1}{N_1}\\sum_{i = 1}^{N_1} (Y_i \\text{ given } Z = 1)\\] which is the sample average of the outcome for individuals who received the treatment (i.e., went to college). Similarly, we have: \\[E[Y_{i}(0)] = \\frac{1}{N_0}\\sum_{i = 1}^{N_0} (Y_i \\text{ given } Z = 0)\\] which is the sample average of the outcome for individuals who did not receive the treatment (i.e., did not go to college)."
  },
  {
    "objectID": "class_08.html#sample-ate-1",
    "href": "class_08.html#sample-ate-1",
    "title": "Data Science for Business Applications",
    "section": "Sample ATE",
    "text": "Sample ATE\n\nSo the ATE is giving by: \\[\n\\text{Sample ATE} = \\frac{1}{N_1}\\sum_{i = 1}^{N_1} (Y_i\\text{ given } Z = 1) - \\frac{1}{N_0}\\sum_{i = 1}^{N_0} (Y_i\\text{ given } Z = 0)\n\\] Which can be rewritten in a more friendly way as \\[\n\\text{Sample ATE} = \\text{mean of the treated} - \\text{mean of the control}\n\\]"
  },
  {
    "objectID": "class_08.html#example",
    "href": "class_08.html#example",
    "title": "Data Science for Business Applications",
    "section": "Example",
    "text": "Example\n\nIn this example we have sample of \\(N = 6\\) students that went to the college, \\(i = 1,2,\\dots,6\\).\nThree of the individuals went to college, \\(N_1 = 3\\), and the other three did not, \\(N_0 = 3\\).\n\\(Y_i\\) are the observed incomes, \\(Y_i(1)\\), and \\(Y_i(0)\\) are the potential outcomes.\n\\(Z_i\\in \\{0,1\\}\\) is the treatment.\n\n\n\n\n\\(i\\)\n\\(Z_i\\)\n\\(Y_i\\)\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(Y_i(1) - Y_i(0)\\)\n\n\n\n\n1\n0\n50.000\n?\n50.000\n?\n\n\n2\n1\n55.000\n55.000\n?\n?\n\n\n3\n1\n120.000\n120.000\n?\n?\n\n\n4\n0\n150.000\n?\n150.000\n?\n\n\n5\n0\n45.000\n?\n45.000\n?\n\n\n6\n1\n130.000\n130.000\n?\n?"
  },
  {
    "objectID": "class_08.html#sample-ate-2",
    "href": "class_08.html#sample-ate-2",
    "title": "Data Science for Business Applications",
    "section": "Sample ATE",
    "text": "Sample ATE\n\nThe estimate of the ATE is giving by\n\n\\[\\begin{eqnarray}\n\\text{Sample ATE} &=& \\text{mean of the treated} - \\text{mean of the control} \\\\\n                  &=& \\frac{305,000}{3} - \\frac{245,000}{3} \\\\\n                  &=& 20,000\n\\end{eqnarray}\\]\n\nOn average, going to college has a positive causal effect of US$ 20,000 on income compared to those who do not go to college.\nAgain, this conclusion was made under very strong assumptions.\nWe will discuss these assumptions in the next class.\nWe are essentially assuming that the treatment is random, meaning that attending college or not was randomly assigned. This is not a reasonable assumption in most cases."
  },
  {
    "objectID": "class_09.html#potential-outcomes",
    "href": "class_09.html#potential-outcomes",
    "title": "Data Science for Business Applications",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nLast week we discussed potential outcomes., (e.g. \\(Y_i(1)\\) and \\(Y_i(0)\\)):\n“The outcome that we would have observed under different scenarios”\nPotential outcomes are related to your choices/possible conditions:\nOne for each path (Counterfactuals).\nDo not confuse them with the values that your outcome variable can take.\nDefinition of Causal Effect for individual \\(i\\): \\[\n\\text{causal effect for an individual} = Y_{i}(1)- Y_{i}(0)\n\\]\nBetter to assume for a population (Difference in means) \\[\n\\text{ATE} = E\\left[Y_{i}(1)- Y_{i}(0)\\right] = E\\left[Y_{i}(1)\\right] - E\\left[Y_{i}(0)\\right]\n\\]"
  },
  {
    "objectID": "class_09.html#causal-effect",
    "href": "class_09.html#causal-effect",
    "title": "Data Science for Business Applications",
    "section": "Causal effect",
    "text": "Causal effect\n\nFor a sample:\n\n\\[\n\\text{Average} [Y_{i}(1)- Y_{i}(0)] = \\text{mean of the treated} - \\text{mean of the untreated}\n\\]\n\nUnder what assumptions is our estimate causal?\nKey assumption: Ignorability means that the potential outcomes \\(Y_i(0)\\) and \\(Y_i(1)\\) are independent of the treatment.\nIn our example this means that the decision to pursue a college degree should not be related to unmeasured factors that could influence income.\nIn reality, this assumption can be difficult to fully satisfy. There could be unobserved factors, such as intrinsic ability or motivation, that affect both the likelihood of obtaining a college degree and future income, leading to potential confounding.\nWhat can we do to make the ignorability assumption hold?"
  },
  {
    "objectID": "class_09.html#randomization",
    "href": "class_09.html#randomization",
    "title": "Data Science for Business Applications",
    "section": "Randomization",
    "text": "Randomization\nOne way to make sure the ignorability assumption holds is to do it by design:\n\nRandomize the assignment of the treatment \\(Z\\)\ni.e. Some units will randomly be chosen to be in the treatment group and others to be in the control group.\nWhat does randomization buy us?\nControl for unforeseen factors (confounders)"
  },
  {
    "objectID": "class_09.html#confounders",
    "href": "class_09.html#confounders",
    "title": "Data Science for Business Applications",
    "section": "Confounders",
    "text": "Confounders\n\nConfounder is a variable that affects both the treatment AND the outcome"
  },
  {
    "objectID": "class_09.html#confounders-1",
    "href": "class_09.html#confounders-1",
    "title": "Data Science for Business Applications",
    "section": "Confounders",
    "text": "Confounders\nLet’s identify some confounders\n\nEstimate the effect of insurance vs no insurance on number of accidents \\(\\rightarrow\\) Compare people with insurance vs people without insurance.\nConfounder: (Driving Behavior/Risk Aversion) Risk-averse individuals are more likely to purchase insurance and may also drive more cautiously, reducing their number of accidents.\nEstimate the effect of gym membership vs no gym membership on physical health \\(\\rightarrow\\) Compare people with gym memberships vs people without gym memberships.\nConfounder: (Motivation for Fitness) Individuals who are more motivated to improve their health are more likely to purchase a gym membership and are also more likely to engage in other healthy behaviors, such as maintaining a balanced diet, which improves their physical health."
  },
  {
    "objectID": "class_09.html#randomization-1",
    "href": "class_09.html#randomization-1",
    "title": "Data Science for Business Applications",
    "section": "Randomization",
    "text": "Randomization\n\nDue to randomization, we know that the treatment is not affected by a confounder\n\n\n\nWe have “clean effect” of the treatment on the outcome\nThis would be the causal effect of the treatment"
  },
  {
    "objectID": "class_09.html#randomized-controlled-trials-rcts",
    "href": "class_09.html#randomized-controlled-trials-rcts",
    "title": "Data Science for Business Applications",
    "section": "Randomized controlled trials (RCTs)",
    "text": "Randomized controlled trials (RCTs)\n\nOften called the “gold standard” for establishing causality.\nRandomly assign the \\(Z\\), “treatment”, to participants\nNow, any observed relationship between \\(Z\\) and \\(Y\\) must be due to \\(Z\\), since the only reason an individual had a particular value of \\(X\\) was the random assignment."
  },
  {
    "objectID": "class_09.html#randomized-controlled-trials-rcts-1",
    "href": "class_09.html#randomized-controlled-trials-rcts-1",
    "title": "Data Science for Business Applications",
    "section": "Randomized controlled trials (RCTs)",
    "text": "Randomized controlled trials (RCTs)"
  },
  {
    "objectID": "class_09.html#rct---steps",
    "href": "class_09.html#rct---steps",
    "title": "Data Science for Business Applications",
    "section": "RCT - Steps",
    "text": "RCT - Steps\n\nCheck for balance\n\n\n(We will see what this is about)\n\n\nRandomize\nCalculate difference in sample means between treatment and control group"
  },
  {
    "objectID": "class_09.html#section",
    "href": "class_09.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Example 1: Clinical Trial for the Moderna COVID-19 vaccine\nRandomly assign study participants to get either the vaccine:\n\nan treatment group of 14,134 people\ncontrol group, the same size\nResults: 11 vaccine recipients got COVID; 235 of placebo recipients got COVID\n\n\nlibrary(mosaic)\n\n# Control and treatment group \n\n# Difference in proportions\nprop.test(outcome ~ treatment, data = data.rct, success = 1)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  tally(outcome ~ treatment)\nX-squared = 215.01, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.01435140 0.01890174\nsample estimates:\n      prop 1       prop 2 \n0.0174048394 0.0007782652"
  },
  {
    "objectID": "class_09.html#issues-with-rct",
    "href": "class_09.html#issues-with-rct",
    "title": "Data Science for Business Applications",
    "section": "Issues with RCT",
    "text": "Issues with RCT\n\nInternal validity is the ability of an experiment to establish cause-and-effect of the treatment within the sample studied.\nExamples of threats to internal validity:\nFailure to randomize.\nFailure to follow the treatment protocol/attrition.\nSmall sample sizes"
  },
  {
    "objectID": "class_09.html#issues-with-rct-1",
    "href": "class_09.html#issues-with-rct-1",
    "title": "Data Science for Business Applications",
    "section": "Issues with RCT",
    "text": "Issues with RCT\n\nExternal validity is the ability of an experimental result to generalize to a larger context or population.\nExamples of threats to external validity:\nFailure to randomize.\nNon representative samples.\nNon representative protocol/policy."
  },
  {
    "objectID": "class_09.html#blocking",
    "href": "class_09.html#blocking",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nRandomization works “on average” but we only get one opportunity at creating treatment and control groups, and there might be imbalances in “nuisance” variables that could affect the outcome.\nFor example, what will happen if the treatment group for the Moderna trial happens to get younger people in it than the control group?\nWe can solve this by blocking or stratifying: randomly assigning to treatment/control within groups."
  },
  {
    "objectID": "class_09.html#blocking-1",
    "href": "class_09.html#blocking-1",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nUnbalanced sample"
  },
  {
    "objectID": "class_09.html#blocking-2",
    "href": "class_09.html#blocking-2",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nBlocking or stratification sample"
  },
  {
    "objectID": "class_09.html#blocking-in-vaccine-trial",
    "href": "class_09.html#blocking-in-vaccine-trial",
    "title": "Data Science for Business Applications",
    "section": "Blocking in vaccine trial",
    "text": "Blocking in vaccine trial\n\nIn the Moderna vaccine trial, they identified two possible variables that could impact COVID outcomes:\nAge (65+ vs under 65)\nUnderlying health condition"
  },
  {
    "objectID": "class_09.html#blocking-in-vaccine-trial-1",
    "href": "class_09.html#blocking-in-vaccine-trial-1",
    "title": "Data Science for Business Applications",
    "section": "Blocking in vaccine trial",
    "text": "Blocking in vaccine trial"
  },
  {
    "objectID": "class_09.html#experiments-using-regression",
    "href": "class_09.html#experiments-using-regression",
    "title": "Data Science for Business Applications",
    "section": "Experiments using regression",
    "text": "Experiments using regression\n\nNon-blocked design: use a simple regression \\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 T,\n\\]\nwhere \\(T\\) is a dummy variable that is \\[\nT =\n\\begin{cases}\n  1, & \\text{for the treatment group}, \\\\\n  0, & \\text{for the control group}\n\\end{cases}\n\\]\n\\(\\widehat{\\beta}_1\\) represents the estimated average treatment effect. The regression needs to be logistic if Y is categorical!"
  },
  {
    "objectID": "class_09.html#experiments-using-regression-1",
    "href": "class_09.html#experiments-using-regression-1",
    "title": "Data Science for Business Applications",
    "section": "Experiments using regression",
    "text": "Experiments using regression\n\nBlocked design: use a regression that controls for the blocking variable \\(B\\):\n\n\\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 T + \\widehat{\\beta}_2 B,\n\\]\n\nwhere \\(B\\) is the fixed effect of each strata, that are interactions between categories.\nImportant: the regression needs to be logistic if \\(Y\\) is categorical."
  },
  {
    "objectID": "class_09.html#get-out-the-vote-gotv",
    "href": "class_09.html#get-out-the-vote-gotv",
    "title": "Data Science for Business Applications",
    "section": "Get Out The Vote (GOTV)",
    "text": "Get Out The Vote (GOTV)\n\nFact: lots of people don’t vote.\nIt’s important for people to vote, to ensure that our government reflects the will of its constituents.\nHow do we get people to vote?"
  },
  {
    "objectID": "class_09.html#get-out-the-vote-gotv-1",
    "href": "class_09.html#get-out-the-vote-gotv-1",
    "title": "Data Science for Business Applications",
    "section": "Get Out The Vote (GOTV)",
    "text": "Get Out The Vote (GOTV)\n\nIn 2002, researchers at Temple and Yale conducted a large phone banking experiment to see calling voters helps:\nFrom among about 381,062 phone numbers of voters in Iowa and Michigan they randomly contacted about 12000 voters\nThe outcome Y of interest is whether each voter actually voted."
  },
  {
    "objectID": "class_09.html#no-blocking",
    "href": "class_09.html#no-blocking",
    "title": "Data Science for Business Applications",
    "section": "No blocking",
    "text": "No blocking\nEstimating the average treatment effect with logistic regression:\n\nglm = glm(vote02 ~ treatment,data = GOTV, family = \"binomial\")\nsummary(glm)\n\n\nCall:\nglm(formula = vote02 ~ treatment, family = \"binomial\", data = GOTV)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        0.184717   0.003306  55.870   &lt;2e-16 ***\ntreatmenttreatment 0.170824   0.018843   9.066   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 524839  on 381061  degrees of freedom\nResidual deviance: 524756  on 381060  degrees of freedom\nAIC: 524760\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nThe coefficients are in log odds."
  },
  {
    "objectID": "class_09.html#no-blocking-1",
    "href": "class_09.html#no-blocking-1",
    "title": "Data Science for Business Applications",
    "section": "No blocking",
    "text": "No blocking\n\nThe average treatment effect will be of approximately 19%\n\n\n(exp(0.17)-1)*100\n\n[1] 18.53049\n\nconfint(glm)\n\n                       2.5 %    97.5 %\n(Intercept)        0.1782378 0.1911978\ntreatmenttreatment 0.1339278 0.2077954\n\n\n\nReceiving a phone call increases the likelihood of voting by 19% compared to those who did not receive a call.\nConfidence interval for the treatment\n\n\nconfint(glm)\n\n                       2.5 %    97.5 %\n(Intercept)        0.1782378 0.1911978\ntreatmenttreatment 0.1339278 0.2077954"
  },
  {
    "objectID": "class_09.html#blocking-3",
    "href": "class_09.html#blocking-3",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nThe researchers actually used a blocking design with two variables that they thought could impact voting rates (separately from the phone calls):\nThe state of the voter (Iowa (0) or Michigan (1))\nWhether the voter was in a “competitive” district (one where there was likely to be a close election)"
  },
  {
    "objectID": "class_09.html#blocking-4",
    "href": "class_09.html#blocking-4",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking"
  },
  {
    "objectID": "class_09.html#blocking-5",
    "href": "class_09.html#blocking-5",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nGOTV = GOTV %&gt;%\n       mutate(block = interaction(state, competiv))\nglm_vote = glm(vote02 ~ treatment + block, data = GOTV, family = 'binomial')\nsummary(glm_vote)\n\n\nCall:\nglm(formula = vote02 ~ treatment + block, family = \"binomial\", \n    data = GOTV)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        0.043236   0.004146   10.43   &lt;2e-16 ***\ntreatmenttreatment 0.028542   0.019279    1.48    0.139    \nblock1.1           0.351686   0.015168   23.19   &lt;2e-16 ***\nblock0.2           0.196691   0.008866   22.18   &lt;2e-16 ***\nblock1.2           0.603739   0.009515   63.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 524839  on 381061  degrees of freedom\nResidual deviance: 520331  on 381057  degrees of freedom\nAIC: 520341\n\nNumber of Fisher Scoring iterations: 4\n\nconfint(glm_vote)\n\n                          2.5 %     97.5 %\n(Intercept)         0.035109260 0.05136249\ntreatmenttreatment -0.009210835 0.06636325\nblock1.1            0.321979732 0.38143873\nblock0.2            0.179317682 0.21407167\nblock1.2            0.585102929 0.62239941"
  },
  {
    "objectID": "class_09.html#blocking-6",
    "href": "class_09.html#blocking-6",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nThe effect of the treatment is not significant under blocking.\nWhat if some callers didn’t stick to the script?\nMany people didn’t answer the phone!\nWhat about voters outside of the Midwest?"
  },
  {
    "objectID": "class_09.html#the-limitations-of-rcts",
    "href": "class_09.html#the-limitations-of-rcts",
    "title": "Data Science for Business Applications",
    "section": "The limitations of RCTs",
    "text": "The limitations of RCTs\n\nAlthough powerful for inferring causation, RCTs are difficult to apply.\nThey can be incredibly expensive.\nCompliance with the treatment protocol isn’t perfect (e.g., mask-wearing, picking up the phone)\nIt can be hard to generalize beyond the participants involved in the study.\nThey can be impractical or (e.g., effect of education on performance) or unethical to conduct (e.g., seatbelts, parachutes, even medical trials)"
  },
  {
    "objectID": "class_10.html#the-limitations-of-rcts",
    "href": "class_10.html#the-limitations-of-rcts",
    "title": "Data Science for Business Applications",
    "section": "The limitations of RCTs",
    "text": "The limitations of RCTs\nAlthough they are powerful for inferring causation, RCTs are hard to pull off:\n\nThey can be incredibly expensive (e.g., Phase 3 clinical trial)\nCompliance with the treatment protocol isn’t perfect\nIt can be hard to generalize beyond the participants involved in the study, if they aren’t representative.\nThey can be impractical (e.g., effect of education on later earnings) or even unethical (e.g., seatbelts, parachutes, even medical trials)"
  },
  {
    "objectID": "class_10.html#quasi-and-natural-experiments",
    "href": "class_10.html#quasi-and-natural-experiments",
    "title": "Data Science for Business Applications",
    "section": "Quasi-and natural experiments",
    "text": "Quasi-and natural experiments\nKey idea: Find a comparison group that is effectively “the same as” the treatment group to create a “quasi-experiment”a or “natural experiment”a\n\nCausal Question: Does serving in the military have an impact upon your long-term earnings after discharge?\nWhy won’t this work: Compare the wages of people who served in the US military in Afghanistan or Iraq, 10 years after discharge, to wages of the general public"
  },
  {
    "objectID": "class_10.html#the-effect-of-military-service-on-earnings",
    "href": "class_10.html#the-effect-of-military-service-on-earnings",
    "title": "Data Science for Business Applications",
    "section": "The effect of military service on earnings",
    "text": "The effect of military service on earnings\nAngrist (1990) wanted to determine what effect military service had on future earnings\n\n“Treatment” group: men selected by lottery to serve in Vietnam\n“Control” group: men eligible to be drafted but not selected to serve\nWe effectively have (almost) random assignment\nThis is called a “natural” experiment because we have “discovered” what is nearly an RCT out “in the wild”!\nFor white men, earnings in the 1980s were 15% lower in the treatment group; military service in Vietnam really did cause those serving to have less earning power long-term"
  },
  {
    "objectID": "class_10.html#quasi-and-natural-experiments-1",
    "href": "class_10.html#quasi-and-natural-experiments-1",
    "title": "Data Science for Business Applications",
    "section": "Quasi-and natural experiments",
    "text": "Quasi-and natural experiments\n\nThese are called quasi-experiments or natural experiments because participants are not randomly assigned to treatment and control groups, but groups are selected in such a way that the assignment can be thought of as effectively random."
  },
  {
    "objectID": "class_10.html#difference-in-differences",
    "href": "class_10.html#difference-in-differences",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\nA natural experiment of the minimum wage\n\nWhat happens if we raise the minimum wage?\nEconomic theory says there should be fewer jobs.\nWhy can’t we just compare the unemployment rate in places with a low minimum wage (e.g., Texas) to places with a high minimum wage (e.g., California)?\nWhy can’t we just do a randomized controlled trial to study the impact of raising the minimum wage?"
  },
  {
    "objectID": "class_10.html#difference-in-differences-1",
    "href": "class_10.html#difference-in-differences-1",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\nIn 1992, New Jersey’s minimum wage went from $4.25 to $5.05\nThe minimum wage in Pennsylvania remained at $4.25\nResearchers measured employment at 410 fast food restaurants in NJ and PA both before and after the change\nThis is a “natural” experiment because the two groups arose naturally (rather than being assigned by the researchers)"
  },
  {
    "objectID": "class_10.html#difference-in-differences-2",
    "href": "class_10.html#difference-in-differences-2",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences"
  },
  {
    "objectID": "class_10.html#pre-vs-post-comparison",
    "href": "class_10.html#pre-vs-post-comparison",
    "title": "Data Science for Business Applications",
    "section": "Pre vs post comparison",
    "text": "Pre vs post comparison\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nNew Jersey\n20.44\n21.03\n0.59\n\n\n\n\nEmployment went up by 0.59 employees per store in NJ. Can we interpret this as a causal effect?\nWe cannot distinguish the effect of the minimum wage increase from other things that changed in NJ at the same time."
  },
  {
    "objectID": "class_10.html#nj-vs-pa-comparison",
    "href": "class_10.html#nj-vs-pa-comparison",
    "title": "Data Science for Business Applications",
    "section": "NJ vs PA comparison",
    "text": "NJ vs PA comparison\n\n\n\n\nAfter\n\n\n\n\nPennsylvania\n21.17\n\n\nNew Jersey\n21.03\n\n\nDifference\n−0.14\n\n\n\n\nAfter the policy change, employment was 0.14 employees per store less in NJ than in PA. Can we interpret this as a causal effect?\nWe cannot distinguish the effect of the minimum wage increase from other differences between PA and NJ."
  },
  {
    "objectID": "class_10.html#difference-in-differences-3",
    "href": "class_10.html#difference-in-differences-3",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nPennsylvania\n23.33\n21.17\n-2.16\n\n\nNew Jersey\n20.44\n21.03\n0.59\n\n\nDifference\n-2.89\n-0.14\n2.76\n\n\n\n\nThe difference of the differences (−0.14 − (−2.89) or 0.59 − (−2.16)) gives us the causal effect of the policy change."
  },
  {
    "objectID": "class_10.html#difference-in-differences-4",
    "href": "class_10.html#difference-in-differences-4",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences"
  },
  {
    "objectID": "class_10.html#difference-in-differences-5",
    "href": "class_10.html#difference-in-differences-5",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\nDifference-in-differences framework"
  },
  {
    "objectID": "class_10.html#difference-in-differences-6",
    "href": "class_10.html#difference-in-differences-6",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\nWhy can’t we just compare college attainment of students who were and were not offered financial aid?\nIn 1982, the US government ended a program that provided $6,700 in financial aid to students whose parents were deceased.\nWe can therefore compare college attainment among four groups to get a natural experiment of the impact of financial aid:\nStudents with deceased parents, before 1982\nStudents with deceased parents, after 1982\nStudents without deceased parents, before 1982\nStudents without deceased parents, after 1982"
  },
  {
    "objectID": "class_10.html#difference-in-differences-7",
    "href": "class_10.html#difference-in-differences-7",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\nWhat is the impact of financial aid on degree attainment?\n\n\n\n\n\n\n\n\n\n\n1979-81\n1982-83\nDifference\n\n\n\n\nWith deceased parents\n56%\n35%\n21%\n\n\nWithout deceased parents\n50%\n48%\n2%\n\n\nDifference\n6%\n-13%\n19%\n\n\n\n\nThe $6,700 in financial aid led to a 19-percentage point increase in college attainment."
  },
  {
    "objectID": "class_10.html#difference-in-differences-8",
    "href": "class_10.html#difference-in-differences-8",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences"
  },
  {
    "objectID": "class_10.html#diff-diff-in-regression",
    "href": "class_10.html#diff-diff-in-regression",
    "title": "Data Science for Business Applications",
    "section": "Diff & Diff in Regression",
    "text": "Diff & Diff in Regression\nWe can also do the difference-in-differences calculation using regression. If we set: \\[\nT =\n\\begin{cases}\n1, & \\text{for stores in New Jersey (treatment)} \\\\\n0, & \\text{for stores in Pennsylvania (control)}\n\\end{cases}\n\\]\nand\n\\[\nX =\n\\begin{cases}\n1, & \\text{for measurements after the policy change (post)} \\\\\n0, & \\text{for measurements before the policy change (pre)}\n\\end{cases}\n\\]\nThen we can fit the same regression discontinuity model with interaction. \\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\beta}_2T + \\widehat{\\beta}_3T\\cdot X\n\\] The coefficient \\(\\widehat{\\beta}_3\\) of \\(T\\) is the causal effect we’re looking for!"
  },
  {
    "objectID": "class_10.html#assumptions",
    "href": "class_10.html#assumptions",
    "title": "Data Science for Business Applications",
    "section": "Assumptions",
    "text": "Assumptions\n\nParallel Trends\nIn the absence of the intervention, treatment and control group would have changed in the same way"
  },
  {
    "objectID": "class_10.html#natural-experiments",
    "href": "class_10.html#natural-experiments",
    "title": "Data Science for Business Applications",
    "section": "Natural experiments",
    "text": "Natural experiments\nWays to create natural experiments\n\nGeographic boundaries (e.g., NJ vs PA minimum wage example)\nPolicy changes (e.g., financial aid policy change example)\nLotteries (e.g., Vietnam draft lottery example)\nArbitrary cutoffs"
  },
  {
    "objectID": "class_11.html#causal-inference",
    "href": "class_11.html#causal-inference",
    "title": "Data Science for Business Applications",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nNatural Experiments (RCTs in the wild).\nAlways check for balance! (All things equal!)\nDifference-in-Differences (Diff-in-Diff):\nHow we can use two wrong estimates to get a right one.\nAssumptions behind DD (Parallel changes)."
  },
  {
    "objectID": "class_11.html#regression-discontinuity-design",
    "href": "class_11.html#regression-discontinuity-design",
    "title": "Data Science for Business Applications",
    "section": "Regression Discontinuity Design",
    "text": "Regression Discontinuity Design\n\nWhat will we learn today?\nRegression Discontinuity Design (RDD)\nHow can we use discontinuities to recover causal effects?\nAssumptions behind RDD designs."
  },
  {
    "objectID": "class_11.html#introduction",
    "href": "class_11.html#introduction",
    "title": "Data Science for Business Applications",
    "section": "Introduction",
    "text": "Introduction\n\nArbitrary rules determine treatment assignment:\nIf you are above a threshold, you are assigned to treatment, and if your below, you are not (or vice versa)\nGeographic discontinuities\nTime discontinuities\nVoting discontinuities\nYou can find discontinuities everywhere!"
  },
  {
    "objectID": "class_11.html#example-cohort-size",
    "href": "class_11.html#example-cohort-size",
    "title": "Data Science for Business Applications",
    "section": "Example: Cohort size",
    "text": "Example: Cohort size\n\nMany people argue that smaller classes lead to better learning outcomes compared to large classes.\nBut why can’t we just compare test scores of students in small classes and students in large classes?\nAngrist & Levy (1999) studied this by taking advantage of a rule in Israeli schools, where cohorts of &gt;40 students are split into two smaller classes"
  },
  {
    "objectID": "class_11.html#example-cohort-size-1",
    "href": "class_11.html#example-cohort-size-1",
    "title": "Data Science for Business Applications",
    "section": "Example: Cohort size",
    "text": "Example: Cohort size\nKey idea: Students in cohorts just below 40 students are essentially identical to students in cohorts just above 40, but the ones in the latter group will get a smaller class.\n\nlibrary(tidyverse)\nggplot(class_1999, aes(x = cohort.size, y = read, color = class_split)) + \n  geom_point() +\n  geom_vline(xintercept = 40, linetype = \"dashed\")\n\n\n\nIs there a difference in the reading score between larger and smaller classes?"
  },
  {
    "objectID": "class_11.html#creating-the-rdd-model",
    "href": "class_11.html#creating-the-rdd-model",
    "title": "Data Science for Business Applications",
    "section": "Creating the RDD model",
    "text": "Creating the RDD model\n\nDefine a treatment variable: \\[\\begin{equation}\nT = \\begin{cases}\n1, \\quad \\text{split cohort},\\\\\n0, \\quad \\text{intact cohort}\n\\end{cases}\n\\end{equation}\\]\nRecenter the selection variable so the cutoff is at 0: \\[\\begin{equation}\nX = (\\texttt{cohort.size}) − 40\n\\end{equation}\\]\nThen fit a model predicting reading scores from both \\(X\\) and \\(T\\): \\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_1 X + \\widehat{\\beta}_2 T\n\\end{equation}\\]\n\n\nThe coefficient \\(\\widehat{\\beta}_2\\) of \\(T\\) is the causal effect we’re looking for!"
  },
  {
    "objectID": "class_11.html#rdd-model",
    "href": "class_11.html#rdd-model",
    "title": "Data Science for Business Applications",
    "section": "RDD model",
    "text": "RDD model\n\nclass_1999 = class_1999 %&gt;% \n  mutate(treatment=ifelse(cohort.size &gt; 40, 1, 0), \n         selection=(cohort.size - 40))\n\nrdd1 &lt;- lm(read ~ selection + treatment, data=class_1999) \nsummary(rdd1)\n\n\nCall:\nlm(formula = read ~ selection + treatment, data = class_1999)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.195  -5.572   1.537   6.617  17.269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  69.7556     1.2697  54.939   &lt;2e-16 ***\nselection    -0.1195     0.2020  -0.592   0.5545    \ntreatment     4.0031     2.1511   1.861   0.0638 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.135 on 294 degrees of freedom\nMultiple R-squared:  0.02237,   Adjusted R-squared:  0.01572 \nF-statistic: 3.363 on 2 and 294 DF,  p-value: 0.03596\n\n\n\nThe effect of the treatment is an increase of \\(\\widehat{\\beta}_2 = 4\\) points in the reading score if the students."
  },
  {
    "objectID": "class_11.html#pre-vs-post-comparison",
    "href": "class_11.html#pre-vs-post-comparison",
    "title": "Data Science for Business Applications",
    "section": "Pre vs post comparison",
    "text": "Pre vs post comparison\nOur first RDD model is forcing the two lines to have the same slope; that isn’t a great fit for the data:"
  },
  {
    "objectID": "class_11.html#rdd-and-interactions",
    "href": "class_11.html#rdd-and-interactions",
    "title": "Data Science for Business Applications",
    "section": "RDD and interactions",
    "text": "RDD and interactions\n\nTo allow the two slopes to be different, we can add an interaction term to allow the slope of \\(X\\) to be different for \\(T = 0\\) (cohort kept intact) and \\(T = 1\\) (cohort split into smaller classes): \\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X + \\widehat{\\beta}_2 T + \\widehat{\\beta}_3 T\\cdot X\n\\end{equation}\\]\nAgain, the slope of \\(\\widehat{\\beta}_2\\) is our estimate of the causal effect of the treatment."
  },
  {
    "objectID": "class_11.html#rdd-with-interactions",
    "href": "class_11.html#rdd-with-interactions",
    "title": "Data Science for Business Applications",
    "section": "RDD with interactions",
    "text": "RDD with interactions\n\nrdd2 &lt;- lm(read ~ selection*treatment, data=class_1999) \nsummary(rdd2)\n\n\nCall:\nlm(formula = read ~ selection * treatment, data = class_1999)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.618  -6.102   1.341   6.922  17.249 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          66.6294     1.8141  36.729   &lt;2e-16 ***\nselection            -0.8945     0.3806  -2.350   0.0194 *  \ntreatment             5.6641     2.2439   2.524   0.0121 *  \nselection:treatment   1.0720     0.4477   2.395   0.0173 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.063 on 293 degrees of freedom\nMultiple R-squared:  0.04113,   Adjusted R-squared:  0.03132 \nF-statistic:  4.19 on 3 and 293 DF,  p-value: 0.00634\n\n\n\nFrom our data we an conclude that smaller class sizes cause reading scores to increase by about 5.7 points."
  },
  {
    "objectID": "class_11.html#rdd-with-interactions-1",
    "href": "class_11.html#rdd-with-interactions-1",
    "title": "Data Science for Business Applications",
    "section": "RDD with interactions",
    "text": "RDD with interactions\nMore flexibility with the interaction gives the model a better fit in relation to the data:"
  },
  {
    "objectID": "class_11.html#conclusion",
    "href": "class_11.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nRDD is usually great for internal validity, but there are lots of threats to external validity.\nWhy this statement holds?\nFor example, would this generalize to different grade levels? Schools outside of Israel?"
  },
  {
    "objectID": "class_11.html#example-sales",
    "href": "class_11.html#example-sales",
    "title": "Data Science for Business Applications",
    "section": "Example: Sales",
    "text": "Example: Sales\n\nYou are managing a retail store and notice that sales are low in the mornings, so you want to improve those numbers.\nA store gives a 10% discount to the first 1,000 customers that arrive.\nIs this a good candidate for regression discontinuity?\nLet’s look at the data."
  },
  {
    "objectID": "class_11.html#example-sales-1",
    "href": "class_11.html#example-sales-1",
    "title": "Data Science for Business Applications",
    "section": "Example: Sales",
    "text": "Example: Sales\n\n\nSales in relation to time since the opening of the store in minutes.\nThe store receives its 1,000th customer around 260 minutes after opening (around 4.3 hours)."
  },
  {
    "objectID": "class_11.html#creating-the-rdd-model-1",
    "href": "class_11.html#creating-the-rdd-model-1",
    "title": "Data Science for Business Applications",
    "section": "Creating the RDD model",
    "text": "Creating the RDD model\n\nDefine a treatment variable: \\[\\begin{equation}\nT = \\begin{cases}\n1, \\quad \\text{promotion}\\\\\n0, \\quad \\text{no promotion}\n\\end{cases}\n\\end{equation}\\]\nRecenter the selection variable so the cutoff is at 0: \\[\\begin{equation}\nX = (\\texttt{time}) - 260\n\\end{equation}\\]\nThen fit a model predicting the sales of a customer from both \\(X\\) and \\(T\\): \\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_1 X + \\widehat{\\beta}_2 T\n\\end{equation}\\]\n\n\nThe coefficient \\(\\widehat{\\beta}_2\\) is the causal effect we are looking for!"
  },
  {
    "objectID": "class_11.html#rdd-model-1",
    "href": "class_11.html#rdd-model-1",
    "title": "Data Science for Business Applications",
    "section": "RDD model",
    "text": "RDD model\n\nsales = sales %&gt;% \n  mutate(selection=(time - 260))\n\nrdd_sales &lt;- lm(sales ~ selection + treat, data=sales) \nsummary(rdd_sales)\n\n\nCall:\nlm(formula = sales ~ selection + treat, data = sales)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-77.32 -14.62   0.14  14.56  68.95 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 165.61482    1.08422  152.75   &lt;2e-16 ***\nselection    -0.10294    0.00661  -15.57   &lt;2e-16 ***\ntreat        31.52628    1.95845   16.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.82 on 1997 degrees of freedom\nMultiple R-squared:  0.6539,    Adjusted R-squared:  0.6535 \nF-statistic:  1886 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\n\n\nOn average, providing a 10% discount increases sales by $31.30 for the 1,000 customers, compared to not having a discount."
  },
  {
    "objectID": "class_11.html#rdd-model-2",
    "href": "class_11.html#rdd-model-2",
    "title": "Data Science for Business Applications",
    "section": "RDD model",
    "text": "RDD model"
  },
  {
    "objectID": "class_11.html#a-more-flexible-rdd-model",
    "href": "class_11.html#a-more-flexible-rdd-model",
    "title": "Data Science for Business Applications",
    "section": "A more flexible RDD model",
    "text": "A more flexible RDD model\n\nAs in the first example, we have that two lines of the RDD model have the same slope.\nWe can make this model more flexible by adding an interaction term.\nTo allow the two slopes to be different, we can add an interaction term to allow the slope of \\(X\\) to be different for \\(T = 0\\) (after 1,000th customer) and \\(T = 1\\) (first thousand customers):\n\n\\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\, X + \\widehat{\\beta}_2 \\, T + \\widehat{\\beta}_3 \\, X\\cdot T\n\\end{equation}\\]"
  },
  {
    "objectID": "class_11.html#rdd-with-interaction",
    "href": "class_11.html#rdd-with-interaction",
    "title": "Data Science for Business Applications",
    "section": "RDD with interaction",
    "text": "RDD with interaction\n\nrdd_sales_inter &lt;- lm(sales ~ selection*treat, data=sales) \nsummary(rdd_sales_inter)\n\n\nCall:\nlm(formula = sales ~ selection * treat, data = sales)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-65.738 -13.940   0.051  13.538  76.515 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     178.574245   1.297815  137.60   &lt;2e-16 ***\nselection        -0.205355   0.008882  -23.12   &lt;2e-16 ***\ntreat            31.399196   1.842316   17.04   &lt;2e-16 ***\nselection:treat   0.200845   0.012438   16.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 1996 degrees of freedom\nMultiple R-squared:  0.6939,    Adjusted R-squared:  0.6934 \nF-statistic:  1508 on 3 and 1996 DF,  p-value: &lt; 2.2e-16\n\n\n\nOn average, providing a 10% discount increases sales by $33.10 for the 1,000 customers, compared to not having a discount."
  },
  {
    "objectID": "class_11.html#rdd-with-interaction-1",
    "href": "class_11.html#rdd-with-interaction-1",
    "title": "Data Science for Business Applications",
    "section": "RDD with interaction",
    "text": "RDD with interaction\n\nWe have different slopes, for before and after the treatment."
  },
  {
    "objectID": "class_11.html#conclusion-1",
    "href": "class_11.html#conclusion-1",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nAgain, RDD is usually great for internal validity, but there are lots of threats to external validity.\nFor example, would this generalize to different types of products? Same store, but a different location?"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Henrique Bolfarine",
    "section": "Contact",
    "text": "Contact\n\nEmail: henrique.bolfarfine at austin.utexas.edu\nAddress: GSB 3.140A, McCombs School of Business, 2110 Speedway, Austin"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Henrique Bolfarine",
    "section": "Research Interests",
    "text": "Research Interests\n\nBayesian Statistics\nLatent Variable modelling\nModel Summarization\nFactor Models\nBayesian mixture models\nNonparametric models"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "Research",
    "section": "",
    "text": "“Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models.” H. Bolfarine, H.F. Lopes, & C.M. Carvalho. Working Paper"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\n“Decoupling Shrinkage and Selection in Gaussian Linear Factor Analysis.” H. Bolfarine, C.M. Carvalho, H.F. Lopes, & J.S. Murray. Bayesian Analysis (2024) 19, Number 1, pp. 181–203\n“2021 International Statistical Institute Mahalanobis Award: A Tribute to Heleno Bolfarine.” F. Ruggeri, H. Bolfarine, J.L. Bazan, & C.M. Carvalho. International Statistical Review (2021), 89, 3, 2021 International Statistical Institute."
  },
  {
    "objectID": "about.html#preprints",
    "href": "about.html#preprints",
    "title": "Research",
    "section": "Preprints",
    "text": "Preprints\n\n“Network reconstruction with local partial correlation: comparative evaluation.” H. Bolfarine, L. Thomas, & A. Yambartsev. Working Paper"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Henrique Bolfarine",
    "section": "CV",
    "text": "CV\n\nHenrique Bolfarine - CV"
  },
  {
    "objectID": "teaching.html#university-of-texas-at-austin",
    "href": "teaching.html#university-of-texas-at-austin",
    "title": "Teaching",
    "section": "",
    "text": "STA 235 - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables\nclass 03 - Interactions\nclass 04 - Regression Assumptions\nclass 05 - Class 05 - Modeling nonlinear relationships\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 Honors - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables and Interactions\nclass 03 - Regression assumptions and Outliers\nclass 04 - Modeling nonlinear relationships\nclass 05 - Time Series Decomposition and Autoregression\nclass 06 - Model Selection\nclass 07 - Logistic Regression\nclass 08 - Basic Causal Inference\nclass 09 - Randomized Control Trials\nclass 10 - Natural Experiments - Diff-in-Diff\nclass 11 - Natural Experiments - RDD\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Application\n\n\n\n\n\nSTA 235 - Data Science for Business Application"
  },
  {
    "objectID": "index.html#cv---henrique-bolfarine---cv",
    "href": "index.html#cv---henrique-bolfarine---cv",
    "title": "Henrique Bolfarine",
    "section": "CV - Henrique Bolfarine - CV",
    "text": "CV - Henrique Bolfarine - CV"
  },
  {
    "objectID": "about.html#invited-reviewer",
    "href": "about.html#invited-reviewer",
    "title": "Research",
    "section": "Invited Reviewer",
    "text": "Invited Reviewer\n\nBayesian Analysis\nEntropy"
  },
  {
    "objectID": "teaching.html#university-of-são-paulo",
    "href": "teaching.html#university-of-são-paulo",
    "title": "Teaching",
    "section": "University of São Paulo",
    "text": "University of São Paulo\n\nSummer 2022\n\nIntroduction to Probability Theory"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html",
    "href": "fall_sta235_2025/week_01/week_01.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Use regression to build predictive models\nUnderstand the benefits and limitations of the models we build\nGiven a new business situation, select an appropriate model, build it, measure its effectiveness, and effectively communicate the results\nThis is a practical course!\n\n\n\n\n\n\n\n\n\n\n\n\nWhy bother learning this stuff when we can get ChatGPT to do data analysis for us?\nAI (and computing in general) is only useful when you have the expertise to be able to recognize the correctness (or not) of its output\nIn this class, you’ll develop that expertise!\n\n\n\n\n\nInstructor: Henrique Bolfarine, Ph.D.\n\nOffice hours: Mondays 1:00 PM - 2:00 PM (GSB 3.140 A)\nEmail: henrique.bolfarine@austin.utexas.edu\n\nCourse Assistants:\n\nLead Course Assistant (CA): Ezgi Durakoglugil\nOffice hours: Many TA/CA office hours every week (both in person and on Zoom) - This should be your first option!\nYou can ask any of the TAs/CAs about course content, but go to Ezgi for questions about logistics"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#course-goals",
    "href": "fall_sta235_2025/week_01/week_01.html#course-goals",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Use regression to build predictive models\nUnderstand the benefits and limitations of the models we build\nGiven a new business situation, select an appropriate model, build it, measure its effectiveness, and effectively communicate the results\nThis is a practical course!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#why-does-this-course-exist",
    "href": "fall_sta235_2025/week_01/week_01.html#why-does-this-course-exist",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Why bother learning this stuff when we can get ChatGPT to do data analysis for us?\nAI (and computing in general) is only useful when you have the expertise to be able to recognize the correctness (or not) of its output\nIn this class, you’ll develop that expertise!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#about-the-course-staff",
    "href": "fall_sta235_2025/week_01/week_01.html#about-the-course-staff",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Instructor: Henrique Bolfarine, Ph.D.\n\nOffice hours: Mondays 1:00 PM - 2:00 PM (GSB 3.140 A)\nEmail: henrique.bolfarine@austin.utexas.edu\n\nCourse Assistants:\n\nLead Course Assistant (CA): Ezgi Durakoglugil\nOffice hours: Many TA/CA office hours every week (both in person and on Zoom) - This should be your first option!\nYou can ask any of the TAs/CAs about course content, but go to Ezgi for questions about logistics"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#course-structure",
    "href": "fall_sta235_2025/week_01/week_01.html#course-structure",
    "title": "Data Science for Business Applications",
    "section": "Course Structure",
    "text": "Course Structure\n\nUnits\n\nUnit A: Fundamentals of regression modeling\nUnit B: Applications and extensions\n\nCanvas\n\nMake sure you can log in and are enrolled in STA 235 in Canvas\nCheck out the home page for the weekly schedule and to meet the course staff"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#statistical-computing",
    "href": "fall_sta235_2025/week_01/week_01.html#statistical-computing",
    "title": "Data Science for Business Applications",
    "section": "Statistical Computing",
    "text": "Statistical Computing\n\n\n\nWe will use R and RStudio for statistical analysis throughout the course\nMake sure both are installed on your laptop and bring it to every class\nIf you aren’t comfortable with R/RStudio from STA 301, don’t worry!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#weekly-cadence-for-a-particular-topic",
    "href": "fall_sta235_2025/week_01/week_01.html#weekly-cadence-for-a-particular-topic",
    "title": "Data Science for Business Applications",
    "section": "Weekly Cadence for a Particular Topic",
    "text": "Weekly Cadence for a Particular Topic\n\nDue by the start of class on Monday/Tuesday: Perusall pre-class video/reading discussion covering the topic\nDuring class on Monday/Tuesday: Lecture, activities, practice topic\nDue by 11:59 PM the following Sunday/Monday: Homework covering the topic\nThe following Monday/Tuesday: at the beginning of class: Checkpoint Quiz on that topic"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#pre-class-work",
    "href": "fall_sta235_2025/week_01/week_01.html#pre-class-work",
    "title": "Data Science for Business Applications",
    "section": "Pre-Class Work",
    "text": "Pre-Class Work\n\nThis is a fast-paced course, so it’s essential that you think about the material before class.\nWe will use Perusall for pre-class video and reading assignments.\nUse Perusall to ask your classmates questions, and share your knowledge, thoughts, and opinions.\nThis helps you better understand the material and will help me gear class time to what topics you are having the most trouble with."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#pre-class-work-1",
    "href": "fall_sta235_2025/week_01/week_01.html#pre-class-work-1",
    "title": "Data Science for Business Applications",
    "section": "Pre-Class Work",
    "text": "Pre-Class Work\n\nPre-class assignments (typically videos) are due at the start of each class.\nAim to chime in with at least a few thoughtful questions, responses, or comments for each reading assignment.\nGrading is based on effort and thoughtfulness of your questions and comments and your engagement with classmates and the text.\nEach assignment is scored 0-3, but with a reasonable effort you will get a 3 on each one (so don’t worry about your grade)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#homework",
    "href": "fall_sta235_2025/week_01/week_01.html#homework",
    "title": "Data Science for Business Applications",
    "section": "Homework",
    "text": "Homework\n\n\n\nWhy homework?\nHomework is due each week at 11:59 PM the night before class and submitted through Canvas.\nAutomatically graded; resubmit as many times as you want!\nOK to work together, but try the problems on your own first for maximum benefit."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checkpoint-quizzes",
    "href": "fall_sta235_2025/week_01/week_01.html#checkpoint-quizzes",
    "title": "Data Science for Business Applications",
    "section": "Checkpoint Quizzes",
    "text": "Checkpoint Quizzes\n\nIt is critical in this course to stay on top of things and not fall behind.\nCheckpoint Quiz at the start of each class will help you ensure that you are really learning the material and give you an early heads-up if you aren’t.\nWe’ll drop your lowest quiz score from each unit (A and B).\nYou’ll have access to RStudio and a “cheat sheet” during quizzes (don’t spend time memorizing anything!)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#mastery-exams",
    "href": "fall_sta235_2025/week_01/week_01.html#mastery-exams",
    "title": "Data Science for Business Applications",
    "section": "Mastery Exams",
    "text": "Mastery Exams\n\nEach unit concludes with a Mastery Exam:\n\nUnit A: October 22 or 23 at 7 to 9 PM\nUnit B: University-assigned final exam period\n\nYou’ll have access to RStudio and a “cheat sheet” during exams (don’t spend time memorizing anything!)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#assessment-grading",
    "href": "fall_sta235_2025/week_01/week_01.html#assessment-grading",
    "title": "Data Science for Business Applications",
    "section": "Assessment Grading",
    "text": "Assessment Grading\n\nUnit A has 7 Checkpoint Quizzes and Unit B has 6.\nFor each unit, we will replace your lowest quiz score with your exam score for that unit (if that helps your overall grade)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#grading",
    "href": "fall_sta235_2025/week_01/week_01.html#grading",
    "title": "Data Science for Business Applications",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nComponent\nPoints\n\n\n\n\nPre-class work (Perusall)\n44\n\n\nClass Participation\n56\n\n\nHomework (13)\n195\n\n\nCheckpoint Quizze (13)\n325\n\n\nExam A\n190\n\n\nExam B\n190\n\n\nTotal\n1,000"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#getting-help",
    "href": "fall_sta235_2025/week_01/week_01.html#getting-help",
    "title": "Data Science for Business Applications",
    "section": "Getting Help",
    "text": "Getting Help\n\nMy office hours: Schedule on Canvas.\nTA/CA office hours: Schedule on Canvas.\nPost questions in videos in Perusall (for questions about the course material).\nPost questions in group chats in Perusall (for general questions about the course, or homework questions).\nWeekly optional TA/CA-led review session (TBD)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#section",
    "href": "fall_sta235_2025/week_01/week_01.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "What personal characteristics about an instructor do you think are predictive of the scores they receive on student evaluations?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#hamermesh-parker-2005-data-set",
    "href": "fall_sta235_2025/week_01/week_01.html#hamermesh-parker-2005-data-set",
    "title": "Data Science for Business Applications",
    "section": "Hamermesh & Parker (2005) Data Set",
    "text": "Hamermesh & Parker (2005) Data Set\n\n\nStudent evaluations of \\(N=463\\) instructors at UT Austin, 2000-2002\nFor each instructor:\neval: average student evaluation of teacher\nbeauty: average beauty score from a six-student panel\ngender: male or female\ncredits: single- or multi-credit course\nage: age of instructor\n(and more…)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-texttteval",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-texttteval",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: \\(\\texttt{eval}\\)",
    "text": "Explore the data: \\(\\texttt{eval}\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-textttbeauty",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-textttbeauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: \\(\\texttt{beauty}\\)",
    "text": "Explore the data: \\(\\texttt{beauty}\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#more-time-series-datasets",
    "href": "fall_sta235_2025/week_01/week_01.html#more-time-series-datasets",
    "title": "Data Science for Business Applications",
    "section": "More time series datasets",
    "text": "More time series datasets\nWhat are some common characteristics of these time series? How do they differ?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#components-of-time-series",
    "href": "fall_sta235_2025/week_01/week_01.html#components-of-time-series",
    "title": "Data Science for Business Applications",
    "section": "Components of time series",
    "text": "Components of time series\nWe can think about a time series as having one or more components, or sources of variability:\n\n\nTrend+cyclic: Persistent, (usually) slow-moving long-run patterns\n\nLong run, Apple’s earnings go up over time\n\nSeasonal: Regular up-and-down movement around long-run trends\n\nApple’s earnings vary predictably each quarter around the long-run trend\n\nRandom or unpredictable variation\n\nCustomers are fickle creatures, so earnings aren’t perfectly predictable\n\nExternal factors, like shocks that interrupt or change previous dynamics\n\nIntroductions of the iPhone, supply chain shocks, …"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#modeling-time-series",
    "href": "fall_sta235_2025/week_01/week_01.html#modeling-time-series",
    "title": "Data Science for Business Applications",
    "section": "Modeling time series",
    "text": "Modeling time series\nWe can model many time series using our standard regression tools!\n\nTrend+cyclic:\n\nModel long-run trends with linear or nonlinear functions of time\nUse today’s outcome directly to predict tomorrow’s\n\nSeasonal:\n\nTreat the season (quarter, month, week, etc) as a categorical variable\n\nExternal factors:\n\nCreate new predictors from other time series or to reflect known events"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#time-series-data",
    "href": "fall_sta235_2025/week_01/week_01.html#time-series-data",
    "title": "Data Science for Business Applications",
    "section": "Time series data",
    "text": "Time series data\nSome notation:\n\\[\\begin{align*}\nt &= \\text{time }(1, 2, 3, \\ldots) \\\\\nY_t &= \\text{the value of the variable we are interested in, at time $t$}\n\\end{align*}\\]\nThe change from \\(Y_i\\) to \\(Y_t\\) reflects that each observation (row) corresponds to a time indexed by \\(t\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#structure-of-the-apple-earnings-data",
    "href": "fall_sta235_2025/week_01/week_01.html#structure-of-the-apple-earnings-data",
    "title": "Data Science for Business Applications",
    "section": "Structure of the Apple earnings data",
    "text": "Structure of the Apple earnings data\n\nhead(apple) # %&gt;% select(-Time))\n\n  Period    Time Year Quarter Revenue\n1      1 2011.50 2011      Q3   28.27\n2      2 2011.75 2011      Q4   46.33\n3      3 2012.00 2012      Q1   39.19\n4      4 2012.25 2012      Q2   35.02\n5      5 2012.50 2012      Q3   35.97\n6      6 2012.75 2012      Q4   54.51\n\n\n\n\\(Y_t\\) = \\(\\text{Revenue_t}\\)\n\\(\\text{Period}_t\\) = \\(t\\)\n\\(Time_t\\): Used for plots"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data",
    "href": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data",
    "title": "Data Science for Business Applications",
    "section": "Trend and Seasonality in the Apple earnings data",
    "text": "Trend and Seasonality in the Apple earnings data\n\nApple’s earnings have a clear trend over time and seasonality by quarter."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data-1",
    "href": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data-1",
    "title": "Data Science for Business Applications",
    "section": "Trend and Seasonality in the Apple earnings data",
    "text": "Trend and Seasonality in the Apple earnings data\n\napple_model1 &lt;- lm(Revenue ~ Period + Quarter, data = apple)\n\n...\n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  29.4695     3.0511    9.66      0.0000000000068 ***\nPeriod        1.3046     0.0892   14.62 &lt; 0.0000000000000002 ***\nQuarterQ2    -8.3919     3.1939   -2.63                0.012 *  \nQuarterQ3    -5.0707     3.1977   -1.59                0.121    \nQuarterQ4    22.4692     3.1939    7.03      0.0000000191875 ***\n...\n\n\n\\[\n\\hat Y_t = 29.5 + 1.3\\text{Period}_t - 8.4\\text{Q2}_t\n- 5.1\\text{Q3}_t\n+ 22.5\\text{Q4}_t\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#trend-seasonality-in-the-apple-earnings-data",
    "href": "fall_sta235_2025/week_01/week_01.html#trend-seasonality-in-the-apple-earnings-data",
    "title": "Data Science for Business Applications",
    "section": "Trend + Seasonality in the Apple earnings data",
    "text": "Trend + Seasonality in the Apple earnings data\n\\[\n\\hat Y_t = 29.5 + 1.3\\text{Period}_t - 8.4\\text{Q2}_t\n- 5.1\\text{Q3}_t\n+ 22.5\\text{Q4}_t\n\\]\n\nOn average, earnings in the same quarter increase \\(4\\times 1.3 = \\$5.2\\)b every year.\nAfter accounting for the trend, Q2 earnings are \\(\\$ 7.8\\)b lower than Q1 on average\nAfter accounting for the trend, Q3 earnings are ??? lower than Q1 on average\nAfter accounting for the trend, Q4 earnings are \\(\\$ 22.4\\)b higher than Q1 on average"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-assumptions",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nLet’s check on our modeling assumptions…\n\nWhich are violated?\n\nLinearity and Equal variance! (Independence TBD)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#whats-going-on",
    "href": "fall_sta235_2025/week_01/week_01.html#whats-going-on",
    "title": "Data Science for Business Applications",
    "section": "What’s going on?",
    "text": "What’s going on?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinearity + Increasing variance over time usually suggests taking log of \\(Y\\) to get a multiplicative model"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#a-multiplicative-model-is-more-appropriate",
    "href": "fall_sta235_2025/week_01/week_01.html#a-multiplicative-model-is-more-appropriate",
    "title": "Data Science for Business Applications",
    "section": "A multiplicative model is more appropriate",
    "text": "A multiplicative model is more appropriate\n\napple_model2 &lt;- lm(log(Revenue) ~ Period + Quarter, data = apple)\n\n...\n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.55729    0.03877   91.75 &lt; 0.0000000000000002 ***\nPeriod       0.02134    0.00113   18.82 &lt; 0.0000000000000002 ***\nQuarterQ2   -0.15339    0.04059   -3.78              0.00053 ***\nQuarterQ3   -0.10239    0.04063   -2.52              0.01595 *  \nQuarterQ4    0.32398    0.04059    7.98          0.000000001 ***\n...\n\n\n\\[\n\\widehat {\\log(Y_t)} = 3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t\n\\]\n\\[\n\\hat Y_t = e^{3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t}\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model",
    "href": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the multiplicative model",
    "text": "Interpreting the multiplicative model\n\\[\n\\hat Y_t = e^{3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t}\n\\]\n\nOn average, earnings in the same quarter increase by a factor of \\(\\exp(4\\times 0.02) = 1.083\\), or 8.3%, every year\nAfter accounting for the trend, on average Q2 earnings are lower than Q1 by a factor of \\(\\exp(-0.16) = 0.852\\), or \\(14.8\\%\\)\nAfter accounting for the trend, on average Q3 earnings are ??? than Q1 by a factor of ???, or ???%.\nAfter accounting for the trend, Q4 earnings are higher than Q1 by a factor of \\(\\exp(0.32) = 1.377\\), or \\(37.7\\%\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model-1",
    "href": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the multiplicative model",
    "text": "Interpreting the multiplicative model\n\\[\n\\hat Y_t = e^{3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t}\n\\]\nOn average, earnings in the same quarter increase by a factor of \\(\\exp(4\\times 0.02) = 1.083\\), or 8.3%, every year. Why?\n\nFor example: Q3 of 2019 (Period = 33): \\[\\hat Y_{32} = \\exp(3.56 + 0.02(33) + 0.1)\\]\n\n\nQ3 of 2020 (Period = 37): \\[\n\\begin{align}\n\\hat Y_{37} &= \\exp(3.56 + 0.02(37) + 0.1)\\\\\n&= \\exp(3.56 + 0.02(33 + 4)  + 0.1)\\\\\n&= \\color{darkred}{\\exp(3.56 + 0.02(33)  + 0.1)}\\times\\color{darkblue}{\\exp(0.02(4))}\\\\\n&= \\color{darkred}{\\hat Y_{32}} \\times \\color{darkblue}{1.083}\n\\end{align}\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#much-better",
    "href": "fall_sta235_2025/week_01/week_01.html#much-better",
    "title": "Data Science for Business Applications",
    "section": "Much better!",
    "text": "Much better!\n\nautoplot(apple_model2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#but-wait",
    "href": "fall_sta235_2025/week_01/week_01.html#but-wait",
    "title": "Data Science for Business Applications",
    "section": "But wait…",
    "text": "But wait…\nWe’re consistently under predicting recent quarters!\n\nIs there time dependence in our residuals?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence",
    "text": "Checking for independence\n\nPrediction errors for consecutive quarters are correlated!\n\\[\n\\mathrm{Cor}(\\text{residual}_t, \\text{residual}_{t-1}) = 0.64\n\\] (This is the lag 1 autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence (ACF)",
    "text": "Checking for independence (ACF)\n\nacf(residuals(apple_model2))\n\n\n\\[\n\\color{darkred}{\n\\mathrm{Cor}(\\text{residual}_t, \\text{residual}_{t-1}) = 0.64\n}\n\\] (This is the lag 1 autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-1",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-1",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence (ACF)",
    "text": "Checking for independence (ACF)\n\nacf(residuals(apple_model2))\n\n\n\\[\n\\color{darkorange}{\n\\mathrm{Cor}(\\text{residual}_t, \\text{residual}_{t-2}) = 0.26\n}\n\\]\n(This is the lag 2 autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-2",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-2",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence (ACF)",
    "text": "Checking for independence (ACF)\nThe autocorrelation function (ACF) plot shows autocorrelation at many lags\n\nacf(residuals(apple_model2))\n\n\nUnder independence, about 95% of the time any given lag will be within the blue. Look out for large lag 1 values in particular!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#improving-our-model",
    "href": "fall_sta235_2025/week_01/week_01.html#improving-our-model",
    "title": "Data Science for Business Applications",
    "section": "Improving our model",
    "text": "Improving our model\nLet’s try to address the bias in recent predictions first.\n\nWhat may have changed mid-late 2020?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks",
    "href": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks",
    "title": "Data Science for Business Applications",
    "section": "Modeling external shocks",
    "text": "Modeling external shocks\nAdding a COVID level-shift:\n\n# Define PostCOVID to be 1 on or after 2020 Q4\n# (i.e., if the year is 2021+ or if we are in 2020 Q4)\napple &lt;- apple %&gt;%\n  mutate(PostCOVID = ifelse(\n    Year &gt;= 2021 | (Year == 2020 & Quarter == \"Q4\"),\n    1, 0)\n  )\n\n\n\n...\n   Period Year Quarter Revenue PostCOVID\n35     35 2020      Q1   58.31         0\n36     36 2020      Q2   59.68         0\n37     37 2020      Q3   64.70         0\n38     38 2020      Q4  111.44         1\n39     39 2021      Q1   89.58         1\n40     40 2021      Q2   81.43         1\n41     41 2021      Q3   83.36         1\n..."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks-1",
    "href": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling external shocks",
    "text": "Modeling external shocks\nAdding a COVID level-shift:\n\napple_model3 &lt;- lm(log(Revenue) ~ Period + Quarter\n  + PostCOVID, data = apple)\n\n...\n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.59676    0.03500  102.77 &lt; 0.0000000000000002 ***\nPeriod       0.01830    0.00126   14.50 &lt; 0.0000000000000002 ***\nQuarterQ2   -0.15036    0.03500   -4.30              0.00012 ***\nQuarterQ3   -0.09328    0.03511   -2.66              0.01148 *  \nQuarterQ4    0.32094    0.03500    9.17       0.000000000036 ***\nPostCOVID    0.16701    0.04390    3.80              0.00050 ***\n...\n\n\n\nconfint(apple_model3)\n\n               2.5 %   97.5 %\n(Intercept)  3.52591  3.66762\nPeriod       0.01575  0.02086\nQuarterQ2   -0.22121 -0.07951\nQuarterQ3   -0.16436 -0.02219\nQuarterQ4    0.25009  0.39179\nPostCOVID    0.07813  0.25588"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#new-model-fit",
    "href": "fall_sta235_2025/week_01/week_01.html#new-model-fit",
    "title": "Data Science for Business Applications",
    "section": "New model fit",
    "text": "New model fit"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#new-models-residuals",
    "href": "fall_sta235_2025/week_01/week_01.html#new-models-residuals",
    "title": "Data Science for Business Applications",
    "section": "New model’s residuals",
    "text": "New model’s residuals\nThe last few quarters look better:\n\nBut is the time dependence gone?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#nope",
    "href": "fall_sta235_2025/week_01/week_01.html#nope",
    "title": "Data Science for Business Applications",
    "section": "Nope!",
    "text": "Nope!\n\nacf(residuals(apple_model3))\n\n\nWe still have time dependence in our residuals!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#adding-a-lag-autoregression",
    "href": "fall_sta235_2025/week_01/week_01.html#adding-a-lag-autoregression",
    "title": "Data Science for Business Applications",
    "section": "Adding a lag (autoregression)",
    "text": "Adding a lag (autoregression)\nLet’s create and add a lag 1 term (\\(\\log(Y_{t-1})\\), the log of last quarter’s earnings):\n\napple &lt;- apple %&gt;% mutate(lag1 = lag(Revenue))\napple_model4 &lt;- lm(log(Revenue) ~ Period + Quarter\n  + PostCOVID + log(lag1), data = apple)\n\n...\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 1.787927   0.484023    3.69       0.00073 ***\nPeriod      0.008745   0.002516    3.48       0.00135 ** \nQuarterQ2   0.000922   0.048638    0.02       0.98497    \nQuarterQ3   0.147878   0.065352    2.26       0.02978 *  \nQuarterQ4   0.521035   0.060089    8.67 0.00000000024 ***\nPostCOVID   0.118981   0.039507    3.01       0.00473 ** \nlog(lag1)   0.468291   0.123846    3.78       0.00057 ***\n..."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#independence-over-time-is-satisfied",
    "href": "fall_sta235_2025/week_01/week_01.html#independence-over-time-is-satisfied",
    "title": "Data Science for Business Applications",
    "section": "Independence (over time) is satisfied!",
    "text": "Independence (over time) is satisfied!\n\nacf(residuals(apple_model4))\n\n\nFor independence to hold:\n\nThe low-lag autocorrelations (1,2,3) should be within the blue, and we should see no clear patterns.\nOccasional values outside the blue (e.g. lags 5,11, etc here) are expected about 5% of the time even under independence"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#forecasting",
    "href": "fall_sta235_2025/week_01/week_01.html#forecasting",
    "title": "Data Science for Business Applications",
    "section": "Forecasting",
    "text": "Forecasting\nOur last observation is from Q2 2022; how can we forecast Q3 2022?\n\ntail(apple, 3)\n\n   Period Time Year Quarter Revenue PostCOVID   lag1\n42     42 2022 2021      Q4  123.94         1  83.36\n43     43 2022 2022      Q1   97.28         1 123.94\n44     44 2022 2022      Q2   82.96         1  97.28\n\n\nHow would the last row change?\n\n\nIncrement Period by 1 (43 -&gt; 44)\nChange Quarter from Q2 to Q3\nPostCOVID = 1 (still)\nlag1 = 82.96 (Revenue for Q2 2022)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#forecasting-1",
    "href": "fall_sta235_2025/week_01/week_01.html#forecasting-1",
    "title": "Data Science for Business Applications",
    "section": "Forecasting",
    "text": "Forecasting\n\nlogpred &lt;- predict(apple_model4,\n  newdata = list(Period = 44, Quarter = \"Q3\",\n    PostCOVID = 1, lag1 = 82.96),\n  interval = \"predict\")\nexp(logpred)\n\n   fit   lwr   upr\n1 90.8 78.26 105.3\n\n\nForecasting tips:\n\nDon’t forget to transform back if you took a log!\nA prediction interval is the right measure of uncertainty, since there will be only one Q3 of 2022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#model-building-strategy",
    "href": "fall_sta235_2025/week_01/week_01.html#model-building-strategy",
    "title": "Data Science for Business Applications",
    "section": "Model building strategy",
    "text": "Model building strategy\n\nStart with a an additive or multiplicative model with trend and seasonal components. (Plot your data! If the seasonal variation increases or decreases over time you’ll want a multiplicative model.)\nExamine the usual diagnostic plots, and plot your residuals as a function of time. Do you need a (different) nonlinear time trend? A transformation of \\(Y\\)?\nCheck your residuals for time dependence If it’s present, is it explained by external factors you can model?\nIf time dependence in the residuals remains, add appropriate lag terms to your model, one at a time."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data",
    "title": "Data Science for Business Applications",
    "section": "Explore the data",
    "text": "Explore the data"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#section-1",
    "href": "fall_sta235_2025/week_01/week_01.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The correlation \\(r\\) between two variables \\(X\\) and \\(Y\\) measures the strength of the linear relationship between them. Correlation ranges from \\(-1\\) (perfect negative relationship) to \\(0\\) (no relationship) to \\(1\\) (perfect positive relationship)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#correlation",
    "href": "fall_sta235_2025/week_01/week_01.html#correlation",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\nThe correlation \\(r\\) between two variables \\(X\\) and \\(Y\\) measures the strength of the linear relationship between them. Correlation ranges from \\(-1\\) (perfect negative relationship) to \\(0\\) (no relationship) to \\(1\\) (perfect positive relationship)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#correlation-1",
    "href": "fall_sta235_2025/week_01/week_01.html#correlation-1",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#correlation-2",
    "href": "fall_sta235_2025/week_01/week_01.html#correlation-2",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\ncor(profs$eval, profs$beauty)\n\n[1] 0.1890391\n\n\n\nHow can we interpret this?\nThe $ sign accesses the variables in the data set profs.csv."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model",
    "href": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Let’s Build a Simple Regression Model",
    "text": "Let’s Build a Simple Regression Model\n\n\\[\n\\text{eval} = \\beta_0 + \\beta_1 \\cdot \\text{beauty} + \\epsilon\n\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are known as coefficients (standard notations)\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope associated with beauty\nThe term \\(\\epsilon\\) (epsilon) accounts for unobserved factors that are not included in this model"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpreting-the-model",
    "href": "fall_sta235_2025/week_01/week_01.html#interpreting-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the Model",
    "text": "Interpreting the Model\n\neval is the response variable (\\(Y\\)); beauty is the predictor variable (\\(X\\)).\nSimple regression uses the best fit line to give us a linear equation to predict \\(Y\\) from \\(X\\):\n\\[\n\\widehat{\\text{eval}} = 3.998 + 0.133 \\cdot \\text{beauty}\n\\]\nWe can predict the evaluation score for someone based on their beauty score just by plugging into the equation.\nWhat do the coefficients mean?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpretation",
    "href": "fall_sta235_2025/week_01/week_01.html#interpretation",
    "title": "Data Science for Business Applications",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nIntercept\n\nWhen the beauty score is zero, the expected evaluation is 3.99 (almost 4).\n\nHere, beauty = 0 represents an “average beauty”.\nImportant, the intercept is evaluated always when the predictor variable is zero.\n\nSlope for Beauty\n\nFor every one-unit increase in the beauty score, there is a 0.133 increase in the professor’s expected evaluation.\n\nIn this context, “expected” refers to the average evaluation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#statistical-significance-of-the-model",
    "href": "fall_sta235_2025/week_01/week_01.html#statistical-significance-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Statistical Significance of the Model",
    "text": "Statistical Significance of the Model\n\nThe population regression line (the best fit line in the population) is \\(Y = \\beta_0 + \\beta_1 X\\) (we can’t know this).\nOur regression equation is the best fit line in the sample, or \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\) (this is what we get from our sample data).\nThe sample intercept and slope \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are our best estimates for the population intercept and slope \\(\\beta_0\\) and \\(\\beta_1\\).\nBut we need to get a sense of how close \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are to \\(\\beta_0\\) and \\(\\beta_1\\)!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#p-values",
    "href": "fall_sta235_2025/week_01/week_01.html#p-values",
    "title": "Data Science for Business Applications",
    "section": "P-values",
    "text": "P-values\n\n\nFor this model, the p-values associated with the coefficients, slope and intercept are close to zero:\n\n\n\n\n\nTerm\np-value - Pr(&gt;|t|)\nSignificance\n\n\n\n\nIntercept\n0.0000000000000002\n***\n\n\nBeauty\n0.0000425\n***\n\n\n\n\n\nIn this case, there’s evidence that beauty has an impact on a professor’s evaluation at a populational level.\nThus, we can conclude that the effect of beauty is statistically significant in relation to the professor’s evaluation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#confidence-intervals",
    "href": "fall_sta235_2025/week_01/week_01.html#confidence-intervals",
    "title": "Data Science for Business Applications",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nLet’s get confidence intervals for the slope and intercept to get a sense of the uncertainty in our estimates:\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) 3.94845765 4.0480866\nbeauty      0.06976869 0.1962342\n\n\n\nSlope: We are 95% confident that the incremental impact of each additional beauty point is between \\(0.07\\) and \\(0.20\\) student evaluation points.\nIntercept: We are 95% confident that the average student evaluation score for average-looking professors (beauty = 0) is between \\(3.95\\) and \\(4.05\\).\nRule of thumb: If zero is inside the CI, the effect is not statistically significant.\nP-values and confidence intervals (CIs) are connected. If the p-value is greater than 0.05, it is likely that zero will be included within the confidence interval."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for",
    "href": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for",
    "title": "Data Science for Business Applications",
    "section": "Rule of Thumb for",
    "text": "Rule of Thumb for"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for-p-values",
    "href": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for-p-values",
    "title": "Data Science for Business Applications",
    "section": "Rule of Thumb for P-values",
    "text": "Rule of Thumb for P-values\n\n\nIf the p-value is smaller than 0.05, we can conclude that the effect is statistically significant.\nOtherwise, if the p-value is greater than 0.05, we conclude that the effect from the predictor is not statistically significant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions",
    "href": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\n\nInterval for a single prediction:\n\nWe are 95% confident that a single professor with a beauty score of 1 will get rated between 3.06 and 5.21.\n\n\npredict(model, list(beauty=1), interval=\"prediction\")\n\n       fit      lwr      upr\n1 4.131274 3.056375 5.206172\n\n\nInterval for an average prediction:\n\nWe are 95% confident that the average rating of all professors with beauty scores of 1 will be between 4.05 and 4.21.\n\n\npredict(model, list(beauty=1), interval=\"confidence\")\n\n       fit      lwr      upr\n1 4.131274 4.050776 4.211771"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions-1",
    "href": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\n\npredict(model, list(beauty=1), interval=\"confidence\")\n\n       fit      lwr      upr\n1 4.131274 4.050776 4.211771\n\n\n\nWe are 95% confident that the will be between 4.05 and 4.21."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#residuals-and-r-squared",
    "href": "fall_sta235_2025/week_01/week_01.html#residuals-and-r-squared",
    "title": "Data Science for Business Applications",
    "section": "Residuals and R-squared",
    "text": "Residuals and R-squared\n\n\nEach instructor has a residual: the difference between their actual and predicted scores (the prediction error)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#residual-standard-error",
    "href": "fall_sta235_2025/week_01/week_01.html#residual-standard-error",
    "title": "Data Science for Business Applications",
    "section": "Residual standard error",
    "text": "Residual standard error\n\n\nThe residual standard error is in the same units as the response variable:\n\nIn this case the RSE is: Residual standard error: 0.5455\nAll predictions made by this model will, on average, differ from the true values by approximately 0.5455, which represents one standard deviation of the residuals.\n\nWe can even get the 95% prediction interval for a single (beauty = 1) prediction as\n\nlower bound: \\(3.998 + 0.133 \\cdot 1 - 2\\times \\text{RSE}\\)\nupper bound: \\(3.998 + 0.133 \\cdot 1 + 2\\times \\text{RSE}\\)\n\n\n\n# lower bound \n3.998 + 0.133 - 2*0.5455\n\n[1] 3.04\n\n# upper bound\n3.998 + 0.133 + 2*0.5455\n\n[1] 5.222"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#r-squared-r2",
    "href": "fall_sta235_2025/week_01/week_01.html#r-squared-r2",
    "title": "Data Science for Business Applications",
    "section": "R-squared (\\(R^2\\))",
    "text": "R-squared (\\(R^2\\))\n\n\nThe \\(R^2\\) provides an understanding of the “fit” of the model in relation to the data.\n\nIf the \\(R^2\\) is close to one, the model has a good fit.\nIf the \\(R^2\\) is close to zero, the model does not provide a good fit for the data.\nMultiple R-squared: 0.03574\nThis indicates not a great fit.\n\nImportant interpretation:\n\nThe \\(R^2\\) represents the percentage of variation in the response variable that can be explained by the predictor.\nFor this model, 3.6% of the variation in evaluation scores can be explained by the beauty variable alone, while the remaining 96.4% is attributed to other unobserved factors."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#adding-more-predictors",
    "href": "fall_sta235_2025/week_01/week_01.html#adding-more-predictors",
    "title": "Data Science for Business Applications",
    "section": "Adding more predictors",
    "text": "Adding more predictors\n\n\nIs the professor’s evaluation explained only by it’s beauty or there might be other factor affecting their evalution?\nLet’s check if the variable age might help us better explain the relationship between beauty and evaluation.\nWe update our model as:\n\n\\[\n\\widehat{\\text{eval}} = \\beta_0 + \\beta_1 \\cdot \\text{beauty} + \\beta_2 \\cdot \\text{age} + \\epsilon\n\\] - We now have a multiple regression model. - Both beauty and age are numerical variables. - The term \\(\\epsilon\\) (epsilon) accounts for unobserved factors that are not included in this model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#multiple-regression-model",
    "href": "fall_sta235_2025/week_01/week_01.html#multiple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression model",
    "text": "Multiple regression model\n\n\n# Build a simple regression model\nmodel &lt;- lm(eval ~ beauty + age, data = profs)\nsummary(model)\n\n\nCall:\nlm(formula = eval ~ beauty + age, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80242 -0.36514  0.07407  0.39913  1.10206 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 3.9844013  0.1337296  29.794 &lt; 0.0000000000000002 ***\nbeauty      0.1340634  0.0337441   3.973            0.0000824 ***\nage         0.0002868  0.0027148   0.106                0.916    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.546 on 460 degrees of freedom\nMultiple R-squared:  0.03576,   Adjusted R-squared:  0.03157 \nF-statistic:  8.53 on 2 and 460 DF,  p-value: 0.0002305\n\n\n\nHow can we interpret this model?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpretation-of-model-coefficients",
    "href": "fall_sta235_2025/week_01/week_01.html#interpretation-of-model-coefficients",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of Model Coefficients",
    "text": "Interpretation of Model Coefficients\n\n\nIntercept\n\nWhen both the beauty score and age are zero, the expected evaluation is 3.98 (almost 4).\n\nHere, beauty = 0 represents an “average beauty,” and age = 0 is not meaningful in this context but is part of the model.\n\nImportant: The intercept is evaluated when all predictor variables (beauty and age) are zero.\n\nSlope for Beauty\n\nFor every one-unit increase in the beauty score, there is a 0.134 increase in the professor’s expected evaluation, holding age constant.\n\nSlope for Age\n\nFor every one-unit increase in age, there is a 0.0003 increase in the professor’s expected evaluation, holding beauty constant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#statistical-significance",
    "href": "fall_sta235_2025/week_01/week_01.html#statistical-significance",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance",
    "text": "Statistical significance\n\n\nFor this model, the p-values associated with the coefficients (intercept, beauty, and age) are as follows:\n\n\n\n\nTerm\np-value - Pr(&gt;|t|)\nSignificance\n\n\n\n\nIntercept\n&lt; 0.0000000000000002\n***\n\n\nBeauty\n0.0000824\n***\n\n\nAge\n0.916\n\n\n\n\n\nIn this model, there is strong evidence that beauty has an impact on a professor’s evaluation at the population level, as its p-value is very close to zero.\nHowever, the p-value for age (0.916 &gt; 0.05) indicates that there is no statistically significant relationship between age and a professor’s evaluation.\nThus, we can conclude that the effect of beauty is statistically significant in relation to a professor’s evaluation, while the effect of age is not statistically significant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#rse-and-r2",
    "href": "fall_sta235_2025/week_01/week_01.html#rse-and-r2",
    "title": "Data Science for Business Applications",
    "section": "RSE and \\(R^2\\)",
    "text": "RSE and \\(R^2\\)\n\n\nThere are no significant changes in relation to the RSE and \\(R^2\\) in relation to the previous model.\nThis means that the quality of the fit and the accuracy of the predictions will be nearly identical to those of the previous model.\nFor this model, 3.6% of the variation in evaluation scores can be explained by the beauty and age variables alone, while the remaining 96.4% is attributed to other unobserved factors.\nWhat about predictions and confidence intervals?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#whats-the-impact-of-gender-on-student-evaluations",
    "href": "fall_sta235_2025/week_01/week_01.html#whats-the-impact-of-gender-on-student-evaluations",
    "title": "Data Science for Business Applications",
    "section": "What’s the Impact of Gender on Student Evaluations?",
    "text": "What’s the Impact of Gender on Student Evaluations?\n\n\nDo you see a difference between men (blue) and women (red)?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#for-for-the-weekend",
    "href": "fall_sta235_2025/week_01/week_01.html#for-for-the-weekend",
    "title": "Data Science for Business Applications",
    "section": "For for the Weekend",
    "text": "For for the Weekend\n\nRead the syllabus.\nDo the first homework assignment in Canvas (covering today’s material).\nDo the first pre-class assignment in Perusall (to prepare for next week’s class)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model-in-rstudio",
    "href": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model-in-rstudio",
    "title": "Data Science for Business Applications",
    "section": "Let’s Build a Simple Regression Model in Rstudio",
    "text": "Let’s Build a Simple Regression Model in Rstudio\n\n\n# Build a simple regression model\nmodel &lt;- lm(eval ~ beauty, data = profs)\nsummary(model)\n\n\nCall:\nlm(formula = eval ~ beauty, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.99827    0.02535 157.727 &lt; 0.0000000000000002 ***\nbeauty       0.13300    0.03218   4.133            0.0000425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 0.00004247"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-eval",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-eval",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: eval",
    "text": "Explore the data: eval"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-beauty",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-beauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: beauty",
    "text": "Explore the data: beauty"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#whats-the-impact-of-gender-on-student-evaluations",
    "href": "fall_sta235_2025/week_01/week_02.html#whats-the-impact-of-gender-on-student-evaluations",
    "title": "Data Science for Business Applications",
    "section": "What’s the impact of gender on student evaluations?",
    "text": "What’s the impact of gender on student evaluations?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model",
    "href": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Incorporating gender into the multiple regression model",
    "text": "Incorporating gender into the multiple regression model\n\nThe model is given by: \\[\n\\texttt{eval} = \\beta_0 + \\beta_1 \\cdot \\texttt{beauty} + \\beta_2 \\cdot \\texttt{gender} + \\epsilon\n\\] Where we have that:\n\n\\(\\texttt{eval}\\): is the response variable - (numerical)\n\\(\\texttt{beauty}\\): is a predictor - (numerical)\n\\(\\texttt{gender}\\): is a predictor - (categorical) - two groups:\n\nfemale\nmale"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model-1",
    "href": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Incorporating gender into the multiple regression model",
    "text": "Incorporating gender into the multiple regression model\n\n\nGender is a categorical variable (male or female in this data set) so we can’t use it as-is as a predictor.\nIdea: Recode gender into the quantitative variable 1 = male, 0 = female. In practice this choice is arbitrary.\nR does this for us!\nIf you put a categorical variable into a model, R will alphabetically select the group that will associated with “0”, and the next to “1”."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model",
    "href": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Run the Regression Model",
    "text": "Run the Regression Model\n\n\noptions(scipen = 999)\nmodel1 &lt;- lm(eval ~ beauty + gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty + gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.88377    0.03866  100.47 &lt; 0.0000000000000002 ***\nbeauty       0.14859    0.03195    4.65           0.00000434 ***\ngendermale   0.19781    0.05098    3.88              0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 0.0000001407"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#regression-a-model-using-gender",
    "href": "fall_sta235_2025/week_01/week_02.html#regression-a-model-using-gender",
    "title": "Data Science for Business Applications",
    "section": "Regression a model using gender",
    "text": "Regression a model using gender\n\nA multiple regression predicting evaluation score from beauty and gender effectively fits two parallel regression lines:"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#is-there-a-generation-gap",
    "href": "fall_sta235_2025/week_01/week_02.html#is-there-a-generation-gap",
    "title": "Data Science for Business Applications",
    "section": "Is there a generation gap?",
    "text": "Is there a generation gap?\n\n\nThe variable generation is either silent (born before 1945), boomer (born 1945-1964), or genx (born after 1965).\nIs generation a significant predictor of evaluations above and beyond gender and beauty?\nTo answer this, we need a model that includes as predictors all of gender, beauty, and generation.\nBut we can’t just create a variable that is 0 for the silent generation, 1 for baby boomers, and 2 for gen X—why not?\nSolution is to pick a “reference category” and create dummy variables for the other categories."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#ok-boomer",
    "href": "fall_sta235_2025/week_01/week_02.html#ok-boomer",
    "title": "Data Science for Business Applications",
    "section": "OK boomer",
    "text": "OK boomer\n\nLet’s arbitrarily pick boomers as a reference category:\n\n\n\nCategory\ngenx\nsilent\n\n\n\n\nBoomers\n0\n0\n\n\nGen Xers\n1\n0\n\n\nSilent Gens\n0\n1\n\n\n\n\nR will do this automatically when you add a categorical variable with 3+ categories to a regression (it will arbitrarily pick a reference category)!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model-1",
    "href": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Run the Regression Model",
    "text": "Run the Regression Model\n\n\nmodel2 &lt;- lm(eval ~ beauty + generation, data=profs)\nsummary(model2)\n\n\nCall:\nlm(formula = eval ~ beauty + generation, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.82584 -0.36581  0.06317  0.42027  1.07809 \n\nCoefficients:\n                 Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       4.02537    0.03201 125.742 &lt; 0.0000000000000002 ***\nbeauty            0.13491    0.03360   4.015            0.0000694 ***\ngenerationgenx   -0.06807    0.06181  -1.101                0.271    \ngenerationsilent -0.08225    0.07889  -1.043                0.298    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 459 degrees of freedom\nMultiple R-squared:  0.03983,   Adjusted R-squared:  0.03355 \nF-statistic: 6.346 on 3 and 459 DF,  p-value: 0.0003192"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#analysis",
    "href": "fall_sta235_2025/week_01/week_02.html#analysis",
    "title": "Data Science for Business Applications",
    "section": "Analysis",
    "text": "Analysis\n\nAll else equal (i.e., among professors of the same gender and beauty):\n\nGen X professors are predicted to get scores that are 0.07 points below those of boomers.\nSilent gen professors are predicted to get scores that are 0.08 points below those of boomers.\nOnly the boomer/silent generation difference is statistically significant; Gen X professors are not significantly different than boomers.\nIn other words: age only seems to matter if you are really old."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#section",
    "href": "fall_sta235_2025/week_01/week_03.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Last week, we predicted evaluation score from beauty and gender, and the model forced the lines to be parallel:\n\n\n\n\n\n\n\n\n\n\n\nWhat if we could build a more flexible model without forcing the lines to be parallel?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#what-is-an-interaction",
    "href": "fall_sta235_2025/week_01/week_03.html#what-is-an-interaction",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "An interaction is an additional term in a regression model that allows the slope of one variable to depend on the value of another."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#does-beauty-matter-more-for-men-or-for-women",
    "href": "fall_sta235_2025/week_01/week_03.html#does-beauty-matter-more-for-men-or-for-women",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "A categorical variable and a quantitative variable\nWe found that for the same level of attractiveness, male professors tend to get higher evaluation scores than female professors\nBut what if the effect of beauty depends on gender (is different for men vs women)?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The idea is to add a new variable that is itself the product of the two variables:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1(\\text{gender}) + \\hat\\beta_2(\\text{beauty}) + \\hat\\beta_3(\\text{gender})(\\text{beauty})\n\\]\n\nFor female professors, male = 0, so the \\(\\beta_1\\) and \\(\\beta_3\\) terms cancel out:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_2(\\text{beauty})\n\\]\n\nFor male professors, male = 1, so we get both a different intercept and a different slope for beauty:\n\n\\[\n\\hat Y = (\\hat\\beta_0 + \\hat\\beta_1) + (\\hat\\beta_2+\\hat\\beta_3)(\\text{beauty})\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender-1",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "model1 &lt;- lm(eval ~ beauty*gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty * gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83820 -0.37387  0.04551  0.39876  1.06764 \n\nCoefficients:\n                  Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)        3.89085    0.03878 100.337 &lt; 0.0000000000000002 ***\nbeauty             0.08762    0.04706   1.862             0.063294 .  \ngendermale         0.19510    0.05089   3.834             0.000144 ***\nbeauty:gendermale  0.11266    0.06398   1.761             0.078910 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5361 on 459 degrees of freedom\nMultiple R-squared:  0.07256,   Adjusted R-squared:  0.0665 \nF-statistic: 11.97 on 3 and 459 DF,  p-value: 0.000000147"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#section-1",
    "href": "fall_sta235_2025/week_01/week_03.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Beauty seems to matter more for men than for women!\nThe gender gap is largest for good-looking professors"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#main-effects-and-interaction-effects",
    "href": "fall_sta235_2025/week_01/week_03.html#main-effects-and-interaction-effects",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "In a model with an interaction term \\(X_1X_2\\), you must also keep the main effects: the variables that are being interacted together.\nThe main effect of \\(X_1\\) represents the predicted increase in \\(Y\\) for a 1-unit change in \\(X_1\\), holding \\(X_2\\) constant at zero.\n\nThe main effect gendermale (0.20) represents the predicted advantage, but only for an average-looking professor (beauty = 0).\n\nThe main effect beauty (0.09) represents the predicted improvement in evaluation scores for each additional beauty point, but only among women (gendermale = 0).\n\nYou can also include other variables in the model that are not being interacted!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-1",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-2",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-2",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#regression-model",
    "href": "fall_sta235_2025/week_01/week_03.html#regression-model",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Let’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model",
    "href": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "How do we interpret this model?\nintercept (baseline), luxury = \\(\"no\"\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\"no\"\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\"yes\"\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model-1",
    "href": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We also have the following interpretation:\nluxury = \\(\\text{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (0) - 0.16\\times \\text{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\text{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (1) - 0.16\\times \\text{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\text{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\text{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#section-2",
    "href": "fall_sta235_2025/week_01/week_03.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#when-should-you-use-interactions-in-a-model",
    "href": "fall_sta235_2025/week_01/week_03.html#when-should-you-use-interactions-in-a-model",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Interactions make a model more complex to analyze and explain, so it’s only worth doing so when you get a substantial bump in \\(R^2\\) by including the interaction.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-neq-correlations-between-predictor-variables",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-neq-correlations-between-predictor-variables",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Instead, interactions let us model a situation where the relationship of one predictor variable and \\(Y\\) is different depending on the value of another \\(X\\) variable:\n\nHow much attractiveness matters for student evaluation scores depends on gender."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#youll-experiment-with-this-in-the-lab",
    "href": "fall_sta235_2025/week_01/week_03.html#youll-experiment-with-this-in-the-lab",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Two categorical variables\nTwo numerical variables"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html",
    "href": "fall_sta235_2025/week_01/week_03.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Last week, we predicted evaluation score from beauty and gender, and the model forced the lines to be parallel:\n\n\n\n\n\n\n\n\n\n\n\nWhat if we could build a more flexible model without forcing the lines to be parallel?\n\n\n\n\n\nAn interaction is an additional term in a regression model that allows the slope of one variable to depend on the value of another.\n\n\n\n\n\nA categorical variable and a quantitative variable\nWe found that for the same level of attractiveness, male professors tend to get higher evaluation scores than female professors\nBut what if the effect of beauty depends on gender (is different for men vs women)?\n\n\n\n\n\nThe idea is to add a new variable that is itself the product of the two variables:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1(\\text{gender}) + \\hat\\beta_2(\\text{beauty}) + \\hat\\beta_3(\\text{gender})(\\text{beauty})\n\\]\n\nFor female professors, male = 0, so the \\(\\beta_1\\) and \\(\\beta_3\\) terms cancel out:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_2(\\text{beauty})\n\\]\n\nFor male professors, male = 1, so we get both a different intercept and a different slope for beauty:\n\n\\[\n\\hat Y = (\\hat\\beta_0 + \\hat\\beta_1) + (\\hat\\beta_2+\\hat\\beta_3)(\\text{beauty})\n\\]\n\n\n\n\n\nmodel1 &lt;- lm(eval ~ beauty*gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty * gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83820 -0.37387  0.04551  0.39876  1.06764 \n\nCoefficients:\n                  Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)        3.89085    0.03878 100.337 &lt; 0.0000000000000002 ***\nbeauty             0.08762    0.04706   1.862             0.063294 .  \ngendermale         0.19510    0.05089   3.834             0.000144 ***\nbeauty:gendermale  0.11266    0.06398   1.761             0.078910 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5361 on 459 degrees of freedom\nMultiple R-squared:  0.07256,   Adjusted R-squared:  0.0665 \nF-statistic: 11.97 on 3 and 459 DF,  p-value: 0.000000147\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeauty seems to matter more for men than for women!\nThe gender gap is largest for good-looking professors\n\n\n\n\n\nIn a model with an interaction term \\(X_1X_2\\), you must also keep the main effects: the variables that are being interacted together.\nThe main effect of \\(X_1\\) represents the predicted increase in \\(Y\\) for a 1-unit change in \\(X_1\\), holding \\(X_2\\) constant at zero.\n\nThe main effect gendermale (0.20) represents the predicted advantage, but only for an average-looking professor (beauty = 0).\n\nThe main effect beauty (0.09) represents the predicted improvement in evaluation scores for each additional beauty point, but only among women (gendermale = 0).\n\nYou can also include other variables in the model that are not being interacted!\n\n\n\n\n\n\nIs there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction.\n\n\n\n\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]\n\n\n\n\n\nLet’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\nHow do we interpret this model?\nintercept (baseline), luxury = \\(\"no\"\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\"no\"\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\"yes\"\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars.\n\n\n\n\n\nWe also have the following interpretation:\nluxury = \\(\\text{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (0) - 0.16\\times \\text{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\text{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (1) - 0.16\\times \\text{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\text{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\text{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope.\n\n\n\n\n\n\n\nThe lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions make a model more complex to analyze and explain, so it’s only worth doing so when you get a substantial bump in \\(R^2\\) by including the interaction.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them!\n\n\n\n\n\nInstead, interactions let us model a situation where the relationship of one predictor variable and \\(Y\\) is different depending on the value of another \\(X\\) variable:\n\nHow much attractiveness matters for student evaluation scores depends on gender.\n\n\n\n\n\n\nTwo categorical variables\nTwo numerical variables"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions---luxury-cars",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions---luxury-cars",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Is there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Linear models are useful:\n\nPrediction - given a new observations\nExplanatory power- which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems-1",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems-1",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#multiple-regression-assumptions",
    "href": "fall_sta235_2025/week_01/week_04.html#multiple-regression-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression assumptions",
    "text": "Multiple regression assumptions\nWe need four things to be true for regression to work properly:\n\nLinearity: \\(Y\\) is a linear function of the \\(X\\)’s (except for the prediction errors).\nIndependence: The prediction errors are independent.\nNormality: The prediction errors are normally distributed.\nEqual Variance: The variance of \\(Y\\) is the same for any value of \\(X\\) (“homoscedasticity”)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity",
    "text": "Non-Linearity\n\nWhat we would expect to observe in a regression where there is a linear relation?\n\n\nlibrary(tidyverse)\nggplot(linear_data, aes(x=X, y=Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#residuals",
    "href": "fall_sta235_2025/week_01/week_04.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nLet’s plot the residuals \\(r_i\\), such that \\[r_i = y_i − \\widehat{y}_i\\] where \\(\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\) vs \\(x_i\\)\nResiduals are basically the distances between the model and the points\nHopefully identify non-linear relationships\nWe are looking for patterns or trends in the residuals"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#residuals-1",
    "href": "fall_sta235_2025/week_01/week_04.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nPlot of the residuals\nHow can these residuals be useful for us?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plots",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plots",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plots",
    "text": "Regression diagnostic plots\nWe’ll use regression diagnostic plots to help us evaluate some of the assumptions.\nThe residuals vs fitted graph plots:\n\nResiduals on the \\(Y\\)-axis\nFitted values (predicted \\(Y\\) values) on the \\(X\\)-axis\n\nThis graph effectively subtracts out the linear trend between \\(Y\\) and the \\(X\\)’s, so we want to see no trend left in this graph."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check non-linearity we focus on the Residual vs. Fitted plot\n\n\nlibrary(ggfortify)\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-1",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nFrom the Residual vs. Fitted plot, we can observe that since the residuals are evenly distributed around zero in relation to the fitted values, we have that the linear regression model is a good fit for this data.\nThis means that we are learning the linear representation contained in this data."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nWhat we would expect to observe if the relation is non linear?\n\n\nggplot(nonlinear_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-1",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-1",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nLet’s look at the residuals for this model\n\n\n\n\n\n\n\n\n\n\n\nLet’s check the residual plot"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-2",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-2",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nlm2 = lm(Y ~ X, data = nonlinear_data)\nautoplot(lm2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-3",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-3",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nFrom the Residual vs. Fitted, we can observe that the residuals are not evenly distributed around zero.\nThis indicates that for lower and higher values of \\(x_i\\) our model is overpredicting and underpredicting in the mid values.\nWhat are the implications in this case?\nWorse predictions"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#independence",
    "href": "fall_sta235_2025/week_01/week_04.html#independence",
    "title": "Data Science for Business Applications",
    "section": "Independence",
    "text": "Independence\n\nIndependence means that knowing the prediction error for one observation doesn’t tell you anything about the error for another observation\nData collected over time are usually not independent\nWe can’t use regression diagnostics to decide the independence\nWe have to measure the autocorrelation of the residuals\nWe’ll get back to autocorrelation when we discuss Time Series models"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-assumption",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-assumption",
    "title": "Data Science for Business Applications",
    "section": "Normality assumption",
    "text": "Normality assumption\n\nWhen we’ve been interpreting residual standard error (RSE) , we’ve used the following interpretation:\n95% of our predictions will be accurate to within plus or minus \\(2\\times RSE\\).\nIn order for this to be true, the residuals have to be Normally distributed"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nWe can check the distribution of the residuals\n\n\nlinear_data = linear_data %&gt;% \n  mutate(resid = residuals(lm1))\n\nggplot(linear_data, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 0.2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example-1",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example-1",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nBut how can we judge if the residuals follows a Normal distribution?\nThe key is to look at the Normal Q-Q plot, which compares the distribution of our residuals to a perfect Normal distribution.\nIf the dots line up along an (approximately) straight line, then the Normality assumption is satisfied."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-2",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-2",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check for Normality we focus on the Normal Q-Q plot\n\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)\n\n\n\n\n\n\n\n\n\nIn this case the normality assumptions seem to be met"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example-2",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example-2",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nLet’s look at different data.\nIn this case the data has non Normal errors."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example-3",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example-3",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\n\nHistogram of the residuals (right skewed)\n\n\n\nlm3 = lm(Y ~ X, data = non_normal)\n\nnon_normal = non_normal %&gt;% \n  mutate(resid = residuals(lm3))\n\nggplot(non_normal, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 1)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-3",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-3",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nautoplot(lm3)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot",
    "href": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Normal Q-Q plot, we can observe that the residuals are not following the line that indicates the Normal quantiles\nThis means that our model results in non-normal residuals\nThis affects statistical tests, and confidence intervals"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#equal-variance",
    "href": "fall_sta235_2025/week_01/week_04.html#equal-variance",
    "title": "Data Science for Business Applications",
    "section": "Equal variance",
    "text": "Equal variance\n\nEqual variance is also known as “homoscedasticity”\nThe variance of \\(Y\\) should be about the same at any \\(X\\) value (or combination of values for the \\(X\\)’s).\nIn other words, the vertical spread of the points should be the same anywhere along the \\(X\\)-axis.\nIf there’s no equal variance then we might have heteroskedasticity.\nLower precision, estimates are further from the correct population value."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#equal-variance-example",
    "href": "fall_sta235_2025/week_01/week_04.html#equal-variance-example",
    "title": "Data Science for Business Applications",
    "section": "Equal variance example",
    "text": "Equal variance example\n\nThe vertical spread of the points is larger along the right side of the graph\n\n\nggplot(heter_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-4",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-4",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check for homoscidacity we focus on the Scale-Location plot\n\n\n\nlm4 = lm(Y ~ X, data = heter_data)\nautoplot(lm4)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot-1",
    "href": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Sacle-Location plot, we can observe that the residuals have a fan shape, indicating that there is heteroscedacity in the data.\nThis resulted in lower precision; thus, estimates are further from the correct population value."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#influential-observations",
    "href": "fall_sta235_2025/week_01/week_04.html#influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Influential observations",
    "text": "Influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\n\n\n\n\n\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#diagnostics-plot",
    "href": "fall_sta235_2025/week_01/week_04.html#diagnostics-plot",
    "title": "Data Science for Business Applications",
    "section": "Diagnostics Plot",
    "text": "Diagnostics Plot\n\nWe can observe the point with high residual on the Residual vs. Leverage plot\n\n\nlm5 = lm(Y ~ X, data = outlier_residual)\nautoplot(lm5)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#high-leverage",
    "href": "fall_sta235_2025/week_01/week_04.html#high-leverage",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can also have points with high leverage - when a point in \\(X\\) is distant from the average on \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\nThis point has low residual but high leverage. RSE = 0.2956"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#high-leverage-1",
    "href": "fall_sta235_2025/week_01/week_04.html#high-leverage-1",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can observe the point with high leverage on the Residual vs. Leverage plot\n\n\nlm6 = lm(Y ~ X, data = outlier_leverage)\nautoplot(lm6)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence",
    "href": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nPoints with high leverage and high residuals are known as influential points\n\n\n\n\n\n\n\n\n\n\n\nThis point has high residual but high leverage. RSE = 0.8281"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-1",
    "href": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-1",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWe can observe the point with high influence on the Residual vs. Leverage plot\n\n\nlm7 = lm(Y ~ X, data = outlier_influence)\nautoplot(lm7)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-2",
    "href": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-2",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWhen a case has a very unusual \\(X\\) value, it has leverage — the potential to have a big impact on the regression line\nIf the case is in line with the overall trend of the regression line, it won’t be a problem\nBut when that case also has a Y (high residual) value that is out of line\nWe need both a large residual and high leverage for an observation to be influential\nWe should be worried about these points\nThey affect the coefficents and predictions"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nThe data set utilities contains information on the utility bills for a house in Minnesota. We’ll focus on two variables:\n\ndailyspend is the average amount of money spent on utilities (e.g. heating) for each day during the month\ntemp is the average temperature outside for that month"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-1",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-1",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWhat problems do you see here?\n\nlibrary(tidyverse)\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point()"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-2",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-2",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm1 &lt;- lm(dailyspend ~ temp, data=utilities) \nsummary(lm1)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59 &lt;0.0000000000000002 ***\ntemp        -0.096432   0.003911  -24.66 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nLet’s interpret this relation\nFor one unit increase in temperature (Fahrenheit), there will be a 7-cent decrease in spending"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-3",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-3",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlibrary(ggfortify)\nautoplot(lm1)\n\n\n\nLinearity and homoscedasticity are violated"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-4",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-4",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe’ll use polynomial regression to fix problems\nIf a polynomial curve (e.g., quadratic, cubic, etc) would be a better fit for the data than a line, we can fit a curve to the data.\nThe way we do this is by adding \\(X^2\\) to the model as a second predictor variable.\nThis can “fix” the linearity problem because now \\(Y\\) is a linear function of \\(X\\) and \\(X^2\\), resulting in: \\[\nY = \\beta_0 + \\beta_1\\cdot X + \\beta\\cdot X^2 + e\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-5",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-5",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe add the term I(temp^2) in the regression equation:\n\n\nlm2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities) \nsummary(lm2)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239 &lt; 0.0000000000000002 ***\ntemp        -0.2115553  0.0191046 -11.074 &lt; 0.0000000000000002 ***\nI(temp^2)    0.0012476  0.0002037   6.124         0.0000000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nWe have that the new term is evaluated as an extra variable."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-6",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-6",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWriting out the equation: \\[\n\\widehat{\\texttt{dailyspend}} = 9.4723 −0.2116\\cdot \\texttt{temp} + 0.0012\\cdot \\texttt{temp}^2\n\\] The effect of the extra variable is statistically significant:\n\nconfint(lm2)\n\n                    2.5 %       97.5 %\n(Intercept)  8.6981381712 10.246438869\ntemp        -0.2494014032 -0.173709160\nI(temp^2)    0.0008440041  0.001651114\n\n\n\nThe residual standard error of the polynomial model is \\(\\texttt{0.75}\\).\nThe residual standard error of the linear model is \\(\\texttt{0.87}\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-7",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-7",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nAdding an \\(X^2\\) term fits a parabola to the data (orange line)\n\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm1)), col = \"lightblue\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-8",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-8",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nIt solves the linearity problem\n\nautoplot(lm2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-9",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-9",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWhat about a higher-order polynomial?\nWe could fit a cubic curve by adding an \\(X^3\\) term\nMaking the polynomial higher order will decrease the RSE\nWhy not go nuts and fit a 7th degree polynomial?\n\n\n\n\nDegree\nname\nRSE\n\n\n\n\n1\nlinear\n0.866\n\n\n2\nquadratic\n0.754\n\n\n3\ncubic\n0.755\n\n\n4\nquartic\n0.755\n\n\n5\nquintic\n0.758\n\n\n6\n\n0.761\n\n\n7\n\n0.761"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-10",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-10",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) +\nI(temp^5) + I(temp^6) + I(temp^7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#building-polynomial-models",
    "href": "fall_sta235_2025/week_01/week_05.html#building-polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Building polynomial models",
    "text": "Building polynomial models\nStart simple: only add higher-degree terms to the extent it gives you a substantial decrease in the RSE, or satisfies an assumption hold that wasn’t satisfied before\n\nYou must include lower-order terms: e.g., if you add \\(X^3\\), you must also include \\(X\\) and \\(X^2\\)\nBe careful about overfitting when adding higher-order terms!\nBe particularly careful about extrapolating beyond the range of the data!\nMind-bender: We can think about an \\(X^2\\) term as an interaction of \\(X\\) with itself: in a parabola, the slope depends on the value of \\(X\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#the-log-transformation",
    "href": "fall_sta235_2025/week_01/week_05.html#the-log-transformation",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nWe saw that we can use transformations to fix problems\nSometimes, a violation of regression assumptions can be fixed by transforming one or the other of the variables (or both).\nWhen we transform a variable, we have to also transform our interpretation of the equation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-1",
    "href": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-1",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\nThe log transformation is frequently useful in regression, because many nonlinear relationships are naturally exponential.\n\n\\(\\log_b x=y\\) when \\(b^y=x\\)\nFor example, \\(\\log_{10} 1000 = 3\\), \\(\\log_{10}100 = 2\\), and \\(\\log_{10}10 = 1\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-2",
    "href": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-2",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!)\nSkewed data is also a good candidate for log"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#moores-law",
    "href": "fall_sta235_2025/week_01/week_05.html#moores-law",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nMoore’s Law was a prediction made by Gordon Moore in 1965 (!) that the number of transistors on computer chips would double every 2 years\nThis implies exponential growth, so a linear model won’t fit well (and neither will any polynomial)\n\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#moores-law-1",
    "href": "fall_sta235_2025/week_01/week_05.html#moores-law-1",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nlm_moore = lm(Transistor.count ~ Date.of.introduction, data = moores)\nautoplot(lm_moore)\n\n\n\nA linear model is a spectacular fail"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth",
    "href": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\nIf \\(Y = ae^{bX}\\), then\n\\[\\log(Y) = \\log(a)+ bX\\]\n\nIn other words, \\(\\log(Y)\\) is a linear function of \\(X\\) when \\(Y\\) is an exponential function of \\(X\\)\nSo if we think \\(Y\\) is an exponential function of \\(X\\), predict \\(\\log(Y)\\) as a linear function of \\(X\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-1",
    "href": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nTransistors does NOT have a linear relationship with year\n\\(\\log(\\texttt{Transistors})\\) does have a linear relationship with year\n\n\nggplot(moores, aes(x = Date.of.introduction, y = log(Transistor.count))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#log-linear-model",
    "href": "fall_sta235_2025/week_01/week_05.html#log-linear-model",
    "title": "Data Science for Business Applications",
    "section": "Log-linear Model",
    "text": "Log-linear Model\nLet’s run the regression model\n\noptions(scipen = 999)\nlm_moore = lm(log(Transistor.count) ~ Date.of.introduction, data = moores)\nsummary(lm_moore)\n\n\nCall:\nlm(formula = log(Transistor.count) ~ Date.of.introduction, data = moores)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1299 -0.3338  0.1767  0.5230  2.0626 \n\nCoefficients:\n                        Estimate  Std. Error t value            Pr(&gt;|t|)    \n(Intercept)          -681.212056   15.958165  -42.69 &lt;0.0000000000000002 ***\nDate.of.introduction    0.349154    0.007981   43.75 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.054 on 99 degrees of freedom\nMultiple R-squared:  0.9508,    Adjusted R-squared:  0.9503 \nF-statistic:  1914 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-2",
    "href": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-2",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nautoplot(lm_moore)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\nOur model is \\[\\widehat{\\log(\\texttt{Transistors})} = −681.21 + 0.35 \\cdot \\texttt{Year}\\]\nTwo interpretations of the slope coefficient:\n\nEvery year, the predicted log of transistors goes up by 0.35\nMore useful: \\(\\exp(0.35\\cdot1)-\\exp(0.35\\cdot0) = \\exp(0.35\\cdot1)-1 =  0.419\\)\nWe then have \\(0.419\\times 100 = 41.9\\%\\) increase for each year.\nA constant percentage increase every year is exponential growth!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-1",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nMaking predictions using the log-linear model\nWhen making predictions, we have to remember that our equation gives us predictions for \\(\\log(\\texttt{Transistors})\\), not Transistors!\n\nExample: To make a prediction for the number of transistors in 2022: \\[\n\\log(\\texttt{Transistors}) = −681.21 + 0.35\\cdot(2025) = 27.54\n\\] But our prediction is not 27.54:\n\\(e^{\\log(\\texttt{Transistors})} = \\exp(27.54) = e^{27.54} = 912,998,431,886\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-2",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_line(aes(x = Date.of.introduction, y = exp(predict(lm_moore))), col = \"orange\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-3",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\n\n\n\n\nModel\nEquation\nInterpretation\n\n\n\n\nLinear\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies \\(\\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-linear\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies ≈ \\(100 \\cdot (\\exp(\\widehat{\\beta}_1)-1) \\%\\) increase in \\(\\widehat{Y}\\)\n\n\nLinear-log\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(0.01 \\cdot \\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-log\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(\\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#conclusion",
    "href": "fall_sta235_2025/week_01/week_05.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhen is the log transformation useful?\nYou can transform \\(X \\rightarrow \\log(X)\\), \\(Y \\rightarrow \\log(Y)\\), or both\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!), try transforming it with a log\nIn this case, Transistors is skewed right so it is a good candidate for log\nYou may need to do a little bit of trial and error to see what works best\nOther transformations are possible!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html",
    "href": "fall_sta235_2025/week_01/week_04.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Linear models are useful:\n\nPrediction - given a new observations\nExplanatory power- which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  }
]