[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Henrique Bolfarine",
    "section": "",
    "text": "Welcome to my homepage! I’m a lecturer of statistics and data science at the McCombs School of Business at the University of Texas at Austin. I obtained my Ph.D. in Statistics at the Institute of Mathematics and Statistics of the University of São Paulo, advised by Professors Hedibert Lopes and Carlos Carvalho, and worked as a postdoctoral researcher at the McCombs School of Business, advised by Professor Carlos Carvalho."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Research",
    "section": "",
    "text": "“Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models.” H. Bolfarine, H.F. Lopes, & C.M. Carvalho. https://arxiv.org/abs/2506.09850 - (Accepted - Statistic and Computing)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Henrique Bolfarine",
    "section": "Education",
    "text": "Education\nUniversidade de São Paulo (USP), São Paulo\nPhD in Statistics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Henrique Bolfarine",
    "section": "",
    "text": "Welcome to my homepage! I’m a lecturer of statistics and data science at the McCombs School of Business at the University of Texas at Austin. I obtained my Ph.D. in Statistics at the Institute of Mathematics and Statistics of the University of São Paulo, advised by Professors Hedibert Lopes and Carlos Carvalho, and worked as a postdoctoral researcher at the McCombs School of Business, advised by Professor Carlos Carvalho."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "STA 235 - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Error, Uncertainty and the Linear Model\nclass 03 - Regression Assumptions\nclass 04 - Categorical Variables\nclass 05 - Interactions\nclass 06 - Modeling nonlinear relationships\nclass 07 - Time series modelling\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables\nclass 03 - Interactions\nclass 04 - Regression Assumptions\nclass 05 - Modeling nonlinear relationships\nclass 06 - Time Series Decomposition and Autoregression\nclass 07 - Model building for prediction\nclass 08 - Logistic Regression 1\nclass 09 - Logistic Regression 2\nclass 10 - Decision Trees\nclass 11 - Random Forest\nclass 12 - Building models for explanation\nclass 13 - Basic Causal Inference\nclass 14 - Natural experiments and RDD\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 Honors - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables and Interactions\nclass 03 - Regression assumptions and Outliers\nclass 04 - Modeling nonlinear relationships\nclass 05 - Time Series Decomposition and Autoregression\nclass 06 - Model Selection\nclass 07 - Logistic Regression\nclass 08 - Basic Causal Inference\nclass 09 - Randomized Control Trials\nclass 10 - Natural Experiments - Diff-in-Diff\nclass 11 - Natural Experiments - RDD\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Application\n\n\n\n\n\nSTA 235 - Data Science for Business Application"
  },
  {
    "objectID": "teaching.html#teaching",
    "href": "teaching.html#teaching",
    "title": "Teaching",
    "section": "",
    "text": "Modifcation"
  },
  {
    "objectID": "class_01.html#what-is-data-science",
    "href": "class_01.html#what-is-data-science",
    "title": "Data Science for Business Applications",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\nIn Data Science we use the available data to obtain:"
  },
  {
    "objectID": "class_01.html#data-science-tasks",
    "href": "class_01.html#data-science-tasks",
    "title": "Data Science for Business Applications",
    "section": "Data Science tasks",
    "text": "Data Science tasks\n\nDescription: Can we classify our customers into different segments? (simple task)\nPrediction: What is the probability of a shopper coming back to our website? (kind of a simple task)\nCausal Inference: What is the effect of increasing our advertising budget on our total revenue? (difficult task)"
  },
  {
    "objectID": "class_01.html#simple-and-multiple-regression",
    "href": "class_01.html#simple-and-multiple-regression",
    "title": "Data Science for Business Applications",
    "section": "Simple and Multiple Regression",
    "text": "Simple and Multiple Regression\n\nLinear Regression will be the most important tool for solving these Data Science tasks.\nBasically, in the linear regression model, we are explaining the relation between different variables by a line (that’s where the linear comes from.)\nMany fancy methods are generalizations or extensions of Linear Regression!\nIn this class, we will do a quick review on linear regression."
  },
  {
    "objectID": "class_01.html#cookie-example",
    "href": "class_01.html#cookie-example",
    "title": "Data Science for Business Applications",
    "section": "Cookie Example",
    "text": "Cookie Example\n\n\n\nSuppose we have some data, and we want to understand how happiness changes in relation to the number of cookies eaten.\nTo do so, we summarize the relation between these two variables through a line.\nThis line is the linear regression line.\nLet’s see how this works!\n\n\n\n\n\ncookies\nhappiness\n\n\n\n\n1\n0.1\n\n\n2\n2\n\n\n3\n1\n\n\n4\n2.5\n\n\n5\n3\n\n\n6\n1.3\n\n\n7\n1.9\n\n\n8\n2.4\n\n\n9\n1.8\n\n\n10\n3"
  },
  {
    "objectID": "class_01.html#cookie-example-1",
    "href": "class_01.html#cookie-example-1",
    "title": "Data Science for Business Applications",
    "section": "Cookie Example",
    "text": "Cookie Example\n\n\n\nThe regression line is defined by two values, the intercept and the slope.\nThe intercept is where the line intercepts the happiness axis.\nThe slope relates to the inclination of the line.\nIf the slope is positive the line has a upward direction.\nIf the slope is negative the line has a downward direction.\nThe regression line is a model of the relation between happiness and the number of cookies eaten."
  },
  {
    "objectID": "class_01.html#some-questions-on-regression",
    "href": "class_01.html#some-questions-on-regression",
    "title": "Data Science for Business Applications",
    "section": "Some questions on regression",
    "text": "Some questions on regression\n\nFrom the regression line, what is the relationship between happiness and the number of cookies eaten?\nAre there other factors other than the number of cookies that might affect the happiness level?\nFrom this model, can we conclude that eating cookies alone causes happiness?\nWhy not look only at the correlation between happiness and the number of cookies?\nHow do we obtain the intercept and the slope?\nDoes this result apply to the entire population?"
  },
  {
    "objectID": "class_01.html#regressions-details",
    "href": "class_01.html#regressions-details",
    "title": "Data Science for Business Applications",
    "section": "Regressions Details",
    "text": "Regressions Details\n\nThe Linear Regression model is represented by the formula: \\[\nY = \\beta_0 + \\beta_1\\cdot X_1 + \\beta_2\\cdot X_2 + e\n\\]\nMultiple Regression means we have two or more \\(X\\)’s.\nLet’s break down this model into its essential parts."
  },
  {
    "objectID": "class_01.html#essential-parts-of-a-regression",
    "href": "class_01.html#essential-parts-of-a-regression",
    "title": "Data Science for Business Applications",
    "section": "Essential Parts of a Regression",
    "text": "Essential Parts of a Regression\n\n\\(Y\\) - Outcome Variable, Response Variable, Dependent Variable (Thing you want to explain or predict)\n\\(X\\) - Explanatory Variable, Predictor Variable, Independent Variable, Covariate (Thing you use to explain or predict \\(Y\\))\n\\(\\beta\\)’s - Coefficients, Parameters (How \\(Y\\) changes numerically in relation to \\(X\\))\n\\(e\\) - Residual, Noise (Things we didn’t account in our model)"
  },
  {
    "objectID": "class_01.html#two-purposes-of-regression",
    "href": "class_01.html#two-purposes-of-regression",
    "title": "Data Science for Business Applications",
    "section": "Two Purposes of Regression",
    "text": "Two Purposes of Regression"
  },
  {
    "objectID": "class_01.html#back-to-the-cookie-example",
    "href": "class_01.html#back-to-the-cookie-example",
    "title": "Data Science for Business Applications",
    "section": "Back to the Cookie example",
    "text": "Back to the Cookie example\n\nBy writing the cookie example as a model where the variable happiness is the response variable and the number of cookies is the predictor, we have \\[\n\\texttt{happiness} = \\beta_0 + \\beta_1\\cdot\\texttt{cookies} + e\n\\]\nhappiness is the response variable (\\(Y\\)).\ncookies is the predictor variable (\\(X\\)).\n\\(\\beta_0\\) is the intercept.\n\\(\\beta_1\\) is the slope associate to the cookies variable.\n\\(e\\) are the unknown factors that might explain the relation between cookies and happiness.\nThe challenge now is to estimate the parameters \\(\\beta_0\\), and \\(\\beta_1\\)."
  },
  {
    "objectID": "class_01.html#how-do-we-estimate-the-coefficients-in-a-regression",
    "href": "class_01.html#how-do-we-estimate-the-coefficients-in-a-regression",
    "title": "Data Science for Business Applications",
    "section": "How do we estimate the coefficients in a regression?",
    "text": "How do we estimate the coefficients in a regression?\n\nA very useful strategy is use what is called the Ordinary Least Squares (OLS).\nThe method is called ordinary least squares because the algorithm selects the coefficients (\\(\\beta\\)’s) that minimize the sum of the squares of the errors in our sample.\nSo the data in this case is fundamental.\nWe use R to learn \\(\\beta\\).\nThe estimated coefficients are referred to as \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "class_01.html#lets-get-into-some-data",
    "href": "class_01.html#lets-get-into-some-data",
    "title": "Data Science for Business Applications",
    "section": "Let’s get into some data",
    "text": "Let’s get into some data\n\nExample: Movie data Set (movie_1990_data.csv)\nWe will create a model that explains and predicts the movie revenue in terms of the budget.\nThere are 1,368 different movies in the data, with 22 different attributes.\nThis means that the data contains 1,368 lines and 22 columns.\nWe are interested in two attributes, the movie budget, and movie revenue.\nMovie budget is in the predictor variable - Adj_Budget.\nMovie revenue is in the response variable - Adj_Revenue.\nThe units in this case are important (both are in millions of dollars).\nLet’s visualize the relation between these two variables."
  },
  {
    "objectID": "class_01.html#section",
    "href": "class_01.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "First we load the library tidyverse, then we use the ggplot function to make plot between Adj_Budget and Adj_Revenue\n\n\n# Load library\nlibrary(tidyverse)\n\n# Create plot\nggplot(movie_1990_data) +\n  geom_point(aes(x = Adj_Budget, y = Adj_Revenue))"
  },
  {
    "objectID": "class_01.html#section-1",
    "href": "class_01.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We encode the model below in R.\n\\[\n\\texttt{Adj_Revenue} = \\beta_0 + \\beta_1\\cdot\\texttt{Adj_Budget} + e\n\\]\n\n# The model\n# Revenue = intercept + slope*Budget + e\nlm1 &lt;- lm(Adj_Revenue ~ Adj_Budget, data = movie_1990_data)\nsummary(lm1)\n\n\nCall:\nlm(formula = Adj_Revenue ~ Adj_Budget, data = movie_1990_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-262.40  -38.01  -16.39   19.24  619.23 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 22.66095    3.15001   7.194 1.03e-12 ***\nAdj_Budget   1.11043    0.03738  29.709  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 79.78 on 1366 degrees of freedom\nMultiple R-squared:  0.3925,    Adjusted R-squared:  0.3921 \nF-statistic: 882.6 on 1 and 1366 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_01.html#lets-interpret-the-output",
    "href": "class_01.html#lets-interpret-the-output",
    "title": "Data Science for Business Applications",
    "section": "Let’s interpret the output",
    "text": "Let’s interpret the output\n\nFrom the coefficients, \\(\\widehat\\beta_0 = 22.7\\), \\(\\widehat\\beta_1 = 1.11\\) we have the updated model: \\[\n\\widehat{\\texttt{Adj_Revenue}} = 22.7 + 1.11\\cdot\\texttt{Adj_Budget}\n\\]\nWhen there’s a hat, it means that the values were generated from the data.\n\\(e\\) (noise) vanishes because we eliminated the unknown factors and concentrated the effect on what we observe.\nNow we have the residuals, that is the distance between the points in the data and the regression model.\nThe question now is: how can we interpret this result?"
  },
  {
    "objectID": "class_01.html#explanation",
    "href": "class_01.html#explanation",
    "title": "Data Science for Business Applications",
    "section": "Explanation",
    "text": "Explanation\n\nIntercept: By setting the movie budget to zero, we have that the average revenue is equal to $22.7 million dollars.\nSlope: For one unit change in the movie budget, that is, millions, there will be an increase of $1.11 million dollars in the movie’s revenue."
  },
  {
    "objectID": "class_01.html#section-2",
    "href": "class_01.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Let’s visualize this model.\n\n# Create plot with regression line\nggplot(movie_1990_data) +\n  geom_point(aes(x = Adj_Budget, y = Adj_Revenue)) +\n  geom_smooth(aes(x = Adj_Budget, y = Adj_Revenue), method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model",
    "href": "class_01.html#statistical-significance-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nAre these coefficients statistically significant?\nCan we extrapolate the results to the larger population?\nWe can answer these questions by looking at the confidence interval (CI).\n\n\n# We use the confint() function to get the confidence interval\nconfint(lm1)\n\n                2.5 %    97.5 %\n(Intercept) 16.481572 28.840332\nAdj_Budget   1.037112  1.183756"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-1",
    "href": "class_01.html#statistical-significance-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\nFrom the confidence interval we have that:\n\n\n\nThe value of the intercept is statistically different from zero since zero is not between the lower and upper values of the interval .\nThe value of the slope is statistically different from zero since zero is not between the lower and upper values of the interval .\n\n\n\nWith 95% confidence, the value of the intercept at the population level is between 16.5 and 28.8 million dollars .\nWith 95% confidence, the value of the slope at the population level is between 1.04 and 28.8 million dollars ."
  },
  {
    "objectID": "class_01.html#predictions",
    "href": "class_01.html#predictions",
    "title": "Data Science for Business Applications",
    "section": "Predictions",
    "text": "Predictions\n\nSuppose we want to predict the revenue of a movie , knowing that the revenue was $25 million.\nWe can use our estimated model to make the prediction.\nInput the value into the adjusted budget, resulting in: \\[\n\\widehat{\\texttt{Adj_Revenue}} = 22.7 + 1.11\\cdot 25 = 50.45\n\\]\nThe average predicted revenue of a $25 million dollar budget movie is $50 million dollars.\nWe can also do this using the predict function in R\n\n\n# We use the predict() function to get predictions\npredict(lm1, list(Adj_Budget = 25))\n\n       1 \n50.42179"
  },
  {
    "objectID": "class_01.html#predictions-1",
    "href": "class_01.html#predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Predictions",
    "text": "Predictions\n\nHow good are these predictions?\nWe can use the residual standard error (RSE), which is a result of the summary function.\n\\(\\texttt{Residual standard error: 79.78}\\),\nThese mean that our predictions will be off by approximately 79.78.\nWith 95% confidence, the prediction of the revenue will be 50.42 plus and minus \\(2\\times 79.78 = 159.56\\).\nQuite a big variation."
  },
  {
    "objectID": "class_01.html#adding-more-variables",
    "href": "class_01.html#adding-more-variables",
    "title": "Data Science for Business Applications",
    "section": "Adding more variables",
    "text": "Adding more variables\n\nWe can add more variables in our model.\nWe will add the variable imdbRating which encodes the different IMDB ratings in the data (1-10).\nThe resulting model now is \\[\n\\texttt{Adj_Revenue} = \\beta_0 + \\beta_1\\cdot\\texttt{Adj_Budget} + \\beta_2\\cdot\\texttt{imdbRating} + e\n\\]\nYou can observe that we have an extra slope in the equation.\nThis will have an impact in how we interpret the model.\nNext we encode this model in R"
  },
  {
    "objectID": "class_01.html#explore-the-data-beauty",
    "href": "class_01.html#explore-the-data-beauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: beauty",
    "text": "Explore the data: beauty"
  },
  {
    "objectID": "class_01.html#relation-bewteen-variables",
    "href": "class_01.html#relation-bewteen-variables",
    "title": "Data Science for Business Applications",
    "section": "Relation bewteen variables",
    "text": "Relation bewteen variables\n\n\nLet’s interpret the model:\n\n\n\\[\n\\texttt{Adj_Revenue} = -136.507 + 1.091 \\cdot\\texttt{Adj_Budget} + 24.099\\cdot\\texttt{imdbRating}\n\\]"
  },
  {
    "objectID": "class_01.html#correlation",
    "href": "class_01.html#correlation",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\nRegressions are super useful…\nBut you need to know how to interpret them.\nBe sure not to overstate your claims!\nRemember the magic words for interpretation"
  },
  {
    "objectID": "class_01.html#correlation-1",
    "href": "class_01.html#correlation-1",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "class_01.html#correlation-between-variables",
    "href": "class_01.html#correlation-between-variables",
    "title": "Data Science for Business Applications",
    "section": "Correlation between variables",
    "text": "Correlation between variables\n\nLet’s calculate the correlation between evaluation and beauty\n\n\ncor(profs$eval, profs$beauty)\n\n[1] 0.1890391\n\n\n\nHow can we interpret this?\nInstead of trying to interpret the correlation, we can build a model that reveals the relationship between the professor’s evaluation and their beauty score."
  },
  {
    "objectID": "class_01.html#simple-regression-model",
    "href": "class_01.html#simple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Simple regression model",
    "text": "Simple regression model\n\nmodel &lt;- lm(eval ~ beauty, data=profs)\nsummary(model)\n\n\nCall:\nlm(formula = eval ~ beauty, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.99827    0.02535 157.727  &lt; 2e-16 ***\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05"
  },
  {
    "objectID": "class_01.html#interpreting-the-model",
    "href": "class_01.html#interpreting-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\neval is the response variable (\\(Y\\));\nbeauty is the predicton variable (\\(X\\))\nSimple regression uses the best fit line to give us a linear equation to predict \\(Y\\) from \\(X\\):\n\n\\[\n\\text{eval} = 3.998 + 0.133\\cdot \\text{beauty}\n\\]\n\nWe can predict the evaluation score for someone based on their beauty score just by plugging into the equation.\nWhat do the coefficients mean?"
  },
  {
    "objectID": "class_01.html#interpreting-the-model-1",
    "href": "class_01.html#interpreting-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model"
  },
  {
    "objectID": "class_01.html#interpreting-the-model-2",
    "href": "class_01.html#interpreting-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\nWe can predict the evaluation score for someone based on their beauty score just by plugging into the equation.\nWhat do the coefficients mean?\nBut what does the population that it was drawn from look like?"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-2",
    "href": "class_01.html#statistical-significance-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nLet’s analyse the confidence interval\n\n\n# We use the confint() function to get the confidence interval\nconfint(lm2)\n\n                  2.5 %      97.5 %\n(Intercept) -165.245169 -107.768758\nAdj_Budget     1.020706    1.161364\nimdbRating    19.841475   28.357234\n\n\n\nAll coefficients are statistically significant."
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-3",
    "href": "class_01.html#statistical-significance-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nThe population regression line (the best fit line in the population) is \\[Y = \\beta_0 + \\beta_1 X + \\text{noise}\\]\nUsually we don’t have access to the entire population, so we can’t know this\nThe noise term represents what we haven’t accounted for in our model"
  },
  {
    "objectID": "class_01.html#statistical-significance-of-the-model-4",
    "href": "class_01.html#statistical-significance-of-the-model-4",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance of the model",
    "text": "Statistical significance of the model\n\nOur regression equation is the best fit line in the sample, or \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\]\nThis is what we get from our sample data\nThe sample intercept and slope \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\)\nare our best estimates for the population intercept\nand slope \\(\\beta_0\\) and \\(\\beta_1\\)\nBut we need to get a sense of how close \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\) are to \\(\\beta_0\\) and \\(\\beta_1\\)!"
  },
  {
    "objectID": "class_01.html#confidence-intervals",
    "href": "class_01.html#confidence-intervals",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nLet’s form confidence intervals for the slope and intercept to get a sense of the uncertainty in our estimates:\n\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) 3.94845765 4.0480866\nbeauty      0.06976869 0.1962342"
  },
  {
    "objectID": "class_01.html#confidence-intervals-1",
    "href": "class_01.html#confidence-intervals-1",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\\(\\beta_1\\): We are 95% confident that the incremental impact of each additional beauty point is between \\(0.07\\) and \\(0.20\\) student evaluation points.\n\\(\\beta_0\\): We are 95% confident that the average student evaluation score for average-looking professors (beauty = 0) is between \\(3.95\\) and \\(4.05\\)"
  },
  {
    "objectID": "class_01.html#confidence-intervals-for-predictions",
    "href": "class_01.html#confidence-intervals-for-predictions",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\nConfidence for a given prediction\n\n\npredict(model, list(beauty=1), interval=\"prediction\")\n\n       fit      lwr      upr\n1 4.131274 3.056375 5.206172\n\n\n\nWe are 95% confident that a single professor with a beauty score of 1 will get rated between 3.06 and 5.21."
  },
  {
    "objectID": "class_01.html#confidence-intervals-for-predictions-1",
    "href": "class_01.html#confidence-intervals-for-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\nConfidence for the average prediction\n\n\npredict(model, list(beauty=1), interval=\"confidence\")\n\n       fit      lwr      upr\n1 4.131274 4.050776 4.211771\n\n\n\nWe are 95% confident that the will be between\n4.05 and 4.21"
  },
  {
    "objectID": "class_01.html#residuals",
    "href": "class_01.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nEach instructor has a residual: the difference between their actual and predicted scores (the prediction error)."
  },
  {
    "objectID": "class_01.html#residuals-1",
    "href": "class_01.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nThis instructor has a positive residual—their evaluation score was higher than expected given their beauty:"
  },
  {
    "objectID": "class_01.html#residuals-2",
    "href": "class_01.html#residuals-2",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nThis instructor has a negative residual—their evaluation score was lower than expected given their beauty:"
  },
  {
    "objectID": "class_01.html#residuals-3",
    "href": "class_01.html#residuals-3",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nThe residuals are approximately Normally distributed with a mean of 0:"
  },
  {
    "objectID": "class_01.html#practical-significance-of-the-model",
    "href": "class_01.html#practical-significance-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Practical significance of the model",
    "text": "Practical significance of the model\n\nWe know that there is a statistically significant relationship between beauty and eval, so we can be highly confident they are indeed related in the larger population. But it that relationship meaningful?\nThe of \\(0.545\\) tells us that the standard deviation of the residuals is about \\(0.545\\) points"
  },
  {
    "objectID": "class_01.html#practical-significance-of-the-model-1",
    "href": "class_01.html#practical-significance-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Practical significance of the model",
    "text": "Practical significance of the model\n\nThat means that 95% of the residuals are less than \\(2\\cdot 0.545=1.09\\) (since 95% of a Normal distribution is within \\(\\pm 2\\) SD of the mean)\nIn other words, 95% of the time when using beauty to predict evaluation scores, we’ll be off by\nless than 1.09 point"
  },
  {
    "objectID": "class_01.html#practical-significance-of-the-model-2",
    "href": "class_01.html#practical-significance-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Practical significance of the model",
    "text": "Practical significance of the model\n\nThe \\(R^2\\) of \\(0.0357\\) tells us that about 4% of the variation between professors in student evaluations can be explained by their beauty\nLook at the difference in \\(\\widehat{\\text{eval}}\\) between someone super-hot (beauty = 2) and super-not (beauty = \\(-1.5\\)); the difference in their predicted student evaluations is \\(3.5 \\cdot 0.133 = 0.466\\)\nWe have to use our subjective judgment to decide whether these indicate a meaningful (practically significant) relationship"
  },
  {
    "objectID": "class_01.html#adding-more-variables-1",
    "href": "class_01.html#adding-more-variables-1",
    "title": "Data Science for Business Applications",
    "section": "Adding more variables",
    "text": "Adding more variables\n\n# The model\n# Revenue = intercept + slope*Budget + slope*Rating + e\nlm2 &lt;- lm(Adj_Revenue ~ Adj_Budget + imdbRating, data = movie_1990_data)\nsummary(lm2)\n\n\nCall:\nlm(formula = Adj_Revenue ~ Adj_Budget + imdbRating, data = movie_1990_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-256.79  -41.25  -14.97   26.55  598.53 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -136.50696   14.64962  -9.318   &lt;2e-16 ***\nAdj_Budget     1.09103    0.03585  30.433   &lt;2e-16 ***\nimdbRating    24.09935    2.17050  11.103   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.43 on 1365 degrees of freedom\nMultiple R-squared:  0.4428,    Adjusted R-squared:  0.442 \nF-statistic: 542.5 on 2 and 1365 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_01.html#visualizing-the-model",
    "href": "class_01.html#visualizing-the-model",
    "title": "Data Science for Business Applications",
    "section": "Visualizing the model",
    "text": "Visualizing the model"
  },
  {
    "objectID": "class_01.html#explanation-1",
    "href": "class_01.html#explanation-1",
    "title": "Data Science for Business Applications",
    "section": "Explanation",
    "text": "Explanation\n\n\nLet’s interpret the model:\n\n\n\\[\n\\texttt{Adj_Revenue} = -136.507 + 1.091 \\cdot\\texttt{Adj_Budget} + 24.099\\cdot\\texttt{imdbRating}\n\\]\n\nIntercept: By setting the movie budget and the rating to zero, we have that the average revenue is equal to $-137 million dollars.\nSlope Budget: For movies with the same fixed rating, for one unit change in the movie budget, there will be an increase of $1.091 million dollars in the movie’s revenue.\nSlope Rating: For movies with the same fixed budget, for one unit change in the movie rating, there will be an increase of $24.1 million dollars in the movie’s revenue."
  },
  {
    "objectID": "class_01.html#what-about-the-predictions",
    "href": "class_01.html#what-about-the-predictions",
    "title": "Data Science for Business Applications",
    "section": "What about the predictions?",
    "text": "What about the predictions?\n\nWe can observe from the summary that the residual standard error when down to \\(\\texttt{76.43}\\).\nWhich will result in more accurate predictions.\nSuppose we have a movie 25 million dollar budget and the movie has a IMDb rating of 5.4 what is the prediction for this movie?\nWhat about the 95% prediction interval for this prediction?\n\n\n# We use the confint() function to get the confidence interval\npredict(lm2, list(Adj_Budget = 25, imdbRating = 5.4))\n\n       1 \n20.90542 \n\n\n\nWhat about the 95% prediction interval for this prediction?\nupper bound: \\(20.90542 + 2\\times 76.43 = 173.7654\\)\nlower bound: \\(20.90542 - 2\\times 76.43 = -131.9546\\)"
  },
  {
    "objectID": "class_01.html#whats-next",
    "href": "class_01.html#whats-next",
    "title": "Data Science for Business Applications",
    "section": "What’s next?",
    "text": "What’s next?\n\nWe’ll include categorical variables.\nInteractions between categorical and numerical variables."
  },
  {
    "objectID": "class_02.html",
    "href": "class_02.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Recall what are the goals of the linear regression model"
  },
  {
    "objectID": "class_02.html#regresison-model-goals",
    "href": "class_02.html#regresison-model-goals",
    "title": "Data Science for Business Applications",
    "section": "Regresison model goals",
    "text": "Regresison model goals\n\n\nRecall what are the goals of the linear regression model"
  },
  {
    "objectID": "class_02.html#data-science-tasks",
    "href": "class_02.html#data-science-tasks",
    "title": "Data Science for Business Applications",
    "section": "Data Science tasks",
    "text": "Data Science tasks\n\nWe found that adding more predictors to linear models increases their accuracy and explanatory power.\nWhat if we want to add instead of Quantative predictors, Qualitative predictors?\nCategorical or Qualitative Variables split the data into different groups or levels.\nHow can we add these types of variables in the regression model?"
  },
  {
    "objectID": "class_02.html#lets-start-with-an-example",
    "href": "class_02.html#lets-start-with-an-example",
    "title": "Data Science for Business Applications",
    "section": "Let’s start with an example!",
    "text": "Let’s start with an example!\n\n\nExample: Cars dataset (cars_luxury.csv)\nData on 2,088 used cars in South California\nFor each car there are several predictors as:\nprice: Price of the car in dollars.\nmileage: Car mileage.\nluxury: If the car is a luxury car: “\\(\\texttt{yes}\\)”or “\\(\\texttt{no}\\)”\nbadge: Badge indicating if the car is considered some type of deal, that can be: “\\(\\texttt{Good Deal}\\)”, “\\(\\texttt{Great Deal}\\)”, “\\(\\texttt{No Badge}\\)” or “\\(\\texttt{Fair Price}\\)”.\n(and others)"
  },
  {
    "objectID": "class_02.html#luxury-and-price",
    "href": "class_02.html#luxury-and-price",
    "title": "Data Science for Business Applications",
    "section": "Luxury and price",
    "text": "Luxury and price\n\n\nBefore we start our analysis, let’s see if there’s a difference between the used price of luxury and not luxury cars.\n\n\n\n\nlibrary(tidyverse)\nggplot(cars_luxury, aes(x = luxury, y = price)) +\n  geom_boxplot()"
  },
  {
    "objectID": "class_02.html#regression-model",
    "href": "class_02.html#regression-model",
    "title": "Data Science for Business Applications",
    "section": "Regression model",
    "text": "Regression model\n\nIt’s interesting to incorporate the categorical variable luxury, into the multiple regression model.\nWe want to see the impact of mileage on price controlling for the type of car. If it’s a luxury or not.\nThe resulting model is equal to: \\[\n\\texttt{price} = \\beta_0 + \\beta_1\\texttt{mileage} + \\beta_2\\texttt{luxury} + e\n\\]\nHow can we assess the categorical variable luxury?"
  },
  {
    "objectID": "class_02.html#price-in-terms-of-mileage-and-luxury",
    "href": "class_02.html#price-in-terms-of-mileage-and-luxury",
    "title": "Data Science for Business Applications",
    "section": "Price in terms of mileage and luxury",
    "text": "Price in terms of mileage and luxury\n\n\nLet’s plot the relation between mileage, price and luxury\n\n\n\nggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n  geom_point()"
  },
  {
    "objectID": "class_02.html#dummy-variable",
    "href": "class_02.html#dummy-variable",
    "title": "Data Science for Business Applications",
    "section": "Dummy variable",
    "text": "Dummy variable\n\nluxury is a categorical variable (\\(\\texttt{\"yes\"}\\) or \\(\\texttt{\"no\"}\\) in this data set).\nThis variable contains two groups or two levels.\nRecode luxury into a quantitative variable where \\(\\texttt{1 = \"yes\"}\\), \\(\\texttt{0 = \"no\"}\\).\nThis quantitative variable is known as dummy variable.\nR does this for us.\nR will choose the alphabetically first category as the 0 level."
  },
  {
    "objectID": "class_02.html#regression-model-1",
    "href": "class_02.html#regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Regression model",
    "text": "Regression model\n\n# Remove scientific notation \noptions(scipen = 999)\n\n# Regression Model\nlm1 = lm(price ~ mileage + luxury, data = cars_luxury)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ mileage + luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24018  -6204  -1919   3727  78453 \n\nCoefficients:\n                Estimate   Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 25422.756210   508.681485   49.98 &lt;0.0000000000000002 ***\nmileage        -0.185784     0.008688  -21.39 &lt;0.0000000000000002 ***\nluxuryyes   12986.388662   569.304402   22.81 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11010 on 2085 degrees of freedom\nMultiple R-squared:  0.3439,    Adjusted R-squared:  0.3433 \nF-statistic: 546.4 on 2 and 2085 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nHow can we interpret these numbers?"
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model",
    "href": "class_02.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\nEstimated model:\n\n\n\\[\n\\texttt{price} = 25,423 - 0.19\\times \\texttt{mileage} + 12,986 \\times \\texttt{luxury}\n\\]\n\nHow can we interpret the coefficients?\nintercept: For a car with zero mileage and luxury = \\(\\texttt{\"no\"}\\) = 0, the average selling price is equal to US$ 25,423.\nmileage: For a fixed type of car, for each extra increase in mileage (in miles), there will be a decrease of US$ 0.19 in the price of the car.\nluxury: For cars with the same mileage, the added price of being a luxury car (luxury = \\(\\texttt{\"yes\"}\\) = 1) is US$ 12,986.\nImportant: When we add a categorical variable to the regression model, the intercept is also referred to as the baseline. The effect of the categorical variable is also known as the offset."
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-1",
    "href": "class_02.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nBy adding a categorical variable, we can also interpret this as different regression models depending on the number of groups.\nTo do so we add the effect of the categorical variable to the intercept.\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\texttt{price} &= 25,423 - 0.19\\times \\texttt{mileage} + 12,986 \\times (1) \\\\\n             &= (25,423 + 12,986) - 0.19\\times \\texttt{mileage} \\\\\n             &= 38,409 - 0.19\\times \\texttt{mileage} \\\\\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"no\"}\\) = 0 \\[\n\\begin{align}\n\\texttt{price} &= 25,423 - 0.19\\times \\texttt{mileage} + 12,986 \\times (0) \\\\\n             &= 25,423 - 0.19\\times \\texttt{mileage} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "class_02.html#section",
    "href": "class_02.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We can even visualize these two models in a plot\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_line(aes(y=predict(lm1)))"
  },
  {
    "objectID": "class_02.html#significance-and-predictions",
    "href": "class_02.html#significance-and-predictions",
    "title": "Data Science for Business Applications",
    "section": "Significance and Predictions",
    "text": "Significance and Predictions\n\nIs the price of a luxury cars statistically different of a non-luxury car?\n\n\nconfint(lm1)\n\n                    2.5 %        97.5 %\n(Intercept) 24425.1797220 26420.3326977\nmileage        -0.2028214    -0.1687463\nluxuryyes   11869.9244244 14102.8528996\n\n\n\nYes, with 95% confidence we can conclude that the price of a luxury car is different from a non luxury one.\nWhat is estimated price of luxury vehicle that has as mileage of 50000.\n\n\npredict(lm1, list(mileage = 50000, luxury = \"yes\"))\n\n       1 \n29119.95 \n\n\n\nThe estimated price of a 50,000-mile luxury car will be US$ 29,120."
  },
  {
    "objectID": "class_02.html#more-than-two-groups",
    "href": "class_02.html#more-than-two-groups",
    "title": "Data Science for Business Applications",
    "section": "More than two groups",
    "text": "More than two groups\n\nSuppose, instead of controlling the fact that a car is a luxury car or not, we want to observe the effect of badge on price.\nThe variable badge contains for groups or levels: “\\(\\texttt{Good Deal}\\)”, “\\(\\texttt{Great Deal}\\)”, “\\(\\texttt{No Badge}\\)” or “\\(\\texttt{Fair Price}\\)”.\nIs there a difference in the price of the car depending on what type of badge it holds?"
  },
  {
    "objectID": "class_02.html#section-1",
    "href": "class_02.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Let’s plot the relation between mileage, price and badge\n\n\nggplot(cars_luxury, aes(x = mileage, y = price, col = badge)) +\n  geom_point()"
  },
  {
    "objectID": "class_02.html#regressions-model",
    "href": "class_02.html#regressions-model",
    "title": "Data Science for Business Applications",
    "section": "Regressions model",
    "text": "Regressions model\n\n\nRun the model: \\(\\texttt{price} = \\beta_0 + \\beta_1\\times \\texttt{mileage} + \\beta_2 \\times \\texttt{badge} + e\\)\n\n\nlm2 = lm(price ~ mileage + badge, data = cars_luxury)\nsummary(lm2)\n\n\nCall:\nlm(formula = price ~ mileage + badge, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-19961  -6981  -2395   3629  82508 \n\nCoefficients:\n                    Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)     35931.481715  1140.032009  31.518 &lt; 0.0000000000000002 ***\nmileage            -0.209568     0.009527 -21.997 &lt; 0.0000000000000002 ***\nbadgeGood Deal  -3556.561624  1057.385699  -3.364             0.000783 ***\nbadgeGreat Deal -8988.415770  1062.334934  -8.461 &lt; 0.0000000000000002 ***\nbadgeNo Badge   -9930.896296  1143.713386  -8.683 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11860 on 2083 degrees of freedom\nMultiple R-squared:  0.2388,    Adjusted R-squared:  0.2374 \nF-statistic: 163.4 on 4 and 2083 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-2",
    "href": "class_02.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\nintercept (baseline): For a car with zero mileage and with a fair price badge, the average selling price is equal to US$ 35,932 (\\(\\texttt{Good Deal} = 0\\),\\(\\texttt{Great Deal} = 0\\), \\(\\texttt{No Badge} = 0\\)).\nmileage: For a car with a fixed badge, for each extra increase in mileage (in miles), there will be a decrease of US$ 0.21 in the price of the car.\n\\(\\texttt{Good Deal} = 1\\), remainig levels equal to zero: For cars with the same mileage, there will be a decrease in their price if they have a good deal badge of US$ 3,557 compared to the baseline, that is, cars with a fair price badge.\n\\(\\texttt{Great Deal}  = 1\\), remainig levels equal to zero: For cars with the same mileage, there will be a decrease in their price if they have a great deal badge of US$ 8,988 compared to the baseline, that is, cars with a fair price badge.\n\\(\\texttt{No Badge}  = 1\\), remainig levels equal to zero: For cars with the same mileage, there will be a decrease in their price if they have no badge of US$ 8,988 compared to the baseline, that is, cars with a fair price badge."
  },
  {
    "objectID": "class_02.html#section-2",
    "href": "class_02.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We have now 4 different models, one for each badge catgory.\nWe have four different intecepts."
  },
  {
    "objectID": "class_02.html#interactions",
    "href": "class_02.html#interactions",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nWe observed that there was a significant difference between the price of luxury and non-luxury cars.\nIs there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction."
  },
  {
    "objectID": "class_02.html#interactions-1",
    "href": "class_02.html#interactions-1",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\texttt{price} = \\beta_0 + \\beta_1\\texttt{mileage} + \\beta_2\\texttt{luxury} + \\beta_3 (\\texttt{luxury} \\times \\texttt{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\texttt{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\texttt{price} = \\beta_0 + \\beta_1\\texttt{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\texttt{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\texttt{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\texttt{mileage} + e\n\\]"
  },
  {
    "objectID": "class_02.html#regression-model-2",
    "href": "class_02.html#regression-model-2",
    "title": "Data Science for Business Applications",
    "section": "Regression Model",
    "text": "Regression Model\n\n\nLet’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-3",
    "href": "class_02.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nHow do we interpret this model?\nintercept (baseline), luxury = \\(\\texttt{\"no\"}\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\\texttt{\"no\"}\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\\texttt{\"yes\"}\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars."
  },
  {
    "objectID": "class_02.html#interpretation-of-the-model-4",
    "href": "class_02.html#interpretation-of-the-model-4",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nWe also have the following interpretation:\nluxury = \\(\\texttt{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\texttt{price} &= 23,894 - 0.15\\times \\texttt{mileage} + 19,772 (0) - 0.16\\times \\texttt{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\texttt{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\texttt{price} &= 23,894 - 0.15\\times \\texttt{mileage} + 19,772 (1) - 0.16\\times \\texttt{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\texttt{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\texttt{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope."
  },
  {
    "objectID": "class_02.html#section-3",
    "href": "class_02.html#section-3",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_02.html#significance-and-predictions-1",
    "href": "class_02.html#significance-and-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Significance and Predictions",
    "text": "Significance and Predictions\n\nDo luxury cars depreciate faster than non-luxury cars?\n\n\nconfint(lm3)\n\n                          2.5 %        97.5 %\n(Intercept)       22824.7212986 24962.4814687\nmileage              -0.1735141    -0.1358795\nluxuryyes         17629.8713295 21914.9959940\nmileage:luxuryyes    -0.1975365    -0.1133770\n\n\n\nYes, with 95% confidence we can conclude that the price of a luxury car depreciates faster than a non-luxury one.\nWhat is estimated price of luxury vehicle that has as mileage of 50000.\n\n\npredict(lm3, list(mileage = 50000, luxury = \"yes\"))\n\n       1 \n28158.36 \n\n\n\nThe estimated price of a 50,000-mile luxury car will be US$ 28,158.\nThere was also a decrese in the RSE of this model compared to the model without the interaction. From \\(\\texttt{11860}\\) to \\(\\texttt{10,880}\\)."
  },
  {
    "objectID": "class_02.html#conlusion",
    "href": "class_02.html#conlusion",
    "title": "Data Science for Business Applications",
    "section": "Conlusion",
    "text": "Conlusion\n\nInteractions make a model more complex to analyze and explain, so it’s only worth doing so when you get better interpretation and more accurate predictions.\nWe can have interactions between different kinds of variables. Between categorical variables, numerical and categorical and, numerical and numerical.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them"
  },
  {
    "objectID": "class_03.html",
    "href": "class_03.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Linear models are useful:\n\nPrediction - given a new observations\nExplanatory power - which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "class_03.html#regression-assumptions-and-potential-problems",
    "href": "class_03.html#regression-assumptions-and-potential-problems",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "class_03.html#regression-assumptions-and-potential-problems-1",
    "href": "class_03.html#regression-assumptions-and-potential-problems-1",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "class_03.html#multiple-regression-assumptions",
    "href": "class_03.html#multiple-regression-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression assumptions",
    "text": "Multiple regression assumptions\nWe need four things to be true for regression to work properly:\n\nLinearity: \\(Y\\) is a linear function of the \\(X\\)’s (except for the prediction errors).\nIndependence: The prediction errors are independent.\nNormality: The prediction errors are normally distributed.\nEqual Variance: The variance of \\(Y\\) is the same for any value of \\(X\\) (“homoscedasticity”)."
  },
  {
    "objectID": "class_03.html#non-linearity",
    "href": "class_03.html#non-linearity",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity",
    "text": "Non-Linearity\n\n\nWhat we would expect to observe in a regression where there is a linear relation?\n\n\n\nlibrary(tidyverse)\nggplot(linear_data, aes(x=X, y=Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "class_03.html#residuals",
    "href": "class_03.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nLet’s plot the residuals \\(r_i\\), such that \\[r_i = y_i − \\widehat{y}_i\\] where \\(\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\) vs \\(x_i\\)\nHopefully identify non-linear relationships\nWe are looking for patterns or trends in the residuals"
  },
  {
    "objectID": "class_03.html#residuals-1",
    "href": "class_03.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\n\nPlot of the residuals\nHow can these residuals be useful for us?"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plots",
    "href": "class_03.html#regression-diagnostic-plots",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plots",
    "text": "Regression diagnostic plots\nWe’ll use regression diagnostic plots to help us evaluate some of the assumptions.\nThe residuals vs fitted graph plots:\n\nResiduals on the \\(Y\\)-axis\nFitted values (predicted \\(Y\\) values) on the \\(X\\)-axis\n\nThis graph effectively subtracts out the linear trend between \\(Y\\) and the \\(X\\)’s, so we want to see no trend left in this graph."
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot",
    "href": "class_03.html#regression-diagnostic-plot",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check non-linearity we focus on the Residual vs. Fitted plot\n\n\n\nlibrary(ggfortify)\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-1",
    "href": "class_03.html#regression-diagnostic-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nFrom the Residual vs. Fitted plot, we can observe that since the residuals are evenly distributed around zero in relation to the fitted values, we have that the linear regression model is a good fit for this data.\nThis means that we are learning the linear representation contained in this data."
  },
  {
    "objectID": "class_03.html#non-linearity-example",
    "href": "class_03.html#non-linearity-example",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\n\nWhat we would expect to observe if the relation is non linear?\n\n\n\nggplot(nonlinear_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "class_03.html#non-linearity-example-1",
    "href": "class_03.html#non-linearity-example-1",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nLet’s look at the residuals for this model\n\n\n\nLet’s check the residual plot"
  },
  {
    "objectID": "class_03.html#non-linearity-example-2",
    "href": "class_03.html#non-linearity-example-2",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nlm2 = lm(Y ~ X, data = nonlinear_data)\nautoplot(lm2)"
  },
  {
    "objectID": "class_03.html#non-linearity-example-3",
    "href": "class_03.html#non-linearity-example-3",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nFrom the Residual vs. Fitted, we can observe that the residuals are not evenly distributed around zero.\nThis indicates that for lower and higher values of \\(x_i\\) our model is overpredicting and underpredicting in the mid values.\nWhat are the implications in this case?\nWorse predictions"
  },
  {
    "objectID": "class_03.html#independence",
    "href": "class_03.html#independence",
    "title": "Data Science for Business Applications",
    "section": "Independence",
    "text": "Independence\n\nIndependence means that knowing the prediction error for one observation doesn’t tell you anything about the error for another observation\nData collected over time are usually not independent\nWe can’t use regression diagnostics to decide the independence\nWe have to measure the autocorrelation of the residuals\nWe’ll get back to autocorrelation when we discuss Time Series models"
  },
  {
    "objectID": "class_03.html#normality-assumption",
    "href": "class_03.html#normality-assumption",
    "title": "Data Science for Business Applications",
    "section": "Normality assumption",
    "text": "Normality assumption\n\nWhen we’ve been interpreting residual standard error (RSE) , we’ve used the following interpretation:\n95% of our predictions will be accurate to within plus or minus \\(2\\times RSE\\).\nIn order for this to be true, the residuals have to be Normally distributed"
  },
  {
    "objectID": "class_03.html#normality-example",
    "href": "class_03.html#normality-example",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\n\nWe can check the distribution of the residuals\n\n\n\nlinear_data = linear_data %&gt;% \n  mutate(resid = residuals(lm1))\n\nggplot(linear_data, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 0.2)"
  },
  {
    "objectID": "class_03.html#normality-example-1",
    "href": "class_03.html#normality-example-1",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nBut how can we judge if the residuals follows a Normal distribution?\nThe key is to look at the Normal Q-Q plot, which compares the distribution of our residuals to a perfect Normal distribution.\nIf the dots line up along an (approximately) straight line, then the Normality assumption is satisfied."
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-2",
    "href": "class_03.html#regression-diagnostic-plot-2",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check for Normality we focus on the Normal Q-Q plot\n\n\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)\n\n\n\nIn this case the normality assumptions seem to be met"
  },
  {
    "objectID": "class_03.html#normality-example-2",
    "href": "class_03.html#normality-example-2",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nLet’s look at different data.\nIn this case the data has non Normal errors."
  },
  {
    "objectID": "class_03.html#normality-example-3",
    "href": "class_03.html#normality-example-3",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\n\nHistogram of the residuals (right skewed)\n\n\n\nlm3 = lm(Y ~ X, data = non_normal)\n\nnon_normal = non_normal %&gt;% \n  mutate(resid = residuals(lm3))\n\nggplot(non_normal, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 1)"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-3",
    "href": "class_03.html#regression-diagnostic-plot-3",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nautoplot(lm3)"
  },
  {
    "objectID": "class_03.html#interpretation-of-the-plot",
    "href": "class_03.html#interpretation-of-the-plot",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Normal Q-Q plot, we can observe that the residuals are not following the line that indicates the Normal quantiles\nThis means that our model results in non-normal residuals\nThis affects statistical tests, and confidence intervals"
  },
  {
    "objectID": "class_03.html#equal-variance",
    "href": "class_03.html#equal-variance",
    "title": "Data Science for Business Applications",
    "section": "Equal variance",
    "text": "Equal variance\n\nEqual variance is also known as “homoscedasticity”\nThe variance of \\(Y\\) should be about the same at any \\(X\\) value (or combination of values for the \\(X\\)’s).\nIn other words, the vertical spread of the points should be the same anywhere along the \\(X\\)-axis.\nIf there’s no equal variance then we might have heteroskedasticity.\nLower precision, estimates are further from the correct population value."
  },
  {
    "objectID": "class_03.html#equal-variance-example",
    "href": "class_03.html#equal-variance-example",
    "title": "Data Science for Business Applications",
    "section": "Equal variance example",
    "text": "Equal variance example\n\nThe vertical spread of the points is larger along the right side of the graph\n\n\nggplot(heter_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "class_03.html#regression-diagnostic-plot-4",
    "href": "class_03.html#regression-diagnostic-plot-4",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check for homoscedasticity we focus on the Scale-Location plot\n\n\n\nlm4 = lm(Y ~ X, data = heter_data)\nautoplot(lm4)"
  },
  {
    "objectID": "class_03.html#interpretation-of-the-plot-1",
    "href": "class_03.html#interpretation-of-the-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Sacle-Location plot, we can observe that the residuals have a fan shape, indicating that there is heteroscedacity in the data.\nThis resulted in lower precision; thus, estimates are further from the correct population value."
  },
  {
    "objectID": "class_03.html#influential-observations",
    "href": "class_03.html#influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Influential observations",
    "text": "Influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "class_03.html#diagnostics-plot",
    "href": "class_03.html#diagnostics-plot",
    "title": "Data Science for Business Applications",
    "section": "Diagnostics Plot",
    "text": "Diagnostics Plot\n\nWe can observe the point with high residual on the Residual vs. Leverage plot\n\n\nlm5 = lm(Y ~ X, data = outlier_residual)\nautoplot(lm5)"
  },
  {
    "objectID": "class_03.html#high-leverage",
    "href": "class_03.html#high-leverage",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can also have points with high leverage - when a point in \\(X\\) is distant from the average on \\(X\\)\n\n\n\nThis point has low residual but high leverage. RSE = 0.2956"
  },
  {
    "objectID": "class_03.html#high-leverage-1",
    "href": "class_03.html#high-leverage-1",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can observe the point with high leverage on the Residual vs. Leverage plot\n\n\nlm6 = lm(Y ~ X, data = outlier_leverage)\nautoplot(lm6)"
  },
  {
    "objectID": "class_03.html#points-with-high-influence",
    "href": "class_03.html#points-with-high-influence",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nPoints with high leverage and high residuals are known as influential points\n\n\n\nThis point has high residual but high leverage. RSE = 0.8281"
  },
  {
    "objectID": "class_03.html#points-with-high-influence-1",
    "href": "class_03.html#points-with-high-influence-1",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWe can observe the point with high influence on the Residual vs. Leverage plot\n\n\nlm7 = lm(Y ~ X, data = outlier_influence)\nautoplot(lm7)"
  },
  {
    "objectID": "class_03.html#points-with-high-influence-2",
    "href": "class_03.html#points-with-high-influence-2",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWhen a case has a very unusual \\(X\\) value, it has leverage — the potential to have a big impact on the regression line\nIf the case is in line with the overall trend of the regression line, it won’t be a problem\nBut when that case also has a \\(Y\\) (high residual) value that is out of line\nWe need both a large residual and high leverage for an observation to be influential\nWe should be worried about these points\nThey affect the coefficients and predictions"
  },
  {
    "objectID": "class_03.html#regression-assumptions-and-outliers",
    "href": "class_03.html#regression-assumptions-and-outliers",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Outliers",
    "text": "Regression Assumptions, and Outliers\nLinear models are useful:\n\nPrediction - given a new observations\nExplanatory power - which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "class_03.html#outliers-and-influential-observations",
    "href": "class_03.html#outliers-and-influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Outliers and influential observations",
    "text": "Outliers and influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "class_04.html#polynomial-models",
    "href": "class_04.html#polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nThe data set utilities contains information on the utility bills for a house in Minnesota. We’ll focus on two variables:\n\ndailyspend is the average amount of money spent on utilities (e.g. heating) for each day during the month\ntemp is the average temperature outside for that month"
  },
  {
    "objectID": "class_04.html#polynomial-models-1",
    "href": "class_04.html#polynomial-models-1",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWhat problems do you see here?\n\nlibrary(tidyverse)\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point()"
  },
  {
    "objectID": "class_04.html#polynomial-models-2",
    "href": "class_04.html#polynomial-models-2",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm1 &lt;- lm(dailyspend ~ temp, data=utilities) \nsummary(lm1)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59   &lt;2e-16 ***\ntemp        -0.096432   0.003911  -24.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 2.2e-16\n\n\n\nLet’s interpret this relation\nFor one unit increase in temperature (Fahrenheit), there will be a 10-cent decrease in spending"
  },
  {
    "objectID": "class_04.html#polynomial-models-3",
    "href": "class_04.html#polynomial-models-3",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlibrary(ggfortify)\nautoplot(lm1)\n\n\n\nLinearity and homoscedasticity are violated"
  },
  {
    "objectID": "class_04.html#polynomial-models-4",
    "href": "class_04.html#polynomial-models-4",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe’ll use polynomial regression to fix problems\nIf a polynomial curve (e.g., quadratic, cubic, etc) would be a better fit for the data than a line, we can fit a curve to the data.\nThe way we do this is by adding \\(X^2\\) to the model as a second predictor variable.\nThis can “fix” the linearity problem because now \\(Y\\) is a linear function of \\(X\\) and \\(X^2\\), resulting in: \\[\nY = \\beta_0 + \\beta_1\\cdot X + \\beta\\cdot X^2 + e\n\\]"
  },
  {
    "objectID": "class_04.html#polynomial-models-5",
    "href": "class_04.html#polynomial-models-5",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe add the term I(temp^2) in the regression equation:\n\n\nlm2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities) \nsummary(lm2)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239  &lt; 2e-16 ***\ntemp        -0.2115553  0.0191046 -11.074  &lt; 2e-16 ***\nI(temp^2)    0.0012476  0.0002037   6.124 1.33e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe have that the new term is evaluated as an extra variable."
  },
  {
    "objectID": "class_04.html#polynomial-models-6",
    "href": "class_04.html#polynomial-models-6",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWriting out the equation: \\[\n\\widehat{\\texttt{dailyspend}} = 9.4723 −0.2116\\cdot \\texttt{temp} + 0.0012\\cdot \\texttt{temp}^2\n\\] The effect of the extra variable is statistically significant:\n\nconfint(lm2)\n\n                    2.5 %       97.5 %\n(Intercept)  8.6981381712 10.246438869\ntemp        -0.2494014032 -0.173709160\nI(temp^2)    0.0008440041  0.001651114\n\n\n\nThe residual standard error of the polynomial model is \\(\\texttt{0.75}\\).\nThe residual standard error of the linear model is \\(\\texttt{0.87}\\)."
  },
  {
    "objectID": "class_04.html#polynomial-models-7",
    "href": "class_04.html#polynomial-models-7",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nAdding an \\(X^2\\) term fits a parabola to the data (orange line)\n\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm1)), col = \"lightblue\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")"
  },
  {
    "objectID": "class_04.html#polynomial-models-8",
    "href": "class_04.html#polynomial-models-8",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nIt solves the linearity problem\n\nautoplot(lm2)"
  },
  {
    "objectID": "class_04.html#polynomial-models-9",
    "href": "class_04.html#polynomial-models-9",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWhat about a higher-order polynomial?\nWe could fit a cubic curve by adding an \\(X^3\\) term\nMaking the polynomial higher order will decrease the RSE\nWhy not go nuts and fit a 7th degree polynomial?\n\n\n\n\nDegree\nname\nRSE\n\n\n\n\n1\nlinear\n0.866\n\n\n2\nquadratic\n0.754\n\n\n3\ncubic\n0.755\n\n\n4\nquartic\n0.755\n\n\n5\nquintic\n0.758\n\n\n6\n\n0.761\n\n\n7\n\n0.761"
  },
  {
    "objectID": "class_04.html#polynomial-models-10",
    "href": "class_04.html#polynomial-models-10",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ poly(temp,7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "class_04.html#building-polynomial-models",
    "href": "class_04.html#building-polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Building polynomial models",
    "text": "Building polynomial models\nStart simple: only add higher-degree terms to the extent it gives you a substantial decrease in the RSE, or satisfies an assumption hold that wasn’t satisfied before\n\nYou must include lower-order terms: e.g., if you add \\(X^3\\), you must also include \\(X\\) and \\(X^2\\)\nBe careful about overfitting when adding higher-order terms!\nBe particularly careful about extrapolating beyond the range of the data!\nMind-bender: We can think about an \\(X^2\\) term as an interaction of \\(X\\) with itself: in a parabola, the slope depends on the value of \\(X\\)"
  },
  {
    "objectID": "class_04.html#the-log-transformation",
    "href": "class_04.html#the-log-transformation",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nWe saw that we can use transformations to fix problems\nSometimes, a violation of regression assumptions can be fixed by transforming one or the other of the variables (or both).\nWhen we transform a variable, we have to also transform our interpretation of the equation."
  },
  {
    "objectID": "class_04.html#the-log-transformation-1",
    "href": "class_04.html#the-log-transformation-1",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\nThe log transformation is frequently useful in regression, because many nonlinear relationships are naturally exponential.\n\n\\(\\log_b x=y\\) when \\(b^y=x\\)\nFor example, \\(\\log_{10} 1000 = 3\\), \\(\\log_{10}100 = 2\\), and \\(\\log_{10}10 = 1\\)"
  },
  {
    "objectID": "class_04.html#the-log-transformation-2",
    "href": "class_04.html#the-log-transformation-2",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!)\nSkewed data is also a good candidate for log"
  },
  {
    "objectID": "class_04.html#moores-law",
    "href": "class_04.html#moores-law",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nMoore’s Law was a prediction made by Gordon Moore in 1965 (!) that the number of transistors on computer chips would double every 2 years\nThis implies exponential growth, so a linear model won’t fit well (and neither will any polynomial)\n\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_04.html#moores-law-1",
    "href": "class_04.html#moores-law-1",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nlm_moore = lm(Transistor.count ~ Date.of.introduction, data = moores)\nautoplot(lm_moore)\n\n\n\nA linear model is a spectacular fail"
  },
  {
    "objectID": "class_04.html#modeling-exponential-growth",
    "href": "class_04.html#modeling-exponential-growth",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\nIf \\(Y = ae^{bX}\\), then\n\\[\\log(Y) = \\log(a)+ bX\\]\n\nIn other words, \\(\\log(Y)\\) is a linear function of \\(X\\) when \\(Y\\) is an exponential function of \\(X\\)\nSo if we think \\(Y\\) is an exponential function of \\(X\\), predict \\(\\log(Y)\\) as a linear function of \\(X\\)"
  },
  {
    "objectID": "class_04.html#modeling-exponential-growth-1",
    "href": "class_04.html#modeling-exponential-growth-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nTransistors does NOT have a linear relationship with year\n\\(\\log(\\texttt{Transistors})\\) does have a linear relationship with year\n\n\nggplot(moores, aes(x = Date.of.introduction, y = log(Transistor.count))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "class_04.html#log-linear-model",
    "href": "class_04.html#log-linear-model",
    "title": "Data Science for Business Applications",
    "section": "Log-linear Model",
    "text": "Log-linear Model\nLet’s run the regression model\n\noptions(scipen = 999)\nlm_moore = lm(log(Transistor.count) ~ Date.of.introduction, data = moores)\nsummary(lm_moore)\n\n\nCall:\nlm(formula = log(Transistor.count) ~ Date.of.introduction, data = moores)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1299 -0.3338  0.1767  0.5230  2.0626 \n\nCoefficients:\n                        Estimate  Std. Error t value            Pr(&gt;|t|)    \n(Intercept)          -681.212056   15.958165  -42.69 &lt;0.0000000000000002 ***\nDate.of.introduction    0.349154    0.007981   43.75 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.054 on 99 degrees of freedom\nMultiple R-squared:  0.9508,    Adjusted R-squared:  0.9503 \nF-statistic:  1914 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "class_04.html#modeling-exponential-growth-2",
    "href": "class_04.html#modeling-exponential-growth-2",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nautoplot(lm_moore)"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model",
    "href": "class_04.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\nOur model is \\[\\widehat{\\log(\\texttt{Transistors})} = −681.21 + 0.35 \\cdot \\texttt{Year}\\]\nTwo interpretations of the slope coefficient:\n\nEvery year, the predicted log of transistors goes up by 0.35\nMore useful: Every year, the predicted number of transistors goes up by 35%\nA constant percentage increase every year is exponential growth!"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model-1",
    "href": "class_04.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nMaking predictions using the log-linear model\nWhen making predictions, we have to remember that our equation gives us predictions for \\(\\log(\\texttt{Transistors})\\), not Transistors!\n\nExample: To make a prediction for the number of transistors in 2022: \\[\n\\log(\\texttt{Transistors}) = −681.21 + 0.35(2022) = 26.49\n\\] But our prediction is not 26.49:\n\\(e^{\\log(\\texttt{Transistors})} = e^{26.49} = 319,492,616,196\\)"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model-2",
    "href": "class_04.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_line(aes(x = Date.of.introduction, y = exp(predict(lm_moore))), col = \"orange\")"
  },
  {
    "objectID": "class_04.html#interpretation-of-the-model-3",
    "href": "class_04.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\n\n\n\n\nModel\nEquation\nInterpretation\n\n\n\n\nLinear\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies \\(\\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-linear\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies ≈ \\(100 \\cdot \\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)\n\n\nLinear-log\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(0.01 \\cdot \\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-log\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(\\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)"
  },
  {
    "objectID": "class_04.html#conclusion",
    "href": "class_04.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhen is the log transformation useful?\nYou can transform \\(X \\rightarrow \\log(X)\\), \\(Y \\rightarrow \\log(Y)\\), or both\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!), try transforming it with a log\nIn this case, Transistors is skewed right so it is a good candidate for log\nYou may need to do a little bit of trial and error to see what works best\nOther transformations are possible!"
  },
  {
    "objectID": "class_05.html#basic-time-series-concepts",
    "href": "class_05.html#basic-time-series-concepts",
    "title": "Data Science for Business Applications",
    "section": "Basic time series concepts",
    "text": "Basic time series concepts\n\nApple quarterly revenue (Billions of dollars)\nGoal: What is the pattern here, and how can we forecast future earnings?\n\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "class_05.html#what-are-time-series",
    "href": "class_05.html#what-are-time-series",
    "title": "Data Science for Business Applications",
    "section": "What are time series?",
    "text": "What are time series?\n\nData where the cases represent time: data collected every day, month, year, etc.\nTime series are important for both explaining how variables change over time and forecasting the future\nExamples of time series data:\nGoogle’s closing daily stock price every day in 2020\nInventory levels of each item at a retail store at the end of every week in 2020\nNumber of new COVID cases in the US each day since the start of the pandemic\nApple’s quarterly revenue since 2009"
  },
  {
    "objectID": "class_05.html#anatomy-of-a-time-series",
    "href": "class_05.html#anatomy-of-a-time-series",
    "title": "Data Science for Business Applications",
    "section": "Anatomy of a time series",
    "text": "Anatomy of a time series\nSome notation:\n\n\\(t = 1,2,3,...\\), time index\n\\(Y_t\\), is the value: of the variable of interest at time \\(t\\)\n\\(Y_t\\) may be composed of one or more components:\nTrend\nSeasonal\nCyclical\nRandom"
  },
  {
    "objectID": "class_05.html#trend-component",
    "href": "class_05.html#trend-component",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nA trend is persistent upwards or downwards movement in the data (not necessarily linear)."
  },
  {
    "objectID": "class_05.html#trend-component-1",
    "href": "class_05.html#trend-component-1",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nExample: Moore’s Law (accelerating increase of transistor count)\nExample: US population over time\nA time series with no trend is called stationary."
  },
  {
    "objectID": "class_05.html#seasonal-component",
    "href": "class_05.html#seasonal-component",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nSeasonal fluctuation occurs when predictable up or down movements occur over a regular interval."
  },
  {
    "objectID": "class_05.html#seasonal-component-1",
    "href": "class_05.html#seasonal-component-1",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nThe ups and downs must occur over a regular interval (e.g., every month, or every year)\nExample: Highway traffic volume is highest during rush hour every day\nExample: Supermarket sales may be highest every month right after common paydays like the 15th and 30th"
  },
  {
    "objectID": "class_05.html#cyclic-component",
    "href": "class_05.html#cyclic-component",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nCyclic fluctuations occur at unpredictable intervals, e.g. due to changing business or economic conditions."
  },
  {
    "objectID": "class_05.html#cyclic-component-1",
    "href": "class_05.html#cyclic-component-1",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nIn contrast to seasonal fluctuations, cyclic fluctuations do not occur at regular, predictable intervals\nIt may be possible to predict cyclic components based on some other (non-time) variable\nExample: Restaurant sales dropped dramatically in 2020 due to COVID, as people ate out less\nExample: Sales of bell bottoms rose in the 60s and 70s, declined by the 80s, and then had a resurgence in the 90s"
  },
  {
    "objectID": "class_05.html#remaindererror-component",
    "href": "class_05.html#remaindererror-component",
    "title": "Data Science for Business Applications",
    "section": "Remainder/Error component",
    "text": "Remainder/Error component\n\nAny real time series will always have random noise as well, which can’t be predicted or forecast."
  },
  {
    "objectID": "class_05.html#time-series-components",
    "href": "class_05.html#time-series-components",
    "title": "Data Science for Business Applications",
    "section": "Time Series Components",
    "text": "Time Series Components\n\nWhich component(s) you see in each of these time series?"
  },
  {
    "objectID": "class_05.html#putting-these-together",
    "href": "class_05.html#putting-these-together",
    "title": "Data Science for Business Applications",
    "section": "Putting these together",
    "text": "Putting these together\nReal time series will usually include a combination of these four components. We will model the time series \\(Y_t\\) either additively:\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\] Or multiplicatively: \\[\nY_t = \\text{Trend}\\cdot\\text{Seasonal}\\cdot\\text{Random}= T_t \\cdot S_t \\cdot E_t\n\\] * (\\(E_t\\) consists of both the cyclic and error components, as both are unpredictable.) This model can be rewritten as a log model: \\[\n\\log{Y_t} = \\log(T_t) + \\log(S_t) + \\log(E_t)\n\\]"
  },
  {
    "objectID": "class_05.html#additive-models",
    "href": "class_05.html#additive-models",
    "title": "Data Science for Business Applications",
    "section": "Additive models",
    "text": "Additive models\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\]\n\nMost appropriate when seasonal fluctuations are consistent (do not increase or decrease over time)\nThe trend component \\(T_t\\) is a function of t (e.g., linear or quadratic)\nThe seasonal component \\(S_t\\) is a set of dummy variable representing “seasons”\nSo we can estimate additive models using regular regression"
  },
  {
    "objectID": "class_05.html#additive-decomposition",
    "href": "class_05.html#additive-decomposition",
    "title": "Data Science for Business Applications",
    "section": "Additive decomposition",
    "text": "Additive decomposition\n\nRun a regression predicting \\(Y\\) as a function of:\n\n\n\\(t\\), \\(t^2\\), \\(\\log(t)\\) etc (the trend component \\(T_t\\))\nDummy variables for the seasons (the seasonal component \\(S_t\\))\n\n\nTo make a prediction for \\(Y\\), plug into the model!\nThe residuals of this model correspond to the error component \\(E_t\\)"
  },
  {
    "objectID": "class_05.html#apple-quarterly-revenue",
    "href": "class_05.html#apple-quarterly-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple quarterly revenue",
    "text": "Apple quarterly revenue\n\nWhat components do you see here?\n\n\nlibrary(tidyverse)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "class_05.html#fitting-additive-model",
    "href": "class_05.html#fitting-additive-model",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nlm_additive = lm(Revenue ~ Period + Quarter, data=apple) \nsummary(lm_additive)\n\n\nCall:\nlm(formula = Revenue ~ Period + Quarter, data = apple)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -5.135   1.280   4.923  17.928 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  33.93619    2.74731  12.353  &lt; 2e-16 ***\nPeriod        1.41324    0.05917  23.884  &lt; 2e-16 ***\nQuarterQ2   -20.62657    2.89298  -7.130 2.31e-09 ***\nQuarterQ3   -27.44818    2.89480  -9.482 3.62e-13 ***\nQuarterQ4   -24.20276    2.89298  -8.366 2.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.921 on 55 degrees of freedom\nMultiple R-squared:  0.9269,    Adjusted R-squared:  0.9216 \nF-statistic: 174.4 on 4 and 55 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_05.html#interpretation-of-the-model",
    "href": "class_05.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nThe trend that we can infer from the variable Period indicates a positive growth in revenue of US$ 1.4 billion for each increase in the periods.\nThe seasonal from the Quarter component indicates:\n\n\nQ2’s are expected to be $20.7 worse than Q1’s\nQ3’s are expected to be $27.4 worse than Q1’s\nQ4’s are expected to be $24.2 worse than Q1’s\nQ3’s are significantly worse than Q1’s\n\n\nThese effects are statistically significant (confint(lm_additive))\nThe RSE from this model is US$ 7.921 billions of dollars.\nHow can we interpret these results?"
  },
  {
    "objectID": "class_05.html#fitting-additive-model-1",
    "href": "class_05.html#fitting-additive-model-1",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line() +\n  geom_line(aes(x = Time, y = predict(lm_additive)), col = \"orange\")"
  },
  {
    "objectID": "class_05.html#fitting-additive-model-2",
    "href": "class_05.html#fitting-additive-model-2",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nWhat does the final model predict from the Quarter component indicates: for Apple in 2024 Q3?\n\n\npredict(lm_additive, list(Period = 61, Quarter = \"Q3\"), interval = \"prediction\")\n\n       fit      lwr     upr\n1 92.69571 75.86745 109.524\n\n\n\nThe actual revenue was US$ 85.78 billions\nWhat does the final model predict from the Quarter component indicates: for Apple in 2030 Q1? (Should we trust that prediction?)"
  },
  {
    "objectID": "class_05.html#fitting-additive-model-3",
    "href": "class_05.html#fitting-additive-model-3",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nThe residuals from this model show the “detrended and deasonalized” data (but there’s still some trend left!):\nWe hadn’t yet dealt with the time dependence\n\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line(aes(x = Time, y = residuals(lm_additive)))"
  },
  {
    "objectID": "class_05.html#autorgression-model",
    "href": "class_05.html#autorgression-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression model",
    "text": "Autorgression model\n\nHow we deal with the time dependence ? Key idea: Instead of predicting \\(Y_t\\) as a function of \\(t\\) (or other variables), predict \\(Y_t\\) as a function of \\(Y_{t-1}\\): \\[\nY_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\n\\]\n\\(Y_{t-1}\\) is called the “1st lag” of \\(Y\\)\nThis is called autoregressive (AR) because it predicts the values of a time series based on previous values\nThe model above is an AR(1) model\nWe can have AR(\\(p\\)) models, with lag \\(p\\)"
  },
  {
    "objectID": "class_05.html#autocorrelation",
    "href": "class_05.html#autocorrelation",
    "title": "Data Science for Business Applications",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation, is the correlation of \\(Y_t\\) with each of its lags \\(Y_t, Y_{t−1},\\dots\\) \\[\nCor(Y_t, Y_{t−1}), Cor(Y_t, Y_{t−2}),\\dots\n\\]\nWe also have the autocorrelation of the residuals, \\(r_t\\)’s, which indicates that there’s a strong indication that the independence assumption is violated \\[\nCor(r_t, r_{t−1}), Cor(r_t, r_{t−2}),\\dots\n\\]"
  },
  {
    "objectID": "class_05.html#ozone-example",
    "href": "class_05.html#ozone-example",
    "title": "Data Science for Business Applications",
    "section": "Ozone example",
    "text": "Ozone example\n\nCreating an AR(1) model: Daily ozone levels in Houston\n\n\nggplot(ozone, aes(x = day, y = ozone)) + \n  geom_line()"
  },
  {
    "objectID": "class_05.html#acf-plot",
    "href": "class_05.html#acf-plot",
    "title": "Data Science for Business Applications",
    "section": "ACF plot",
    "text": "ACF plot\n\nVisualizing the autocorrelation function (ACF)\n\n\nacf(ozone$ozone)\n\n\n\nAutocorrelations outside of the dashed blue lines are statistically significant."
  },
  {
    "objectID": "class_05.html#autorgression-of-the-model",
    "href": "class_05.html#autorgression-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression of the model",
    "text": "Autorgression of the model\n\nWe use the lag function to create the lagged observations\n\n\nozone &lt;- ozone %&gt;% \n  mutate(lag1=lag(ozone)) \nozone.model = lm(ozone ~ lag1, data=ozone) \nsummary(ozone.model)\n\n\nCall:\nlm(formula = ozone ~ lag1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.192  -3.464  -1.108   2.679  16.679 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.87446    1.06976   6.426 2.76e-09 ***\nlag1         0.40419    0.08381   4.823 4.20e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.999 on 120 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1624,    Adjusted R-squared:  0.1554 \nF-statistic: 23.26 on 1 and 120 DF,  p-value: 4.197e-06"
  },
  {
    "objectID": "class_05.html#assumptions-of-an-ar1-model",
    "href": "class_05.html#assumptions-of-an-ar1-model",
    "title": "Data Science for Business Applications",
    "section": "Assumptions of an AR(1) model",
    "text": "Assumptions of an AR(1) model\n\nLinearity, Normality, Equal Variance: Check using residual plot (linearity + homoscedasticity), Q-Q plot (normality), scale/location (homoscedasticity) like any other regression model\nIndependence: Since this is a time series, we can actually check this by looking at the autocorrelation of the residuals (we want no significant autocorrelation)"
  },
  {
    "objectID": "class_05.html#autoplot",
    "href": "class_05.html#autoplot",
    "title": "Data Science for Business Applications",
    "section": "Autoplot",
    "text": "Autoplot\n\nLinearity, Normality, Equal Variance\n\n\nautoplot(ozone.model)"
  },
  {
    "objectID": "class_05.html#acf-of-the-residuals",
    "href": "class_05.html#acf-of-the-residuals",
    "title": "Data Science for Business Applications",
    "section": "ACF of the residuals",
    "text": "ACF of the residuals\n\nacf(ozone.model$residuals)\n\n\n\nWe expect 5% of autocorrelations to be significant just by chance, so having just 1 out of the 20 lags flagged as significant indicates we are OK on independence!"
  },
  {
    "objectID": "class_05.html#making-predictions-in-time-series",
    "href": "class_05.html#making-predictions-in-time-series",
    "title": "Data Science for Business Applications",
    "section": "Making predictions in time series",
    "text": "Making predictions in time series\n\n\n\n\n\n\n\n\nType\nModel\nPredicted \\(Y_t\\)\n\n\n\n\nWhite noise\n\\(Y_t = e_t\\)\n\\(0\\)\n\n\nRandom sample\n\\(Y_t = \\beta_0 + e_t\\)\n\\(\\widehat{\\beta}_0\\) (or average \\(Y\\))\n\n\nRandom walk\n\\(Y_t = \\beta_0 + Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + Y_{t-1}\\)\n\n\nGeneral AR(1)\n\\(Y_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 Y_{t-1}\\)\n\n\n\n\nUnit root occurs when \\(\\beta_1 = 1\\). This means:\nThe series is a random walk.\nThere’s no mean reversion, and any shocks will have a permanent effect.\nWhen \\(\\beta_1 = 1\\), the model is non-stationary, meaning the series tends to “drift” without stabilizing around a fixed mean.\nIf \\(|\\beta_1| &lt; 1\\), the series is mean-reverting, and shocks are temporary."
  },
  {
    "objectID": "class_05.html#statistical-analysis",
    "href": "class_05.html#statistical-analysis",
    "title": "Data Science for Business Applications",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nconfint(ozone.model)\n\n                2.5 %    97.5 %\n(Intercept) 4.7564110 8.9925161\nlag1        0.2382561 0.5701286\n\n\n\nThe coefficient \\(\\widehat{\\beta}_1\\) is associated with the variable lag1.\nIn this case, for the larger population, with 95% confidence, \\(\\widehat{\\beta}_1\\) lies between 0.24 and 0.57.\nThis means that \\(|\\beta_1| &lt; 1\\), indicating that the series is mean-reverting."
  },
  {
    "objectID": "class_05.html#apple-revenue-acf-plot",
    "href": "class_05.html#apple-revenue-acf-plot",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the additive model."
  },
  {
    "objectID": "class_05.html#apple-revenue",
    "href": "class_05.html#apple-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nCombining decomposition and autoregression in a multiplicative model\n\n\\[\n\\log(\\texttt{Revenue}_t) = \\log(\\texttt{Period}_t) + \\texttt{Quarter}_t + \\log(\\texttt{Revenue}_{t-1})\n\\]\n\nWe need to create the lag variable.\nIt will have only one lag, and thus is an AR(1) model.\n\n\napple = apple %&gt;% \n  mutate(lag1 = lag(Revenue))"
  },
  {
    "objectID": "class_05.html#apple-revenue-1",
    "href": "class_05.html#apple-revenue-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nlog_apple = lm(log(Revenue) ~ log(Period) + Quarter + log(lag1), data = apple)\nsummary(log_apple)\n\n\nCall:\nlm(formula = log(Revenue) ~ log(Period) + Quarter + log(lag1), \n    data = apple)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.204851 -0.056602  0.005991  0.066084  0.193337 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.14400    0.17945   6.375 4.56e-08 ***\nlog(Period)  0.20622    0.06918   2.981  0.00433 ** \nQuarterQ2   -0.53559    0.04911 -10.906 3.72e-15 ***\nQuarterQ3   -0.47076    0.03397 -13.859  &lt; 2e-16 ***\nQuarterQ4   -0.31872    0.03346  -9.526 4.47e-13 ***\nlog(lag1)    0.63410    0.10109   6.273 6.65e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09013 on 53 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9751,    Adjusted R-squared:  0.9728 \nF-statistic: 415.4 on 5 and 53 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "class_05.html#apple-revenue-predictions",
    "href": "class_05.html#apple-revenue-predictions",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nPredictions of multiplicative model"
  },
  {
    "objectID": "class_05.html#apple-revenue-predictions-1",
    "href": "class_05.html#apple-revenue-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nConfidence interval of the multiplicative model\n\n\nconfint(log_apple)\n\n                  2.5 %     97.5 %\n(Intercept)  0.78406737  1.5039420\nlog(Period)  0.06746219  0.3449861\nQuarterQ2   -0.63409896 -0.4370871\nQuarterQ3   -0.53888914 -0.4026276\nQuarterQ4   -0.38583509 -0.2516142\nlog(lag1)    0.43133359  0.8368601\n\n\n\nThe slope associated with lag is statistically significant, and its value is between minus and plus one; we have that this is a mean-reverting time series.\nWe also have a better fit (here we feed lag1 with prediction from the previous period, US$ 90.75 billions):\n\n\n exp(predict(log_apple, list(Period = 61, Quarter = \"Q3\", lag1 = 90.75), interval = \"prediction\"))\n\n       fit      lwr      upr\n1 79.80492 66.06926 96.39618\n\n\n\nThe confidence interval for the forecast is narrower, and the difference between what we observe and predict is smaller."
  },
  {
    "objectID": "class_05.html#apple-revenue-acf-plot-1",
    "href": "class_05.html#apple-revenue-acf-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the multiplicative model.\n\n\n\nThe independent assumptions look better, but it might be necessary to add more lags."
  },
  {
    "objectID": "class_05.html#time-series-strategy",
    "href": "class_05.html#time-series-strategy",
    "title": "Data Science for Business Applications",
    "section": "Time Series Strategy",
    "text": "Time Series Strategy\nTo building a time series model:\n\nStart with a an additive or multiplicative model with trend and seasonal components. (Plot your data! If the seasonal variation increases or decreases over time you’ll want a multiplicative model.)\nExamine the usual diagnostic plots, and plot your residuals as a function of time. Do you need a (different) nonlinear time trend? A transformation of \\(Y\\)?\nCheck your residuals for autocorrelation. If it’s present, add appropriate lag terms to your model."
  },
  {
    "objectID": "class_06.html#introduction-to-prediction",
    "href": "class_06.html#introduction-to-prediction",
    "title": "Data Science for Business Applications",
    "section": "Introduction to prediction",
    "text": "Introduction to prediction\n\nSo far, we have been focusing mostly on trying to explain the effects from the predictors \\(X\\) through the coefficients.\nUntil now, our focus was on the soundness of our model in relation to statistical significance and how well our model was fitting the data (Regression Assumptions).\nToday, we will focus on making models that return Estimate/predict outcomes with high accuracy without extrapolating, with previously unseen data."
  },
  {
    "objectID": "class_06.html#inference-and-prediction",
    "href": "class_06.html#inference-and-prediction",
    "title": "Data Science for Business Applications",
    "section": "Inference and Prediction",
    "text": "Inference and Prediction\n\nInference \\(\\rightarrow\\) focus on the predictor\nInterpretability of model\nPrediction \\(\\rightarrow\\) focus on outcome variable\nAccuracy of model"
  },
  {
    "objectID": "class_06.html#bias-vs.-variance",
    "href": "class_06.html#bias-vs.-variance",
    "title": "Data Science for Business Applications",
    "section": "Bias vs. Variance",
    "text": "Bias vs. Variance\n\nBias vs Variance trade-off\nVariance: The amount by which the function \\(f\\) would change if we estimated it using a different training dataset\nBias: Error introduced by approximating a real-life problem with a model\nMore flexible models have a higher variance and a lower bias\nLess flexible models have a lower variance but a higher bias\nValidation set approach: Training and testing data\nBalance between flexibility and accuracy"
  },
  {
    "objectID": "class_06.html#bias-vs.-variance-1",
    "href": "class_06.html#bias-vs.-variance-1",
    "title": "Data Science for Business Applications",
    "section": "Bias vs. Variance",
    "text": "Bias vs. Variance\n\nWhen explaining, bias is usually greater than variance\nIn prediction, we care about both\nMeasures of accuracy will have both bias and variance"
  },
  {
    "objectID": "class_06.html#measures-of-accuracy",
    "href": "class_06.html#measures-of-accuracy",
    "title": "Data Science for Business Applications",
    "section": "Measures of accuracy",
    "text": "Measures of accuracy\n\nHow do we measure accuracy?\nMean Squared Error (MSE): Can be decomposed into variance and bias terms \\[\n\\text{MSE} = \\text{Var} + \\text{Bias}^2 + \\text{Irreducible Error}\n\\] where MSE is equal to \\[\nMSE = \\frac{1}{n} \\sum_{i = 1}^n(y_i-\\widehat{y}_i)^2\n\\]\nRoot Mean Squared Error (RMSE): Measured in the same units as the outcome \\[\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n\\]\nOther measures: Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC)"
  },
  {
    "objectID": "class_06.html#is-flexibility-always-better",
    "href": "class_06.html#is-flexibility-always-better",
    "title": "Data Science for Business Applications",
    "section": "Is flexibility always better?",
    "text": "Is flexibility always better?"
  },
  {
    "objectID": "class_06.html#measures-of-accuracy-1",
    "href": "class_06.html#measures-of-accuracy-1",
    "title": "Data Science for Business Applications",
    "section": "Measures of accuracy",
    "text": "Measures of accuracy\n\nModels with increasing flexibility (linear, cubic, spline).\nThink of a spline as a polynomial model with a high degree.\nRMSE decreases with flexibility in the training data.\nThe spline overfits the training data since the RMSE of the testing data is large."
  },
  {
    "objectID": "class_06.html#what-is-churn",
    "href": "class_06.html#what-is-churn",
    "title": "Data Science for Business Applications",
    "section": "What is churn?",
    "text": "What is churn?\n\n\n\nChurn: Measure of how many customers stop using your product (e.g. cancel a subscription).\nLess costly to keep a customer than bring a new one\nGoal: Prevent churn\nIdentify customer that are likely to cancel/quit/fail to renew"
  },
  {
    "objectID": "class_06.html#predicting-pre-churn",
    "href": "class_06.html#predicting-pre-churn",
    "title": "Data Science for Business Applications",
    "section": "Predicting “pre-churn”",
    "text": "Predicting “pre-churn”\n\nWe will predict “pre-churn”.\nAt a good measure for someone at risk of unsubscribing (“pre-churn”) is the times they’ve logged in the past week.\nWe are interested in the number of log ins in the variable logins.\nWe will predict logins from the other variable in the data.\nWe two candidates: Simple vs Complex"
  },
  {
    "objectID": "class_06.html#predicting-pre-churn-1",
    "href": "class_06.html#predicting-pre-churn-1",
    "title": "Data Science for Business Applications",
    "section": "Predicting “pre-churn”",
    "text": "Predicting “pre-churn”\n\nSimple Model: \\[\nlogins = \\beta_0 + \\beta_1 \\cdot Succession + \\beta_2 \\cdot city + \\epsilon\n\\]\nComplex Model: \\[\nlogins = \\beta_0 + \\beta_1 \\cdot Succession + \\beta_2 \\cdot age + \\beta_3 \\cdot age^2 + \\beta_4 \\cdot city + \\beta_5 \\cdot female + \\epsilon\n\\]\nCan we build more complex methods? Yes!\nFirst we will just analyse these two."
  },
  {
    "objectID": "class_06.html#create-validation-sets",
    "href": "class_06.html#create-validation-sets",
    "title": "Data Science for Business Applications",
    "section": "Create Validation Sets",
    "text": "Create Validation Sets\n\nCreate Training and Testing sets\nWe will use 75% of the data to train the data\nThe remaining part of the data, 25%, we reserve for testing\nThis split is done randomly\nTo do so we use the libraries modelr, and rsample\n\n\nlibrary(modelr) # for common model performance metrics\nlibrary(rsample)  # for creating train/test splits\n\nset.seed(100) #Always set seed for replication\nhbo_split =  initial_split(hbomax, prop=0.75)\nhbo_train = training(hbo_split)\nhbo_test  = testing(hbo_split)"
  },
  {
    "objectID": "class_06.html#rmse-in-training-and-testing-data",
    "href": "class_06.html#rmse-in-training-and-testing-data",
    "title": "Data Science for Business Applications",
    "section": "RMSE in training and testing data",
    "text": "RMSE in training and testing data\n\n# Simple Model\nlm_simple = lm(logins ~ succession + city, data = hbo_train)\n\n# Complex Model\nlm_complex = lm(logins ~ female + city + age + I(age^2) + succession, data = hbo_train)\n\n# Testing error for the simple model:\nrmse(lm_simple, hbo_test)\n\n[1] 2.075106\n\n# Testing error for the complex model:\nrmse(lm_complex, hbo_test)\n\n[1] 2.080211\n\n\n\nWhich model we should choose?\nThe model with the smallest out of sample error\nOut of sample means evaluation in the testing data"
  },
  {
    "objectID": "class_06.html#cross-validation",
    "href": "class_06.html#cross-validation",
    "title": "Data Science for Business Applications",
    "section": "Cross-Validation",
    "text": "Cross-Validation\n\nTo avoid using only one training and testing dataset, we can iterate over k-fold division of our data:\n\n\n\nGrey: all of the data\nPink: Testing data\nYellow: Training data"
  },
  {
    "objectID": "class_06.html#cross-validation-1",
    "href": "class_06.html#cross-validation-1",
    "title": "Data Science for Business Applications",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nProcedure for k-fold cross-validation:\n\nDivide your data in k-folds (usually, \\(K = 5\\) or \\(K = 10\\)).\nUse as \\(k = 1\\) the testing data and \\(k = 2,3,\\dots, K\\) as the training data.\nCalculate the accuracy measure on the testing data, \\(RMSE_k\\).\nRepeat for each \\(k\\).\nAverage \\(RMSE_k\\) for all \\(k\\).\n\nMain advantage: Use the entire dataset for training AND testing."
  },
  {
    "objectID": "class_06.html#apple-quarterly-revenue",
    "href": "class_06.html#apple-quarterly-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple quarterly revenue",
    "text": "Apple quarterly revenue\n\nInstall the library caret\n\n\nlibrary(caret)\nset.seed(100)\ntrain.control = trainControl(method = \"cv\", number = 10)\n\nlm_simple = train(logins ~ succession + city, data = hbomax, method= \"lm\", trControl = train.control)\n\nlm_simple\n\nLinear Regression \n\n5000 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 4500, 4501, 4499, 4500, 4500, 4501, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  2.087314  0.6724741  1.639618\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "class_06.html#stepwise-selection",
    "href": "class_06.html#stepwise-selection",
    "title": "Data Science for Business Applications",
    "section": "Stepwise selection",
    "text": "Stepwise selection\n\nWe have seen how to choose between some given models. But what if we want to test all possible models?\nStepwise selection: Computationally-efficient algorithm to select a model based on the data we have (subset selection).\nAlgorithm for forward stepwise selection:\n\n\nStart with the null model, (no predictors)\nFor : (a) Consider all models that augment with one additional predictor. (b) Choose the best among these models and call it .\nSelect the single best model from using CV.\n\n\nBackwards stepwise follows the same procedure, but starts with the full model."
  },
  {
    "objectID": "class_06.html#stepwise-selection-and-cv",
    "href": "class_06.html#stepwise-selection-and-cv",
    "title": "Data Science for Business Applications",
    "section": "Stepwise selection and CV",
    "text": "Stepwise selection and CV\n\nset.seed(100)\n# Linear Regression with Forward Selection\n# Remove unsubscribe\ntrain.control = trainControl(method = \"cv\", number = 10) #set up a 10-fold cv\nlm.fwd = train(logins ~.- unsubscribe, data = hbomax, method = \"leapForward\", \n               tuneGrid = data.frame(nvmax = 1:5), trControl = train.control)\n\n\nlm.fwd$results\n\n  nvmax     RMSE    Rsquared      MAE     RMSESD  RsquaredSD      MAESD\n1     1 3.643876 0.001423859 3.168804 0.05856896 0.001837302 0.08173805\n2     2 3.643778 0.002541723 3.168174 0.06094142 0.003036447 0.08474783\n3     3 3.186594 0.206309738 2.719227 0.62445616 0.282844240 0.59591617\n4     4 2.580810 0.468546464 2.125469 0.62430763 0.278310925 0.59607716\n5     5 2.087951 0.672274342 1.640141 0.04906724 0.014296583 0.04888083\n\n\n\nWhich one would you choose out of the 5 models? Why?\nThe model with the smallest RMSE, which is model 5.\nCan we see this model?"
  },
  {
    "objectID": "class_06.html#stepwise-selection-and-cv-1",
    "href": "class_06.html#stepwise-selection-and-cv-1",
    "title": "Data Science for Business Applications",
    "section": "Stepwise selection and CV",
    "text": "Stepwise selection and CV\n\nAnd how does that model looks like:\n\n\nsummary(lm.fwd$finalModel)\n\nSubset selection object\n6 Variables  (and intercept)\n           Forced in Forced out\nX              FALSE      FALSE\nfemale         FALSE      FALSE\ncity           FALSE      FALSE\nage            FALSE      FALSE\nsuccession     FALSE      FALSE\nid             FALSE      FALSE\n1 subsets of each size up to 5\nSelection Algorithm: forward\n         X   id  female city age succession\n1  ( 1 ) \" \" \" \" \" \"    \" \"  \" \" \"*\"       \n2  ( 1 ) \" \" \" \" \" \"    \"*\"  \" \" \"*\"       \n3  ( 1 ) \" \" \" \" \" \"    \"*\"  \"*\" \"*\"       \n4  ( 1 ) \" \" \" \" \"*\"    \"*\"  \"*\" \"*\"       \n5  ( 1 ) \"*\" \" \" \"*\"    \"*\"  \"*\" \"*\"       \n\n\n\nThe selected model has the following variables:\nfemale, city, age, succession, id"
  },
  {
    "objectID": "class_06.html#conclusion",
    "href": "class_06.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn prediction, everything is going to be about:\nBias vs Variance\nImportance of validation sets\nWe now have methods to select models"
  },
  {
    "objectID": "class_06.html",
    "href": "class_06.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "So far, we have been focusing mostly on trying to explain the effects from the predictors \\(X\\) through the coefficients.\nUntil now, our focus was on the soundness of our model in relation to statistical significance and how well our model was fitting the data (Regression Assumptions).\nToday, we will focus on making models that return Estimate/predict outcomes with high accuracy without extrapolating, with previously unseen data."
  },
  {
    "objectID": "class_07.html#quick-review-of-our-class",
    "href": "class_07.html#quick-review-of-our-class",
    "title": "Data Science for Business Applications",
    "section": "Quick review of our Class",
    "text": "Quick review of our Class\nThis is what we covered in previous classes:\n\nSimple and Multiple Regression\nCategorical Variables and Interactions\nResidual Analysis\nNonlinear Transformations\nTime Series\nModel Selection\n\nToday we will introduce a new model."
  },
  {
    "objectID": "class_07.html#the-okcupid-data-set",
    "href": "class_07.html#the-okcupid-data-set",
    "title": "Data Science for Business Applications",
    "section": "The OkCupid data set",
    "text": "The OkCupid data set\n\n\n\nThe OkCupid data set contains information about 59826 profiles from users of the OkCupid online dating service.\nWe have data on user age, height, sex, income , sexual orientation, education level, body type , ethnicity, and more.\nLet’s see if we can predict the sex of the user based on their height. (In this data set, everyone is classified as male or female.)"
  },
  {
    "objectID": "class_07.html#lets-build-the-model",
    "href": "class_07.html#lets-build-the-model",
    "title": "Data Science for Business Applications",
    "section": "Let’s build the model",
    "text": "Let’s build the model\n\nWhat’s wrong with this regression?\n\n\\[\n\\widehat{\\text{sex}} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_{0} \\cdot \\text{height}\n\\]\n\nThe \\(Y\\) variable here is categorical (two levels—everyone in this data set is either labeled male or female), so regular linear regression won’t work here.\nBut what if we just do it anyway?"
  },
  {
    "objectID": "class_07.html#binary-variable",
    "href": "class_07.html#binary-variable",
    "title": "Data Science for Business Applications",
    "section": "Binary Variable",
    "text": "Binary Variable\n\nLet’s first create a dummy variable male to convert sex to a quantitative dummy variable:\n\n\nlibrary(tidyverse)\nokcupid = okcupid %&gt;% \n  mutate(male = ifelse(okcupid$sex == \"m\", 1, 0))\n\n\nWe could do this with 1 representing either male or female (it wouldn’t matter)."
  },
  {
    "objectID": "class_07.html#regular-linear-regression",
    "href": "class_07.html#regular-linear-regression",
    "title": "Data Science for Business Applications",
    "section": "Regular Linear Regression",
    "text": "Regular Linear Regression\n\nggplot(okcupid, aes(x=height, y = male)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se = F)\n\n\n\nA line is not a great fit to this data—it’s not even close to linear. And what does it mean to predict that male = 0.7 (or 1.2)?"
  },
  {
    "objectID": "class_07.html#logistic-regression",
    "href": "class_07.html#logistic-regression",
    "title": "Data Science for Business Applications",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nInstead of predicting whether someone is male, let’s predict the probability that they are male\nIn logistic regression, one level of \\(Y\\) is always called “success” and the other called “failure.” Since \\(Y = 1\\) for males, in our setup we have designated males as “success.” (You could also set \\(Y = 1\\) for females and call females “success.”)\nLet’s fit a curve that is always between 0 and 1."
  },
  {
    "objectID": "class_07.html#odds-and-probabilty",
    "href": "class_07.html#odds-and-probabilty",
    "title": "Data Science for Business Applications",
    "section": "Odds and Probabilty",
    "text": "Odds and Probabilty\n\nTo fit the Logistic regression model we need to know the difference between odds and probability and how they relate.\nWhen something has “even (1/1) odds,” the probability of success is 1/2.\nWhen something has “2/1 odds,” the probability of success is 2/3.\nWhen something has “3/2 odds,” the probability of success is 3/5.\nIn general, the odds of something happening are \\(p/(1 − p)\\).\nWhere \\(p\\) is the probability defined bewteen zero and one.\nYou can transform odds to probability: \\[\n\\text{Odds} = \\frac{3}{2} = \\frac{3/(3+2)}{2/(3+2)} = \\frac{3/5}{2/5}  = \\frac{p}{1-p}\n\\]\nIf the odds are between zero and one they are not in your favor, \\((1-p)&gt;p\\)\nLet’s the explore this relation!"
  },
  {
    "objectID": "class_07.html#probability-vs-odds-vs-log-odds",
    "href": "class_07.html#probability-vs-odds-vs-log-odds",
    "title": "Data Science for Business Applications",
    "section": "Probability vs odds vs log odds",
    "text": "Probability vs odds vs log odds\n\n\n\nProbability \\(p\\)\nOdds \\(p/(1 − p)\\)\nLog odds \\(\\log(p/(1 − p))\\)\n\n\n\n\n0\n0\n\\(-\\infty\\)\n\n\n0.25\n0.33\n−1.10\n\n\n0.5\n1\n0\n\n\n0.75\n3\n1.10\n\n\n0.8\n4\n1.39\n\n\n0.9\n9\n2.20\n\n\n0.95\n19\n2.94\n\n\n1\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n\nProbability is between zero and one.\nOdds are strictly positive (greater than zero).\nLog odds ranges the whole real line."
  },
  {
    "objectID": "class_07.html#the-logistic-regression-model",
    "href": "class_07.html#the-logistic-regression-model",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nLogistic regression models the log odds of success \\(p\\) as a linear function of \\(X\\): \\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot X + e\n\\]\nThis fits an “S-shaped” curve to the data\nWe’ll see what it looks like later\nBy making this choice, we have a series of benefits.\nLet’s try it!"
  },
  {
    "objectID": "class_07.html#the-logistic-regression-model-1",
    "href": "class_07.html#the-logistic-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nWe need a different function - glm() (generalized linear models)\n\n\nmodel &lt;- glm(male ~ height, data = okcupid, family = binomial)\nsummary(model)\n\n\nCall:\nglm(formula = male ~ height, family = binomial, data = okcupid)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -44.448609   0.357510  -124.3   &lt;2e-16 ***\nheight        0.661904   0.005293   125.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 80654  on 59825  degrees of freedom\nResidual deviance: 44637  on 59824  degrees of freedom\nAIC: 44641\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nHow ca we interpret this model?"
  },
  {
    "objectID": "class_07.html#the-logistic-regression-model-2",
    "href": "class_07.html#the-logistic-regression-model-2",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nThe logistic regression output tells us that our prediction is \\[\n\\log(\\text{odds}) = \\log\\left(\\frac{\\widehat{p(\\text{male})}}{1-\\widehat{p(\\text{male})}}\\right) = −44.45 + 0.66 \\cdot \\text{height}\n\\]\nTo get the probability we have to solve in terms \\(\\widehat{p(\\text{male})}\\)\nThe probability of being male given height: \\[\n\\widehat{p(\\text{male})} = \\frac{\\exp(−44.45 + 0.66 \\cdot \\text{height})}{1+ \\exp(−44.45 + 0.66 \\cdot \\text{height})}\n\\] where \\(\\exp()\\) is the exponential function \\(e^x\\)."
  },
  {
    "objectID": "class_07.html#lets-show-this",
    "href": "class_07.html#lets-show-this",
    "title": "Data Science for Business Applications",
    "section": "Let’s show this",
    "text": "Let’s show this\nLet \\(\\widehat{p} = \\widehat{p(\\text{male})}\\), and \\(\\exp(X\\widehat{\\beta}) = \\exp(−44.45 + 0.66 \\cdot \\text{height})\\):\n\\[\n\\begin{eqnarray}\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right) &=& X\\widehat{\\beta} \\\\\n\\exp\\left(\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\\right) &=& \\exp(X\\widehat{\\beta}) \\\\\n\\frac{\\widehat{p}}{1-\\widehat{p}} &=& \\exp(X\\widehat{\\beta})\\\\\n\\widehat{p} &=& \\exp(X\\widehat{\\beta})\\cdot (1-\\widehat{p})\\\\\n\\widehat{p} &=&\\exp(X\\widehat{\\beta}) - \\exp(X\\widehat{\\beta}) \\cdot \\widehat{p} \\\\\n\\widehat{p} &=& \\frac{\\exp(X\\widehat{\\beta})}{1 + \\exp(X\\widehat{\\beta})}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "class_07.html#visualizing-the-model",
    "href": "class_07.html#visualizing-the-model",
    "title": "Data Science for Business Applications",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\n\nHow to interpret this curve?\nThe blue line is \\(\\widehat{p(\\text{male})}\\), given the height."
  },
  {
    "objectID": "class_07.html#interpreting-the-coefficients",
    "href": "class_07.html#interpreting-the-coefficients",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\nOur prediction equation is:\n\\[\n\\log(\\text{odds}) = \\log\\left(\\frac{\\widehat{p(\\text{male})}}{1-\\widehat{p(\\text{male})}}\\right) = −44.45 + 0.66 \\cdot \\text{height}\n\\]\nLet’s start with some basic, but not particularly useful, interpretations:\n\nWhen height = 0, we predict that the log odds will be -44.45 , so the probability of male is predicted to be very close to 0%.\nWhen height increases by 1 inch, we predict that the log odds of being male will increase by 0.66.\nInstead of log odds is better to have the interpretation in odds."
  },
  {
    "objectID": "class_07.html#interpreting-the-coefficients-1",
    "href": "class_07.html#interpreting-the-coefficients-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nLet’s rewrite the prediction equation as:\nPredicted odds of male, \\(\\exp(−44.45 + 0.66 \\cdot \\text{height})\\).\nIncreasing height by 1 inch will multiply the odds by \\(\\exp(0.66) = 1.94\\); i.e., increase the odds by 94%.\nIn summary, \\[(\\exp(\\widehat{\\beta}) - 1)\\times 100 = \\text{percentage change in odds}.\\]\nIncreasing height by 2 inches will multiply the odds by \\(\\exp(2\\cdot0.66) = 3.76\\); i.e., increase the odds by 276%.\nOdds equal to 1 indicate an one-to-one chance."
  },
  {
    "objectID": "class_07.html#making-predictions",
    "href": "class_07.html#making-predictions",
    "title": "Data Science for Business Applications",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is the probability of being male given we have a height of 69.\n\n\npredict(model, list(height=69), type=\"response\")\n\n        1 \n0.7725447 \n\n\n\nUsing the probability equation in R:\n\n\\[\n\\widehat{p(\\text{male})} = \\frac{\\exp(−44.45 + 0.66 \\cdot 69)}{1+ \\exp(−44.45 + 0.66 \\cdot 69)} = 0.77\n\\]\n\nexp(-44.448609 + 0.661904*69)/(1+exp(-44.448609 + 0.661904*69))\n\n[1] 0.7725501"
  },
  {
    "objectID": "class_07.html#adding-more-predictors",
    "href": "class_07.html#adding-more-predictors",
    "title": "Data Science for Business Applications",
    "section": "Adding more predictors",
    "text": "Adding more predictors\n\nAdding another predictor: can we do better?\nJust like with a linear regression model, we can add additional predictors to the model.\nOur interpretation of the coefficients in multiple logistic regression is similar to multiple linear regression, in the sense that each coefficient represents the predicted effect of one \\(X\\) on \\(Y\\), holding the other \\(X\\) variables constant."
  },
  {
    "objectID": "class_07.html#how-good-is-our-model",
    "href": "class_07.html#how-good-is-our-model",
    "title": "Data Science for Business Applications",
    "section": "How good is our model?",
    "text": "How good is our model?\n\nUnfortunately, the typical root mean squared error, RSE metric isn’t available for logistic regression.\nHowever, there are many metrics that indicate model fit.\nBut: most of these metrics are difficult to interpret, so we’ll focus on something simpler to interpret and communicate."
  },
  {
    "objectID": "class_07.html#accuracy-of-the-model",
    "href": "class_07.html#accuracy-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\n\nWe could use our model to make a prediction of sex based on the probability.\nSuppose we say that our prediction is: \\[\n\\text{Prediction} = \\begin{cases}\n\\text{male}, & \\text{if $\\widehat{P(\\text{male})} \\geq 0.5$}, \\\\\n\\text{female}, & \\text{if $\\widehat{P(\\text{male})} &lt; 0.5$}. \\\\\n\\end{cases}\n\\]\nGiven a threshold of 0.5, now we can compute the fraction of individuals whose sex we correctly predicted.\nFor males and females.\nThis is known as the accuracy of the model."
  },
  {
    "objectID": "class_07.html#accuracy-of-the-model-1",
    "href": "class_07.html#accuracy-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\n\nWe can use the xtabs function to get the accuracy:\nWe add we the number of correctly predicted groups for both male and female, and divide by the total of observations.\n\n\nokcupid = okcupid %&gt;% \n  mutate(predict.sex = ifelse(predict(model, type=\"response\") &gt;= 0.5,\"m\",\"f\"))\nxtabs(~ predict.sex + sex,okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nCorrectly predicted that is female - 19466\nCorrectly predicted that is male - 30243\nTotal number of individuals in the sample - 59826\nThe accuracy is (19466 + 30243)/59826 = 0.831, or 83%"
  },
  {
    "objectID": "class_07.html#confusion-matrix",
    "href": "class_07.html#confusion-matrix",
    "title": "Data Science for Business Applications",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nThe table from xtabs is also called a Confusion matrix\n\n\n\n\n\nActual failure\nActual success\n\n\n\n\nModel predicts failure\nTrue negative\nFalse negative\n\n\nModel predicts success\nFalse positive\nTrue positive\n\n\n\n\nTrue positives: predicting male for someone that is male\nTrue negatives: predicting female for someone that is female\nFalse positives: predicting male for someone that is female\nFalse negatives: predicting female for someone that is male\nIf we had designated female as 1 and male as 0, these would have switched\nSo Accuracy = (True negative + True positive)/(Total cases)"
  },
  {
    "objectID": "class_07.html#accuracy-of-the-model-2",
    "href": "class_07.html#accuracy-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\nSuppose that the Amazon is trying to build a model to predict which costumers buy a certain product:\n\nSuppose that 0.01% of people are costumers of this product (the product is really expensive / high revenue)\nA “null” or “no-brainer” model that predicts that no one is a costumer will be 99.99% accurate.\nThe revenue coming from our model would be zero.\nBut the model could make two different kinds of prediction errors:\nFalse positive: predicting someone is a customer when they really are not\nFalse negative: predicting someone is not a customer when they really are\nThese two measures give us a better idea of the predictive power of our model."
  },
  {
    "objectID": "class_07.html#false-positive-rate",
    "href": "class_07.html#false-positive-rate",
    "title": "Data Science for Business Applications",
    "section": "False positive rate",
    "text": "False positive rate\nThe false positive rate is the proportion of actual failures where the model predicted success.\n\nxtabs(~ predict.sex + sex, okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nFalse Positives - predicting someone is a male when they really are female\nActual failure - number of cases that are female\nFalse positive rate = False positives/ Actual failure\nIn our model, the false positive rate is 4623/24089 = 0.19"
  },
  {
    "objectID": "class_07.html#false-negative-rate",
    "href": "class_07.html#false-negative-rate",
    "title": "Data Science for Business Applications",
    "section": "False negative rate",
    "text": "False negative rate\nThe false negative rate is the proportion of actual successes where the model predicted failure.\n\nxtabs(~ predict.sex + sex, okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nFalse Negatives - predicting someone is a female when they really are male\nActual success - number of cases that are male\nFalse negative rate = False negatives/ Actual success\nIn our model, the false positive rate is 5494/35737 = 0.15"
  },
  {
    "objectID": "class_07.html#changing-rates",
    "href": "class_07.html#changing-rates",
    "title": "Data Science for Business Applications",
    "section": "Changing rates",
    "text": "Changing rates\nHow do we reduce false positive/negative rates?\n\nInstead of using 50% as a cutoff probability to decide when to predict success, use a higher (or lower) probability.\nFor example, we could have the model predict that someone is male only if \\(\\widehat{p(\\text{male})} \\geq 0.8\\), instead of 0.5:\n\n\nokcupid = okcupid %&gt;% \n  mutate(predict.sex = ifelse(predict(model, type=\"response\") &gt;= 0.8,\"m\",\"f\"))\nxtabs(~ predict.sex + sex,okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   22650 12938 35588\n        m    1439 22799 24238\n        Sum 24089 35737 59826\n\n\n\nAccuracy = (21425+26753)/59826 = 0.76\nFalse positive rate = 2664/24089 = 0.06\nFalse negative rate = 8984/59826 = 0.36"
  },
  {
    "objectID": "class_07.html#prediction-trade-off",
    "href": "class_07.html#prediction-trade-off",
    "title": "Data Science for Business Applications",
    "section": "Prediction Trade off",
    "text": "Prediction Trade off\n\nWe can decrease the false positive rate, but at the expense of increasing the false negative rate.\nOr we can decrease the false negative rate, but at the expense of increasing the false positive rate.\nWe might choose a cutoff probability other than 50% based on our assessment of the relative costs of the two different kinds of errors."
  },
  {
    "objectID": "class_07.html#summary",
    "href": "class_07.html#summary",
    "title": "Data Science for Business Applications",
    "section": "Summary",
    "text": "Summary\nIn a logistic regression model, the response variable is binary, taking values of either zero or one.\n\nThe model estimates the log odds of the event associated with a response of one.\nThe model’s effects are interpreted in terms of odds.\nPredictions are expressed as probabilities.\nThe performance of the model is evaluated based on its prediction accuracy."
  },
  {
    "objectID": "class_07.html",
    "href": "class_07.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "This is what we covered in previous classes:\n\nSimple and Multiple Regression\nCategorical Variables and Interactions\nResidual Analysis\nNonlinear Transformations\nTime Series\nModel Selection\n\nToday we will introduce a new model."
  },
  {
    "objectID": "class_08.html#what-is-causal-inference",
    "href": "class_08.html#what-is-causal-inference",
    "title": "Data Science for Business Applications",
    "section": "What is Causal Inference?",
    "text": "What is Causal Inference?"
  },
  {
    "objectID": "class_08.html#early-key-ideas",
    "href": "class_08.html#early-key-ideas",
    "title": "Data Science for Business Applications",
    "section": "Early key ideas",
    "text": "Early key ideas\nInformal review on causes and effects\n\nFrancis Bacon (1561-1626) was talking about “control” and specifically “controlled experiments”.\nDavid Hume (1711-1776) was worried about “confounding”. Correlation is not causation.\nJohn Stuart Mill (1806-1873) Mill was focusing on “contrasts”.\nCan we measure causality from what we observe?"
  },
  {
    "objectID": "class_08.html#causality",
    "href": "class_08.html#causality",
    "title": "Data Science for Business Applications",
    "section": "Causality",
    "text": "Causality\nCauses of effects: Given an outcome, what were its causes?\n\nA patient has a headache. Why?\nA city experiences a crime wave. Why?\nThe stock market is down today. Why?\n\nEffects of causes: Given a cause, what was its effect?\n\nThe patient took an aspirin. Did it mitigate the severity or duration of their headache?\nWhat is the impact of police presence on crime rates?\nHow much is the coronovirus affecting the stock market?\n\nWe will focus exclusively on the latter – measuring effects – in order to avoid the ill-posedness of multiple causes. Inferring the effect of one thing on another thing."
  },
  {
    "objectID": "class_08.html#measuring-the-causal-effect",
    "href": "class_08.html#measuring-the-causal-effect",
    "title": "Data Science for Business Applications",
    "section": "Measuring the causal effect",
    "text": "Measuring the causal effect\nHow do measure this causal effect?\n\nCounterfactual comparison\n\nSuppose we are measuring the effect of college degree on income:\n\n\\(Z_i \\in \\{0,1\\}\\) is a binary variable indicating if a person went to college.\n\\(Y_{i}(1)\\) is the income of person \\(i\\) if they went to college, i.e., when \\(Z_i = 1\\).\n\\(Y_{i}(0)\\) is the income of person \\(i\\) if they did not go to college, i.e., when \\(Z_i = 0\\)\n\nThe causal effect of the college degree on income is: \\[\n\\text{causal effect of college on income} = Y_{i}(1)- Y_{i}(0)\n\\] - This framework is known as the Potential Outcomes approach to causal inference. - This framework is also known as the Neyman-Rubin model (1974)."
  },
  {
    "objectID": "class_08.html#potential-outcomes",
    "href": "class_08.html#potential-outcomes",
    "title": "Data Science for Business Applications",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\nIn the Potential Outcomes framework, we aim to compare each unit to the alternative reality, that is, the counterfactual, where they received the opposite treatment assignment from what they actually received.\nIn the previous case:\n\n\\(Y_{i}(1)\\) is the counterfactual of \\(Y_{i}(0)\\), and vice versa.\nBoth are potential outcomes of income given the treatment \\(Z_i\\).\nThe causal effect is the difference between the potential outcomes.\n\\(Y_{i}(1)\\) is referred to as the treatment group.\n\\(Y_{i}(0)\\) is referred to as the control group.\nThe observations \\(Y_i\\) can be rewritten in terms of the potential outcomes as: \\[Y_i = Z_i \\cdot Y_i(1) + (1 − Z_i) ⋅ Y_i(0)\\]\nWhat is the problem with this framework?"
  },
  {
    "objectID": "class_08.html#potential-outcomes-1",
    "href": "class_08.html#potential-outcomes-1",
    "title": "Data Science for Business Applications",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nHolland (1986) defines in “Statistics and Causal Inference” the “fundamental problem” of causal inference.\nTo determine the causal effect, we must observe both \\(Y_{i}(1)\\) and \\(Y_{i}(0)\\), but we only get to see one of the two!\nThis means that either person \\(i\\) went to college or they did not. We cannot have both scenarios happening at the same time.\nHow can we deal with this problem?"
  },
  {
    "objectID": "class_08.html#average-treatment-effect",
    "href": "class_08.html#average-treatment-effect",
    "title": "Data Science for Business Applications",
    "section": "Average treatment effect",
    "text": "Average treatment effect\n\nInstead of measuring the effect on an individual unit, we aim to estimate the average treatment effect (ATE) of going to college across several individuals in a population of interest. At the population level, we have: \\[\n\\text{Population ATE} = E[Y_{i}(1) - Y_{i}(0)]\n\\]\nHere, \\(E[X_i]\\) is the population average, or better known as the expected value of the random variable \\(X_i\\).\nBut this is on the population level we can’t infer nothing yet.\nAlso, we still have the problem of only observing one possible outcome \\(Y_{i}(1)\\) or \\(Y_{i}(0)\\)."
  },
  {
    "objectID": "class_08.html#estimating-ate",
    "href": "class_08.html#estimating-ate",
    "title": "Data Science for Business Applications",
    "section": "Estimating ATE",
    "text": "Estimating ATE\n\nWe can estimate the sample ATE as:\n\n\\[\n\\text{Sample ATE} = E[Y_{i}(1)] - E[Y_{i}(0)] + \\text{bias}\n\\]\n\nThis bias can be eliminated if we adopt certain assumptions, such as the independence of treatment assumption (also called ignorability), which ensures that the treatment assignment is independent of the potential outcomes.\nIn our example, this would mean that the potential outcomes, \\(Y_{i}(1)\\) and \\(Y_{i}(0)\\), are independent of whether a person chooses to go to college (i.e., the treatment assignment \\(Z\\))."
  },
  {
    "objectID": "class_08.html#sample-ate",
    "href": "class_08.html#sample-ate",
    "title": "Data Science for Business Applications",
    "section": "Sample ATE",
    "text": "Sample ATE\nSuppose we have \\(N\\) observations of incomes \\(Y_i\\) from individuals who did and did not go to college. \\(N_1\\) is the number of individuals who went to college, and \\(N_0\\) is the number of individuals who did not go to college, so \\(N = N_0 + N_1\\).\nFrom the sample ATE, we have: \\[E[Y_{i}(1)] = \\frac{1}{N_1}\\sum_{i = 1}^{N_1} (Y_i \\text{ given } Z = 1)\\] which is the sample average of the outcome for individuals who received the treatment (i.e., went to college). Similarly, we have: \\[E[Y_{i}(0)] = \\frac{1}{N_0}\\sum_{i = 1}^{N_0} (Y_i \\text{ given } Z = 0)\\] which is the sample average of the outcome for individuals who did not receive the treatment (i.e., did not go to college)."
  },
  {
    "objectID": "class_08.html#sample-ate-1",
    "href": "class_08.html#sample-ate-1",
    "title": "Data Science for Business Applications",
    "section": "Sample ATE",
    "text": "Sample ATE\n\nSo the ATE is giving by: \\[\n\\text{Sample ATE} = \\frac{1}{N_1}\\sum_{i = 1}^{N_1} (Y_i\\text{ given } Z = 1) - \\frac{1}{N_0}\\sum_{i = 1}^{N_0} (Y_i\\text{ given } Z = 0)\n\\] Which can be rewritten in a more friendly way as \\[\n\\text{Sample ATE} = \\text{mean of the treated} - \\text{mean of the control}\n\\]"
  },
  {
    "objectID": "class_08.html#example",
    "href": "class_08.html#example",
    "title": "Data Science for Business Applications",
    "section": "Example",
    "text": "Example\n\nIn this example we have sample of \\(N = 6\\) students that went to the college, \\(i = 1,2,\\dots,6\\).\nThree of the individuals went to college, \\(N_1 = 3\\), and the other three did not, \\(N_0 = 3\\).\n\\(Y_i\\) are the observed incomes, \\(Y_i(1)\\), and \\(Y_i(0)\\) are the potential outcomes.\n\\(Z_i\\in \\{0,1\\}\\) is the treatment.\n\n\n\n\n\\(i\\)\n\\(Z_i\\)\n\\(Y_i\\)\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(Y_i(1) - Y_i(0)\\)\n\n\n\n\n1\n0\n50.000\n?\n50.000\n?\n\n\n2\n1\n55.000\n55.000\n?\n?\n\n\n3\n1\n120.000\n120.000\n?\n?\n\n\n4\n0\n150.000\n?\n150.000\n?\n\n\n5\n0\n45.000\n?\n45.000\n?\n\n\n6\n1\n130.000\n130.000\n?\n?"
  },
  {
    "objectID": "class_08.html#sample-ate-2",
    "href": "class_08.html#sample-ate-2",
    "title": "Data Science for Business Applications",
    "section": "Sample ATE",
    "text": "Sample ATE\n\nThe estimate of the ATE is giving by\n\n\\[\\begin{eqnarray}\n\\text{Sample ATE} &=& \\text{mean of the treated} - \\text{mean of the control} \\\\\n                  &=& \\frac{305,000}{3} - \\frac{245,000}{3} \\\\\n                  &=& 20,000\n\\end{eqnarray}\\]\n\nOn average, going to college has a positive causal effect of US$ 20,000 on income compared to those who do not go to college.\nAgain, this conclusion was made under very strong assumptions.\nWe will discuss these assumptions in the next class.\nWe are essentially assuming that the treatment is random, meaning that attending college or not was randomly assigned. This is not a reasonable assumption in most cases."
  },
  {
    "objectID": "class_09.html#potential-outcomes",
    "href": "class_09.html#potential-outcomes",
    "title": "Data Science for Business Applications",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nLast week we discussed potential outcomes., (e.g. \\(Y_i(1)\\) and \\(Y_i(0)\\)):\n“The outcome that we would have observed under different scenarios”\nPotential outcomes are related to your choices/possible conditions:\nOne for each path (Counterfactuals).\nDo not confuse them with the values that your outcome variable can take.\nDefinition of Causal Effect for individual \\(i\\): \\[\n\\text{causal effect for an individual} = Y_{i}(1)- Y_{i}(0)\n\\]\nBetter to assume for a population (Difference in means) \\[\n\\text{ATE} = E\\left[Y_{i}(1)- Y_{i}(0)\\right] = E\\left[Y_{i}(1)\\right] - E\\left[Y_{i}(0)\\right]\n\\]"
  },
  {
    "objectID": "class_09.html#causal-effect",
    "href": "class_09.html#causal-effect",
    "title": "Data Science for Business Applications",
    "section": "Causal effect",
    "text": "Causal effect\n\nFor a sample:\n\n\\[\n\\text{Average} [Y_{i}(1)- Y_{i}(0)] = \\text{mean of the treated} - \\text{mean of the untreated}\n\\]\n\nUnder what assumptions is our estimate causal?\nKey assumption: Ignorability means that the potential outcomes \\(Y_i(0)\\) and \\(Y_i(1)\\) are independent of the treatment.\nIn our example this means that the decision to pursue a college degree should not be related to unmeasured factors that could influence income.\nIn reality, this assumption can be difficult to fully satisfy. There could be unobserved factors, such as intrinsic ability or motivation, that affect both the likelihood of obtaining a college degree and future income, leading to potential confounding.\nWhat can we do to make the ignorability assumption hold?"
  },
  {
    "objectID": "class_09.html#randomization",
    "href": "class_09.html#randomization",
    "title": "Data Science for Business Applications",
    "section": "Randomization",
    "text": "Randomization\nOne way to make sure the ignorability assumption holds is to do it by design:\n\nRandomize the assignment of the treatment \\(Z\\)\ni.e. Some units will randomly be chosen to be in the treatment group and others to be in the control group.\nWhat does randomization buy us?\nControl for unforeseen factors (confounders)"
  },
  {
    "objectID": "class_09.html#confounders",
    "href": "class_09.html#confounders",
    "title": "Data Science for Business Applications",
    "section": "Confounders",
    "text": "Confounders\n\nConfounder is a variable that affects both the treatment AND the outcome"
  },
  {
    "objectID": "class_09.html#confounders-1",
    "href": "class_09.html#confounders-1",
    "title": "Data Science for Business Applications",
    "section": "Confounders",
    "text": "Confounders\nLet’s identify some confounders\n\nEstimate the effect of insurance vs no insurance on number of accidents \\(\\rightarrow\\) Compare people with insurance vs people without insurance.\nConfounder: (Driving Behavior/Risk Aversion) Risk-averse individuals are more likely to purchase insurance and may also drive more cautiously, reducing their number of accidents.\nEstimate the effect of gym membership vs no gym membership on physical health \\(\\rightarrow\\) Compare people with gym memberships vs people without gym memberships.\nConfounder: (Motivation for Fitness) Individuals who are more motivated to improve their health are more likely to purchase a gym membership and are also more likely to engage in other healthy behaviors, such as maintaining a balanced diet, which improves their physical health."
  },
  {
    "objectID": "class_09.html#randomization-1",
    "href": "class_09.html#randomization-1",
    "title": "Data Science for Business Applications",
    "section": "Randomization",
    "text": "Randomization\n\nDue to randomization, we know that the treatment is not affected by a confounder\n\n\n\nWe have “clean effect” of the treatment on the outcome\nThis would be the causal effect of the treatment"
  },
  {
    "objectID": "class_09.html#randomized-controlled-trials-rcts",
    "href": "class_09.html#randomized-controlled-trials-rcts",
    "title": "Data Science for Business Applications",
    "section": "Randomized controlled trials (RCTs)",
    "text": "Randomized controlled trials (RCTs)\n\nOften called the “gold standard” for establishing causality.\nRandomly assign the \\(Z\\), “treatment”, to participants\nNow, any observed relationship between \\(Z\\) and \\(Y\\) must be due to \\(Z\\), since the only reason an individual had a particular value of \\(X\\) was the random assignment."
  },
  {
    "objectID": "class_09.html#randomized-controlled-trials-rcts-1",
    "href": "class_09.html#randomized-controlled-trials-rcts-1",
    "title": "Data Science for Business Applications",
    "section": "Randomized controlled trials (RCTs)",
    "text": "Randomized controlled trials (RCTs)"
  },
  {
    "objectID": "class_09.html#rct---steps",
    "href": "class_09.html#rct---steps",
    "title": "Data Science for Business Applications",
    "section": "RCT - Steps",
    "text": "RCT - Steps\n\nCheck for balance\n\n\n(We will see what this is about)\n\n\nRandomize\nCalculate difference in sample means between treatment and control group"
  },
  {
    "objectID": "class_09.html#section",
    "href": "class_09.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Example 1: Clinical Trial for the Moderna COVID-19 vaccine\nRandomly assign study participants to get either the vaccine:\n\nan treatment group of 14,134 people\ncontrol group, the same size\nResults: 11 vaccine recipients got COVID; 235 of placebo recipients got COVID\n\n\nlibrary(mosaic)\n\n# Control and treatment group \n\n# Difference in proportions\nprop.test(outcome ~ treatment, data = data.rct, success = 1)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  tally(outcome ~ treatment)\nX-squared = 215.01, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.01435140 0.01890174\nsample estimates:\n      prop 1       prop 2 \n0.0174048394 0.0007782652"
  },
  {
    "objectID": "class_09.html#issues-with-rct",
    "href": "class_09.html#issues-with-rct",
    "title": "Data Science for Business Applications",
    "section": "Issues with RCT",
    "text": "Issues with RCT\n\nInternal validity is the ability of an experiment to establish cause-and-effect of the treatment within the sample studied.\nExamples of threats to internal validity:\nFailure to randomize.\nFailure to follow the treatment protocol/attrition.\nSmall sample sizes"
  },
  {
    "objectID": "class_09.html#issues-with-rct-1",
    "href": "class_09.html#issues-with-rct-1",
    "title": "Data Science for Business Applications",
    "section": "Issues with RCT",
    "text": "Issues with RCT\n\nExternal validity is the ability of an experimental result to generalize to a larger context or population.\nExamples of threats to external validity:\nFailure to randomize.\nNon representative samples.\nNon representative protocol/policy."
  },
  {
    "objectID": "class_09.html#blocking",
    "href": "class_09.html#blocking",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nRandomization works “on average” but we only get one opportunity at creating treatment and control groups, and there might be imbalances in “nuisance” variables that could affect the outcome.\nFor example, what will happen if the treatment group for the Moderna trial happens to get younger people in it than the control group?\nWe can solve this by blocking or stratifying: randomly assigning to treatment/control within groups."
  },
  {
    "objectID": "class_09.html#blocking-1",
    "href": "class_09.html#blocking-1",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nUnbalanced sample"
  },
  {
    "objectID": "class_09.html#blocking-2",
    "href": "class_09.html#blocking-2",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nBlocking or stratification sample"
  },
  {
    "objectID": "class_09.html#blocking-in-vaccine-trial",
    "href": "class_09.html#blocking-in-vaccine-trial",
    "title": "Data Science for Business Applications",
    "section": "Blocking in vaccine trial",
    "text": "Blocking in vaccine trial\n\nIn the Moderna vaccine trial, they identified two possible variables that could impact COVID outcomes:\nAge (65+ vs under 65)\nUnderlying health condition"
  },
  {
    "objectID": "class_09.html#blocking-in-vaccine-trial-1",
    "href": "class_09.html#blocking-in-vaccine-trial-1",
    "title": "Data Science for Business Applications",
    "section": "Blocking in vaccine trial",
    "text": "Blocking in vaccine trial"
  },
  {
    "objectID": "class_09.html#experiments-using-regression",
    "href": "class_09.html#experiments-using-regression",
    "title": "Data Science for Business Applications",
    "section": "Experiments using regression",
    "text": "Experiments using regression\n\nNon-blocked design: use a simple regression \\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 T,\n\\]\nwhere \\(T\\) is a dummy variable that is \\[\nT =\n\\begin{cases}\n  1, & \\text{for the treatment group}, \\\\\n  0, & \\text{for the control group}\n\\end{cases}\n\\]\n\\(\\widehat{\\beta}_1\\) represents the estimated average treatment effect. The regression needs to be logistic if Y is categorical!"
  },
  {
    "objectID": "class_09.html#experiments-using-regression-1",
    "href": "class_09.html#experiments-using-regression-1",
    "title": "Data Science for Business Applications",
    "section": "Experiments using regression",
    "text": "Experiments using regression\n\nBlocked design: use a regression that controls for the blocking variable \\(B\\):\n\n\\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 T + \\widehat{\\beta}_2 B,\n\\]\n\nwhere \\(B\\) is the fixed effect of each strata, that are interactions between categories.\nImportant: the regression needs to be logistic if \\(Y\\) is categorical."
  },
  {
    "objectID": "class_09.html#get-out-the-vote-gotv",
    "href": "class_09.html#get-out-the-vote-gotv",
    "title": "Data Science for Business Applications",
    "section": "Get Out The Vote (GOTV)",
    "text": "Get Out The Vote (GOTV)\n\nFact: lots of people don’t vote.\nIt’s important for people to vote, to ensure that our government reflects the will of its constituents.\nHow do we get people to vote?"
  },
  {
    "objectID": "class_09.html#get-out-the-vote-gotv-1",
    "href": "class_09.html#get-out-the-vote-gotv-1",
    "title": "Data Science for Business Applications",
    "section": "Get Out The Vote (GOTV)",
    "text": "Get Out The Vote (GOTV)\n\nIn 2002, researchers at Temple and Yale conducted a large phone banking experiment to see calling voters helps:\nFrom among about 381,062 phone numbers of voters in Iowa and Michigan they randomly contacted about 12000 voters\nThe outcome Y of interest is whether each voter actually voted."
  },
  {
    "objectID": "class_09.html#no-blocking",
    "href": "class_09.html#no-blocking",
    "title": "Data Science for Business Applications",
    "section": "No blocking",
    "text": "No blocking\nEstimating the average treatment effect with logistic regression:\n\nglm = glm(vote02 ~ treatment,data = GOTV, family = \"binomial\")\nsummary(glm)\n\n\nCall:\nglm(formula = vote02 ~ treatment, family = \"binomial\", data = GOTV)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        0.184717   0.003306  55.870   &lt;2e-16 ***\ntreatmenttreatment 0.170824   0.018843   9.066   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 524839  on 381061  degrees of freedom\nResidual deviance: 524756  on 381060  degrees of freedom\nAIC: 524760\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nThe coefficients are in log odds."
  },
  {
    "objectID": "class_09.html#no-blocking-1",
    "href": "class_09.html#no-blocking-1",
    "title": "Data Science for Business Applications",
    "section": "No blocking",
    "text": "No blocking\n\nThe average treatment effect will be of approximately 19%\n\n\n(exp(0.17)-1)*100\n\n[1] 18.53049\n\nconfint(glm)\n\n                       2.5 %    97.5 %\n(Intercept)        0.1782378 0.1911978\ntreatmenttreatment 0.1339278 0.2077954\n\n\n\nReceiving a phone call increases the likelihood of voting by 19% compared to those who did not receive a call.\nConfidence interval for the treatment\n\n\nconfint(glm)\n\n                       2.5 %    97.5 %\n(Intercept)        0.1782378 0.1911978\ntreatmenttreatment 0.1339278 0.2077954"
  },
  {
    "objectID": "class_09.html#blocking-3",
    "href": "class_09.html#blocking-3",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nThe researchers actually used a blocking design with two variables that they thought could impact voting rates (separately from the phone calls):\nThe state of the voter (Iowa (0) or Michigan (1))\nWhether the voter was in a “competitive” district (one where there was likely to be a close election)"
  },
  {
    "objectID": "class_09.html#blocking-4",
    "href": "class_09.html#blocking-4",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking"
  },
  {
    "objectID": "class_09.html#blocking-5",
    "href": "class_09.html#blocking-5",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nGOTV = GOTV %&gt;%\n       mutate(block = interaction(state, competiv))\nglm_vote = glm(vote02 ~ treatment + block, data = GOTV, family = 'binomial')\nsummary(glm_vote)\n\n\nCall:\nglm(formula = vote02 ~ treatment + block, family = \"binomial\", \n    data = GOTV)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        0.043236   0.004146   10.43   &lt;2e-16 ***\ntreatmenttreatment 0.028542   0.019279    1.48    0.139    \nblock1.1           0.351686   0.015168   23.19   &lt;2e-16 ***\nblock0.2           0.196691   0.008866   22.18   &lt;2e-16 ***\nblock1.2           0.603739   0.009515   63.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 524839  on 381061  degrees of freedom\nResidual deviance: 520331  on 381057  degrees of freedom\nAIC: 520341\n\nNumber of Fisher Scoring iterations: 4\n\nconfint(glm_vote)\n\n                          2.5 %     97.5 %\n(Intercept)         0.035109260 0.05136249\ntreatmenttreatment -0.009210835 0.06636325\nblock1.1            0.321979732 0.38143873\nblock0.2            0.179317682 0.21407167\nblock1.2            0.585102929 0.62239941"
  },
  {
    "objectID": "class_09.html#blocking-6",
    "href": "class_09.html#blocking-6",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nThe effect of the treatment is not significant under blocking.\nWhat if some callers didn’t stick to the script?\nMany people didn’t answer the phone!\nWhat about voters outside of the Midwest?"
  },
  {
    "objectID": "class_09.html#the-limitations-of-rcts",
    "href": "class_09.html#the-limitations-of-rcts",
    "title": "Data Science for Business Applications",
    "section": "The limitations of RCTs",
    "text": "The limitations of RCTs\n\nAlthough powerful for inferring causation, RCTs are difficult to apply.\nThey can be incredibly expensive.\nCompliance with the treatment protocol isn’t perfect (e.g., mask-wearing, picking up the phone)\nIt can be hard to generalize beyond the participants involved in the study.\nThey can be impractical or (e.g., effect of education on performance) or unethical to conduct (e.g., seatbelts, parachutes, even medical trials)"
  },
  {
    "objectID": "class_10.html#the-limitations-of-rcts",
    "href": "class_10.html#the-limitations-of-rcts",
    "title": "Data Science for Business Applications",
    "section": "The limitations of RCTs",
    "text": "The limitations of RCTs\nAlthough they are powerful for inferring causation, RCTs are hard to pull off:\n\nThey can be incredibly expensive (e.g., Phase 3 clinical trial)\nCompliance with the treatment protocol isn’t perfect\nIt can be hard to generalize beyond the participants involved in the study, if they aren’t representative.\nThey can be impractical (e.g., effect of education on later earnings) or even unethical (e.g., seatbelts, parachutes, even medical trials)"
  },
  {
    "objectID": "class_10.html#quasi-and-natural-experiments",
    "href": "class_10.html#quasi-and-natural-experiments",
    "title": "Data Science for Business Applications",
    "section": "Quasi-and natural experiments",
    "text": "Quasi-and natural experiments\nKey idea: Find a comparison group that is effectively “the same as” the treatment group to create a “quasi-experiment”a or “natural experiment”a\n\nCausal Question: Does serving in the military have an impact upon your long-term earnings after discharge?\nWhy won’t this work: Compare the wages of people who served in the US military in Afghanistan or Iraq, 10 years after discharge, to wages of the general public"
  },
  {
    "objectID": "class_10.html#the-effect-of-military-service-on-earnings",
    "href": "class_10.html#the-effect-of-military-service-on-earnings",
    "title": "Data Science for Business Applications",
    "section": "The effect of military service on earnings",
    "text": "The effect of military service on earnings\nAngrist (1990) wanted to determine what effect military service had on future earnings\n\n“Treatment” group: men selected by lottery to serve in Vietnam\n“Control” group: men eligible to be drafted but not selected to serve\nWe effectively have (almost) random assignment\nThis is called a “natural” experiment because we have “discovered” what is nearly an RCT out “in the wild”!\nFor white men, earnings in the 1980s were 15% lower in the treatment group; military service in Vietnam really did cause those serving to have less earning power long-term"
  },
  {
    "objectID": "class_10.html#quasi-and-natural-experiments-1",
    "href": "class_10.html#quasi-and-natural-experiments-1",
    "title": "Data Science for Business Applications",
    "section": "Quasi-and natural experiments",
    "text": "Quasi-and natural experiments\n\nThese are called quasi-experiments or natural experiments because participants are not randomly assigned to treatment and control groups, but groups are selected in such a way that the assignment can be thought of as effectively random."
  },
  {
    "objectID": "class_10.html#difference-in-differences",
    "href": "class_10.html#difference-in-differences",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\nA natural experiment of the minimum wage\n\nWhat happens if we raise the minimum wage?\nEconomic theory says there should be fewer jobs.\nWhy can’t we just compare the unemployment rate in places with a low minimum wage (e.g., Texas) to places with a high minimum wage (e.g., California)?\nWhy can’t we just do a randomized controlled trial to study the impact of raising the minimum wage?"
  },
  {
    "objectID": "class_10.html#difference-in-differences-1",
    "href": "class_10.html#difference-in-differences-1",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\nIn 1992, New Jersey’s minimum wage went from $4.25 to $5.05\nThe minimum wage in Pennsylvania remained at $4.25\nResearchers measured employment at 410 fast food restaurants in NJ and PA both before and after the change\nThis is a “natural” experiment because the two groups arose naturally (rather than being assigned by the researchers)"
  },
  {
    "objectID": "class_10.html#difference-in-differences-2",
    "href": "class_10.html#difference-in-differences-2",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences"
  },
  {
    "objectID": "class_10.html#pre-vs-post-comparison",
    "href": "class_10.html#pre-vs-post-comparison",
    "title": "Data Science for Business Applications",
    "section": "Pre vs post comparison",
    "text": "Pre vs post comparison\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nNew Jersey\n20.44\n21.03\n0.59\n\n\n\n\nEmployment went up by 0.59 employees per store in NJ. Can we interpret this as a causal effect?\nWe cannot distinguish the effect of the minimum wage increase from other things that changed in NJ at the same time."
  },
  {
    "objectID": "class_10.html#nj-vs-pa-comparison",
    "href": "class_10.html#nj-vs-pa-comparison",
    "title": "Data Science for Business Applications",
    "section": "NJ vs PA comparison",
    "text": "NJ vs PA comparison\n\n\n\n\nAfter\n\n\n\n\nPennsylvania\n21.17\n\n\nNew Jersey\n21.03\n\n\nDifference\n−0.14\n\n\n\n\nAfter the policy change, employment was 0.14 employees per store less in NJ than in PA. Can we interpret this as a causal effect?\nWe cannot distinguish the effect of the minimum wage increase from other differences between PA and NJ."
  },
  {
    "objectID": "class_10.html#difference-in-differences-3",
    "href": "class_10.html#difference-in-differences-3",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nPennsylvania\n23.33\n21.17\n-2.16\n\n\nNew Jersey\n20.44\n21.03\n0.59\n\n\nDifference\n-2.89\n-0.14\n2.76\n\n\n\n\nThe difference of the differences (−0.14 − (−2.89) or 0.59 − (−2.16)) gives us the causal effect of the policy change."
  },
  {
    "objectID": "class_10.html#difference-in-differences-4",
    "href": "class_10.html#difference-in-differences-4",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences"
  },
  {
    "objectID": "class_10.html#difference-in-differences-5",
    "href": "class_10.html#difference-in-differences-5",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\nDifference-in-differences framework"
  },
  {
    "objectID": "class_10.html#difference-in-differences-6",
    "href": "class_10.html#difference-in-differences-6",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\nWhy can’t we just compare college attainment of students who were and were not offered financial aid?\nIn 1982, the US government ended a program that provided $6,700 in financial aid to students whose parents were deceased.\nWe can therefore compare college attainment among four groups to get a natural experiment of the impact of financial aid:\nStudents with deceased parents, before 1982\nStudents with deceased parents, after 1982\nStudents without deceased parents, before 1982\nStudents without deceased parents, after 1982"
  },
  {
    "objectID": "class_10.html#difference-in-differences-7",
    "href": "class_10.html#difference-in-differences-7",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\nWhat is the impact of financial aid on degree attainment?\n\n\n\n\n\n\n\n\n\n\n1979-81\n1982-83\nDifference\n\n\n\n\nWith deceased parents\n56%\n35%\n21%\n\n\nWithout deceased parents\n50%\n48%\n2%\n\n\nDifference\n6%\n-13%\n19%\n\n\n\n\nThe $6,700 in financial aid led to a 19-percentage point increase in college attainment."
  },
  {
    "objectID": "class_10.html#difference-in-differences-8",
    "href": "class_10.html#difference-in-differences-8",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences"
  },
  {
    "objectID": "class_10.html#diff-diff-in-regression",
    "href": "class_10.html#diff-diff-in-regression",
    "title": "Data Science for Business Applications",
    "section": "Diff & Diff in Regression",
    "text": "Diff & Diff in Regression\nWe can also do the difference-in-differences calculation using regression. If we set: \\[\nT =\n\\begin{cases}\n1, & \\text{for stores in New Jersey (treatment)} \\\\\n0, & \\text{for stores in Pennsylvania (control)}\n\\end{cases}\n\\]\nand\n\\[\nX =\n\\begin{cases}\n1, & \\text{for measurements after the policy change (post)} \\\\\n0, & \\text{for measurements before the policy change (pre)}\n\\end{cases}\n\\]\nThen we can fit the same regression discontinuity model with interaction. \\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\beta}_2T + \\widehat{\\beta}_3T\\cdot X\n\\] The coefficient \\(\\widehat{\\beta}_3\\) of \\(T\\) is the causal effect we’re looking for!"
  },
  {
    "objectID": "class_10.html#assumptions",
    "href": "class_10.html#assumptions",
    "title": "Data Science for Business Applications",
    "section": "Assumptions",
    "text": "Assumptions\n\nParallel Trends\nIn the absence of the intervention, treatment and control group would have changed in the same way"
  },
  {
    "objectID": "class_10.html#natural-experiments",
    "href": "class_10.html#natural-experiments",
    "title": "Data Science for Business Applications",
    "section": "Natural experiments",
    "text": "Natural experiments\nWays to create natural experiments\n\nGeographic boundaries (e.g., NJ vs PA minimum wage example)\nPolicy changes (e.g., financial aid policy change example)\nLotteries (e.g., Vietnam draft lottery example)\nArbitrary cutoffs"
  },
  {
    "objectID": "class_11.html#causal-inference",
    "href": "class_11.html#causal-inference",
    "title": "Data Science for Business Applications",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nNatural Experiments (RCTs in the wild).\nAlways check for balance! (All things equal!)\nDifference-in-Differences (Diff-in-Diff):\nHow we can use two wrong estimates to get a right one.\nAssumptions behind DD (Parallel changes)."
  },
  {
    "objectID": "class_11.html#regression-discontinuity-design",
    "href": "class_11.html#regression-discontinuity-design",
    "title": "Data Science for Business Applications",
    "section": "Regression Discontinuity Design",
    "text": "Regression Discontinuity Design\n\nWhat will we learn today?\nRegression Discontinuity Design (RDD)\nHow can we use discontinuities to recover causal effects?\nAssumptions behind RDD designs."
  },
  {
    "objectID": "class_11.html#introduction",
    "href": "class_11.html#introduction",
    "title": "Data Science for Business Applications",
    "section": "Introduction",
    "text": "Introduction\n\nArbitrary rules determine treatment assignment:\nIf you are above a threshold, you are assigned to treatment, and if your below, you are not (or vice versa)\nGeographic discontinuities\nTime discontinuities\nVoting discontinuities\nYou can find discontinuities everywhere!"
  },
  {
    "objectID": "class_11.html#example-cohort-size",
    "href": "class_11.html#example-cohort-size",
    "title": "Data Science for Business Applications",
    "section": "Example: Cohort size",
    "text": "Example: Cohort size\n\nMany people argue that smaller classes lead to better learning outcomes compared to large classes.\nBut why can’t we just compare test scores of students in small classes and students in large classes?\nAngrist & Levy (1999) studied this by taking advantage of a rule in Israeli schools, where cohorts of &gt;40 students are split into two smaller classes"
  },
  {
    "objectID": "class_11.html#example-cohort-size-1",
    "href": "class_11.html#example-cohort-size-1",
    "title": "Data Science for Business Applications",
    "section": "Example: Cohort size",
    "text": "Example: Cohort size\nKey idea: Students in cohorts just below 40 students are essentially identical to students in cohorts just above 40, but the ones in the latter group will get a smaller class.\n\nlibrary(tidyverse)\nggplot(class_1999, aes(x = cohort.size, y = read, color = class_split)) + \n  geom_point() +\n  geom_vline(xintercept = 40, linetype = \"dashed\")\n\n\n\nIs there a difference in the reading score between larger and smaller classes?"
  },
  {
    "objectID": "class_11.html#creating-the-rdd-model",
    "href": "class_11.html#creating-the-rdd-model",
    "title": "Data Science for Business Applications",
    "section": "Creating the RDD model",
    "text": "Creating the RDD model\n\nDefine a treatment variable: \\[\\begin{equation}\nT = \\begin{cases}\n1, \\quad \\text{split cohort},\\\\\n0, \\quad \\text{intact cohort}\n\\end{cases}\n\\end{equation}\\]\nRecenter the selection variable so the cutoff is at 0: \\[\\begin{equation}\nX = (\\texttt{cohort.size}) − 40\n\\end{equation}\\]\nThen fit a model predicting reading scores from both \\(X\\) and \\(T\\): \\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_1 X + \\widehat{\\beta}_2 T\n\\end{equation}\\]\n\n\nThe coefficient \\(\\widehat{\\beta}_2\\) of \\(T\\) is the causal effect we’re looking for!"
  },
  {
    "objectID": "class_11.html#rdd-model",
    "href": "class_11.html#rdd-model",
    "title": "Data Science for Business Applications",
    "section": "RDD model",
    "text": "RDD model\n\nclass_1999 = class_1999 %&gt;% \n  mutate(treatment=ifelse(cohort.size &gt; 40, 1, 0), \n         selection=(cohort.size - 40))\n\nrdd1 &lt;- lm(read ~ selection + treatment, data=class_1999) \nsummary(rdd1)\n\n\nCall:\nlm(formula = read ~ selection + treatment, data = class_1999)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.195  -5.572   1.537   6.617  17.269 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  69.7556     1.2697  54.939   &lt;2e-16 ***\nselection    -0.1195     0.2020  -0.592   0.5545    \ntreatment     4.0031     2.1511   1.861   0.0638 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.135 on 294 degrees of freedom\nMultiple R-squared:  0.02237,   Adjusted R-squared:  0.01572 \nF-statistic: 3.363 on 2 and 294 DF,  p-value: 0.03596\n\n\n\nThe effect of the treatment is an increase of \\(\\widehat{\\beta}_2 = 4\\) points in the reading score if the students."
  },
  {
    "objectID": "class_11.html#pre-vs-post-comparison",
    "href": "class_11.html#pre-vs-post-comparison",
    "title": "Data Science for Business Applications",
    "section": "Pre vs post comparison",
    "text": "Pre vs post comparison\nOur first RDD model is forcing the two lines to have the same slope; that isn’t a great fit for the data:"
  },
  {
    "objectID": "class_11.html#rdd-and-interactions",
    "href": "class_11.html#rdd-and-interactions",
    "title": "Data Science for Business Applications",
    "section": "RDD and interactions",
    "text": "RDD and interactions\n\nTo allow the two slopes to be different, we can add an interaction term to allow the slope of \\(X\\) to be different for \\(T = 0\\) (cohort kept intact) and \\(T = 1\\) (cohort split into smaller classes): \\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X + \\widehat{\\beta}_2 T + \\widehat{\\beta}_3 T\\cdot X\n\\end{equation}\\]\nAgain, the slope of \\(\\widehat{\\beta}_2\\) is our estimate of the causal effect of the treatment."
  },
  {
    "objectID": "class_11.html#rdd-with-interactions",
    "href": "class_11.html#rdd-with-interactions",
    "title": "Data Science for Business Applications",
    "section": "RDD with interactions",
    "text": "RDD with interactions\n\nrdd2 &lt;- lm(read ~ selection*treatment, data=class_1999) \nsummary(rdd2)\n\n\nCall:\nlm(formula = read ~ selection * treatment, data = class_1999)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.618  -6.102   1.341   6.922  17.249 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          66.6294     1.8141  36.729   &lt;2e-16 ***\nselection            -0.8945     0.3806  -2.350   0.0194 *  \ntreatment             5.6641     2.2439   2.524   0.0121 *  \nselection:treatment   1.0720     0.4477   2.395   0.0173 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.063 on 293 degrees of freedom\nMultiple R-squared:  0.04113,   Adjusted R-squared:  0.03132 \nF-statistic:  4.19 on 3 and 293 DF,  p-value: 0.00634\n\n\n\nFrom our data we an conclude that smaller class sizes cause reading scores to increase by about 5.7 points."
  },
  {
    "objectID": "class_11.html#rdd-with-interactions-1",
    "href": "class_11.html#rdd-with-interactions-1",
    "title": "Data Science for Business Applications",
    "section": "RDD with interactions",
    "text": "RDD with interactions\nMore flexibility with the interaction gives the model a better fit in relation to the data:"
  },
  {
    "objectID": "class_11.html#conclusion",
    "href": "class_11.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nRDD is usually great for internal validity, but there are lots of threats to external validity.\nWhy this statement holds?\nFor example, would this generalize to different grade levels? Schools outside of Israel?"
  },
  {
    "objectID": "class_11.html#example-sales",
    "href": "class_11.html#example-sales",
    "title": "Data Science for Business Applications",
    "section": "Example: Sales",
    "text": "Example: Sales\n\nYou are managing a retail store and notice that sales are low in the mornings, so you want to improve those numbers.\nA store gives a 10% discount to the first 1,000 customers that arrive.\nIs this a good candidate for regression discontinuity?\nLet’s look at the data."
  },
  {
    "objectID": "class_11.html#example-sales-1",
    "href": "class_11.html#example-sales-1",
    "title": "Data Science for Business Applications",
    "section": "Example: Sales",
    "text": "Example: Sales\n\n\nSales in relation to time since the opening of the store in minutes.\nThe store receives its 1,000th customer around 260 minutes after opening (around 4.3 hours)."
  },
  {
    "objectID": "class_11.html#creating-the-rdd-model-1",
    "href": "class_11.html#creating-the-rdd-model-1",
    "title": "Data Science for Business Applications",
    "section": "Creating the RDD model",
    "text": "Creating the RDD model\n\nDefine a treatment variable: \\[\\begin{equation}\nT = \\begin{cases}\n1, \\quad \\text{promotion}\\\\\n0, \\quad \\text{no promotion}\n\\end{cases}\n\\end{equation}\\]\nRecenter the selection variable so the cutoff is at 0: \\[\\begin{equation}\nX = (\\texttt{time}) - 260\n\\end{equation}\\]\nThen fit a model predicting the sales of a customer from both \\(X\\) and \\(T\\): \\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_1 X + \\widehat{\\beta}_2 T\n\\end{equation}\\]\n\n\nThe coefficient \\(\\widehat{\\beta}_2\\) is the causal effect we are looking for!"
  },
  {
    "objectID": "class_11.html#rdd-model-1",
    "href": "class_11.html#rdd-model-1",
    "title": "Data Science for Business Applications",
    "section": "RDD model",
    "text": "RDD model\n\nsales = sales %&gt;% \n  mutate(selection=(time - 260))\n\nrdd_sales &lt;- lm(sales ~ selection + treat, data=sales) \nsummary(rdd_sales)\n\n\nCall:\nlm(formula = sales ~ selection + treat, data = sales)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-77.32 -14.62   0.14  14.56  68.95 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 165.61482    1.08422  152.75   &lt;2e-16 ***\nselection    -0.10294    0.00661  -15.57   &lt;2e-16 ***\ntreat        31.52628    1.95845   16.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.82 on 1997 degrees of freedom\nMultiple R-squared:  0.6539,    Adjusted R-squared:  0.6535 \nF-statistic:  1886 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\n\n\nOn average, providing a 10% discount increases sales by $31.30 for the 1,000 customers, compared to not having a discount."
  },
  {
    "objectID": "class_11.html#rdd-model-2",
    "href": "class_11.html#rdd-model-2",
    "title": "Data Science for Business Applications",
    "section": "RDD model",
    "text": "RDD model"
  },
  {
    "objectID": "class_11.html#a-more-flexible-rdd-model",
    "href": "class_11.html#a-more-flexible-rdd-model",
    "title": "Data Science for Business Applications",
    "section": "A more flexible RDD model",
    "text": "A more flexible RDD model\n\nAs in the first example, we have that two lines of the RDD model have the same slope.\nWe can make this model more flexible by adding an interaction term.\nTo allow the two slopes to be different, we can add an interaction term to allow the slope of \\(X\\) to be different for \\(T = 0\\) (after 1,000th customer) and \\(T = 1\\) (first thousand customers):\n\n\\[\\begin{equation}\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\, X + \\widehat{\\beta}_2 \\, T + \\widehat{\\beta}_3 \\, X\\cdot T\n\\end{equation}\\]"
  },
  {
    "objectID": "class_11.html#rdd-with-interaction",
    "href": "class_11.html#rdd-with-interaction",
    "title": "Data Science for Business Applications",
    "section": "RDD with interaction",
    "text": "RDD with interaction\n\nrdd_sales_inter &lt;- lm(sales ~ selection*treat, data=sales) \nsummary(rdd_sales_inter)\n\n\nCall:\nlm(formula = sales ~ selection * treat, data = sales)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-65.738 -13.940   0.051  13.538  76.515 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     178.574245   1.297815  137.60   &lt;2e-16 ***\nselection        -0.205355   0.008882  -23.12   &lt;2e-16 ***\ntreat            31.399196   1.842316   17.04   &lt;2e-16 ***\nselection:treat   0.200845   0.012438   16.15   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 1996 degrees of freedom\nMultiple R-squared:  0.6939,    Adjusted R-squared:  0.6934 \nF-statistic:  1508 on 3 and 1996 DF,  p-value: &lt; 2.2e-16\n\n\n\nOn average, providing a 10% discount increases sales by $33.10 for the 1,000 customers, compared to not having a discount."
  },
  {
    "objectID": "class_11.html#rdd-with-interaction-1",
    "href": "class_11.html#rdd-with-interaction-1",
    "title": "Data Science for Business Applications",
    "section": "RDD with interaction",
    "text": "RDD with interaction\n\nWe have different slopes, for before and after the treatment."
  },
  {
    "objectID": "class_11.html#conclusion-1",
    "href": "class_11.html#conclusion-1",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nAgain, RDD is usually great for internal validity, but there are lots of threats to external validity.\nFor example, would this generalize to different types of products? Same store, but a different location?"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Henrique Bolfarine",
    "section": "Contact",
    "text": "Contact\n\nEmail: henrique.bolfarfine at austin.utexas.edu\nAddress: GSB 3.140A, McCombs School of Business, 2110 Speedway, Austin"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Henrique Bolfarine",
    "section": "Research Interests",
    "text": "Research Interests\n\nBayesian Statistics\nLatent Variable modelling\nModel Summarization\nFactor Models\nBayesian mixture models\nNonparametric models"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "Research",
    "section": "",
    "text": "“Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models.” H. Bolfarine, H.F. Lopes, & C.M. Carvalho. https://arxiv.org/abs/2506.09850 - (Accepted - Statistic and Computing)"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\n“Decoupling Shrinkage and Selection in Gaussian Linear Factor Analysis.” H. Bolfarine, C.M. Carvalho, H.F. Lopes, & J.S. Murray. Bayesian Analysis (2024) 19, Number 1, pp. 181–203\n“2021 International Statistical Institute Mahalanobis Award: A Tribute to Heleno Bolfarine.” F. Ruggeri, H.Bolfarine, J. L. Bazán, R. B. Arellano-Valle, V. H. L. Davila, M. de Castro. International Statistical Review (2021), 89, 3, 2021 International Statistical Institute."
  },
  {
    "objectID": "about.html#preprints",
    "href": "about.html#preprints",
    "title": "Research",
    "section": "Preprints",
    "text": "Preprints\n\n“Network reconstruction with local partial correlation: comparative evaluation.” H. Bolfarine, L. Thomas, & A. Yambartsev. https://arxiv.org/abs/1806.04098"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Henrique Bolfarine",
    "section": "CV",
    "text": "CV\n\nHenrique Bolfarine - CV"
  },
  {
    "objectID": "teaching.html#university-of-texas-at-austin",
    "href": "teaching.html#university-of-texas-at-austin",
    "title": "Teaching",
    "section": "",
    "text": "STA 235 - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Error, Uncertainty and the Linear Model\nclass 03 - Regression Assumptions\nclass 04 - Categorical Variables\nclass 05 - Interactions\nclass 06 - Modeling nonlinear relationships\nclass 07 - Time series modelling\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables\nclass 03 - Interactions\nclass 04 - Regression Assumptions\nclass 05 - Modeling nonlinear relationships\nclass 06 - Time Series Decomposition and Autoregression\nclass 07 - Model building for prediction\nclass 08 - Logistic Regression 1\nclass 09 - Logistic Regression 2\nclass 10 - Decision Trees\nclass 11 - Random Forest\nclass 12 - Building models for explanation\nclass 13 - Basic Causal Inference\nclass 14 - Natural experiments and RDD\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 Honors - Data Science for Business Applications\n\nclass 01 - Simple and Multiple Regression\nclass 02 - Categorical Variables and Interactions\nclass 03 - Regression assumptions and Outliers\nclass 04 - Modeling nonlinear relationships\nclass 05 - Time Series Decomposition and Autoregression\nclass 06 - Model Selection\nclass 07 - Logistic Regression\nclass 08 - Basic Causal Inference\nclass 09 - Randomized Control Trials\nclass 10 - Natural Experiments - Diff-in-Diff\nclass 11 - Natural Experiments - RDD\n\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Applications\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 301 - Introduction to Data Science\n\n\n\n\n\nSTA 235 - Data Science for Business Application\n\n\n\n\n\nSTA 235 - Data Science for Business Application"
  },
  {
    "objectID": "index.html#cv---henrique-bolfarine---cv",
    "href": "index.html#cv---henrique-bolfarine---cv",
    "title": "Henrique Bolfarine",
    "section": "CV - Henrique Bolfarine - CV",
    "text": "CV - Henrique Bolfarine - CV"
  },
  {
    "objectID": "about.html#invited-reviewer",
    "href": "about.html#invited-reviewer",
    "title": "Research",
    "section": "Invited Reviewer",
    "text": "Invited Reviewer\n\nBayesian Analysis\nEntropy\nStatistics and Computing"
  },
  {
    "objectID": "teaching.html#university-of-são-paulo",
    "href": "teaching.html#university-of-são-paulo",
    "title": "Teaching",
    "section": "University of São Paulo",
    "text": "University of São Paulo\n\nSummer 2021\n\nIntroduction to Probability Theory\n\n\n\nSummer 2020\n\nIntroduction to Probability Theory"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html",
    "href": "fall_sta235_2025/week_01/week_01.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Use regression to build predictive models\nUnderstand the benefits and limitations of the models we build\nGiven a new business situation, select an appropriate model, build it, measure its effectiveness, and effectively communicate the results\nThis is a practical course!\n\n\n\n\n\n\n\n\n\n\n\n\nWhy bother learning this stuff when we can get ChatGPT to do data analysis for us?\nAI (and computing in general) is only useful when you have the expertise to be able to recognize the correctness (or not) of its output\nIn this class, you’ll develop that expertise!\n\n\n\n\n\nInstructor: Henrique Bolfarine, Ph.D.\n\nOffice hours: Mondays 1:00 PM - 2:00 PM (GSB 3.140 A)\nEmail: henrique.bolfarine@austin.utexas.edu\n\nCourse Assistants:\n\nLead Course Assistant (CA): Ezgi Durakoglugil\nOffice hours: Many TA/CA office hours every week (both in person and on Zoom) - This should be your first option!\nYou can ask any of the TAs/CAs about course content, but go to Ezgi for questions about logistics"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#course-goals",
    "href": "fall_sta235_2025/week_01/week_01.html#course-goals",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Use regression to build predictive models\nUnderstand the benefits and limitations of the models we build\nGiven a new business situation, select an appropriate model, build it, measure its effectiveness, and effectively communicate the results\nThis is a practical course!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#why-does-this-course-exist",
    "href": "fall_sta235_2025/week_01/week_01.html#why-does-this-course-exist",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Why bother learning this stuff when we can get ChatGPT to do data analysis for us?\nAI (and computing in general) is only useful when you have the expertise to be able to recognize the correctness (or not) of its output\nIn this class, you’ll develop that expertise!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#about-the-course-staff",
    "href": "fall_sta235_2025/week_01/week_01.html#about-the-course-staff",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Instructor: Henrique Bolfarine, Ph.D.\n\nOffice hours: Mondays 1:00 PM - 2:00 PM (GSB 3.140 A)\nEmail: henrique.bolfarine@austin.utexas.edu\n\nCourse Assistants:\n\nLead Course Assistant (CA): Ezgi Durakoglugil\nOffice hours: Many TA/CA office hours every week (both in person and on Zoom) - This should be your first option!\nYou can ask any of the TAs/CAs about course content, but go to Ezgi for questions about logistics"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#course-structure",
    "href": "fall_sta235_2025/week_01/week_01.html#course-structure",
    "title": "Data Science for Business Applications",
    "section": "Course Structure",
    "text": "Course Structure\n\nUnits\n\nUnit A: Fundamentals of regression modeling\nUnit B: Applications and extensions\n\nCanvas\n\nMake sure you can log in and are enrolled in STA 235 in Canvas\nCheck out the home page for the weekly schedule and to meet the course staff"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#statistical-computing",
    "href": "fall_sta235_2025/week_01/week_01.html#statistical-computing",
    "title": "Data Science for Business Applications",
    "section": "Statistical Computing",
    "text": "Statistical Computing\n\n\n\nWe will use R and RStudio for statistical analysis throughout the course\nMake sure both are installed on your laptop and bring it to every class\nIf you aren’t comfortable with R/RStudio from STA 301, don’t worry!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#weekly-cadence-for-a-particular-topic",
    "href": "fall_sta235_2025/week_01/week_01.html#weekly-cadence-for-a-particular-topic",
    "title": "Data Science for Business Applications",
    "section": "Weekly Cadence for a Particular Topic",
    "text": "Weekly Cadence for a Particular Topic\n\nDue by the start of class on Monday/Tuesday: Perusall pre-class video/reading discussion covering the topic\nDuring class on Monday/Tuesday: Lecture, activities, practice topic\nDue by 11:59 PM the following Sunday/Monday: Homework covering the topic\nThe following Monday/Tuesday: at the beginning of class: Checkpoint Quiz on that topic"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#pre-class-work",
    "href": "fall_sta235_2025/week_01/week_01.html#pre-class-work",
    "title": "Data Science for Business Applications",
    "section": "Pre-Class Work",
    "text": "Pre-Class Work\n\nThis is a fast-paced course, so it’s essential that you think about the material before class.\nWe will use Perusall for pre-class video and reading assignments.\nUse Perusall to ask your classmates questions, and share your knowledge, thoughts, and opinions.\nThis helps you better understand the material and will help me gear class time to what topics you are having the most trouble with."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#pre-class-work-1",
    "href": "fall_sta235_2025/week_01/week_01.html#pre-class-work-1",
    "title": "Data Science for Business Applications",
    "section": "Pre-Class Work",
    "text": "Pre-Class Work\n\nPre-class assignments (typically videos) are due at the start of each class.\nAim to chime in with at least a few thoughtful questions, responses, or comments for each reading assignment.\nGrading is based on effort and thoughtfulness of your questions and comments and your engagement with classmates and the text.\nEach assignment is scored 0-3, but with a reasonable effort you will get a 3 on each one (so don’t worry about your grade)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#homework",
    "href": "fall_sta235_2025/week_01/week_01.html#homework",
    "title": "Data Science for Business Applications",
    "section": "Homework",
    "text": "Homework\n\n\n\nWhy homework?\nHomework is due each week at 11:59 PM the night before class and submitted through Canvas.\nAutomatically graded; resubmit as many times as you want!\nOK to work together, but try the problems on your own first for maximum benefit."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checkpoint-quizzes",
    "href": "fall_sta235_2025/week_01/week_01.html#checkpoint-quizzes",
    "title": "Data Science for Business Applications",
    "section": "Checkpoint Quizzes",
    "text": "Checkpoint Quizzes\n\nIt is critical in this course to stay on top of things and not fall behind.\nCheckpoint Quiz at the start of each class will help you ensure that you are really learning the material and give you an early heads-up if you aren’t.\nWe’ll drop your lowest quiz score from each unit (A and B).\nYou’ll have access to RStudio and a “cheat sheet” during quizzes (don’t spend time memorizing anything!)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#mastery-exams",
    "href": "fall_sta235_2025/week_01/week_01.html#mastery-exams",
    "title": "Data Science for Business Applications",
    "section": "Mastery Exams",
    "text": "Mastery Exams\n\nEach unit concludes with a Mastery Exam:\n\nUnit A: October 22 or 23 at 7 to 9 PM\nUnit B: University-assigned final exam period\n\nYou’ll have access to RStudio and a “cheat sheet” during exams (don’t spend time memorizing anything!)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#assessment-grading",
    "href": "fall_sta235_2025/week_01/week_01.html#assessment-grading",
    "title": "Data Science for Business Applications",
    "section": "Assessment Grading",
    "text": "Assessment Grading\n\nUnit A has 7 Checkpoint Quizzes and Unit B has 6.\nFor each unit, we will replace your lowest quiz score with your exam score for that unit (if that helps your overall grade)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#grading",
    "href": "fall_sta235_2025/week_01/week_01.html#grading",
    "title": "Data Science for Business Applications",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nComponent\nPoints\n\n\n\n\nPre-class work (Perusall)\n44\n\n\nClass Participation\n56\n\n\nHomework (13)\n195\n\n\nCheckpoint Quizze (13)\n325\n\n\nExam A\n190\n\n\nExam B\n190\n\n\nTotal\n1,000"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#getting-help",
    "href": "fall_sta235_2025/week_01/week_01.html#getting-help",
    "title": "Data Science for Business Applications",
    "section": "Getting Help",
    "text": "Getting Help\n\nMy office hours: Schedule on Canvas.\nTA/CA office hours: Schedule on Canvas.\nPost questions in videos in Perusall (for questions about the course material).\nPost questions in group chats in Perusall (for general questions about the course, or homework questions).\nWeekly optional TA/CA-led review session (TBD)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#section",
    "href": "fall_sta235_2025/week_01/week_01.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "What personal characteristics about an instructor do you think are predictive of the scores they receive on student evaluations?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#hamermesh-parker-2005-data-set",
    "href": "fall_sta235_2025/week_01/week_01.html#hamermesh-parker-2005-data-set",
    "title": "Data Science for Business Applications",
    "section": "Hamermesh & Parker (2005) Data Set",
    "text": "Hamermesh & Parker (2005) Data Set\n\n\nStudent evaluations of \\(N=463\\) instructors at UT Austin, 2000-2002\nFor each instructor:\neval: average student evaluation of teacher\nbeauty: average beauty score from a six-student panel\ngender: male or female\ncredits: single- or multi-credit course\nage: age of instructor\n(and more…)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-texttteval",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-texttteval",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: \\(\\texttt{eval}\\)",
    "text": "Explore the data: \\(\\texttt{eval}\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-textttbeauty",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-textttbeauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: \\(\\texttt{beauty}\\)",
    "text": "Explore the data: \\(\\texttt{beauty}\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#more-time-series-datasets",
    "href": "fall_sta235_2025/week_01/week_01.html#more-time-series-datasets",
    "title": "Data Science for Business Applications",
    "section": "More time series datasets",
    "text": "More time series datasets\nWhat are some common characteristics of these time series? How do they differ?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#components-of-time-series",
    "href": "fall_sta235_2025/week_01/week_01.html#components-of-time-series",
    "title": "Data Science for Business Applications",
    "section": "Components of time series",
    "text": "Components of time series\nWe can think about a time series as having one or more components, or sources of variability:\n\n\nTrend+cyclic: Persistent, (usually) slow-moving long-run patterns\n\nLong run, Apple’s earnings go up over time\n\nSeasonal: Regular up-and-down movement around long-run trends\n\nApple’s earnings vary predictably each quarter around the long-run trend\n\nRandom or unpredictable variation\n\nCustomers are fickle creatures, so earnings aren’t perfectly predictable\n\nExternal factors, like shocks that interrupt or change previous dynamics\n\nIntroductions of the iPhone, supply chain shocks, …"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#modeling-time-series",
    "href": "fall_sta235_2025/week_01/week_01.html#modeling-time-series",
    "title": "Data Science for Business Applications",
    "section": "Modeling time series",
    "text": "Modeling time series\nWe can model many time series using our standard regression tools!\n\nTrend+cyclic:\n\nModel long-run trends with linear or nonlinear functions of time\nUse today’s outcome directly to predict tomorrow’s\n\nSeasonal:\n\nTreat the season (quarter, month, week, etc) as a categorical variable\n\nExternal factors:\n\nCreate new predictors from other time series or to reflect known events"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#time-series-data",
    "href": "fall_sta235_2025/week_01/week_01.html#time-series-data",
    "title": "Data Science for Business Applications",
    "section": "Time series data",
    "text": "Time series data\nSome notation:\n\\[\\begin{align*}\nt &= \\text{time }(1, 2, 3, \\ldots) \\\\\nY_t &= \\text{the value of the variable we are interested in, at time $t$}\n\\end{align*}\\]\nThe change from \\(Y_i\\) to \\(Y_t\\) reflects that each observation (row) corresponds to a time indexed by \\(t\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#structure-of-the-apple-earnings-data",
    "href": "fall_sta235_2025/week_01/week_01.html#structure-of-the-apple-earnings-data",
    "title": "Data Science for Business Applications",
    "section": "Structure of the Apple earnings data",
    "text": "Structure of the Apple earnings data\n\nhead(apple) # %&gt;% select(-Time))\n\n  Period    Time Year Quarter Revenue\n1      1 2011.50 2011      Q3   28.27\n2      2 2011.75 2011      Q4   46.33\n3      3 2012.00 2012      Q1   39.19\n4      4 2012.25 2012      Q2   35.02\n5      5 2012.50 2012      Q3   35.97\n6      6 2012.75 2012      Q4   54.51\n\n\n\n\\(Y_t\\) = \\(\\text{Revenue_t}\\)\n\\(\\text{Period}_t\\) = \\(t\\)\n\\(Time_t\\): Used for plots"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data",
    "href": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data",
    "title": "Data Science for Business Applications",
    "section": "Trend and Seasonality in the Apple earnings data",
    "text": "Trend and Seasonality in the Apple earnings data\n\nApple’s earnings have a clear trend over time and seasonality by quarter."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data-1",
    "href": "fall_sta235_2025/week_01/week_01.html#trend-and-seasonality-in-the-apple-earnings-data-1",
    "title": "Data Science for Business Applications",
    "section": "Trend and Seasonality in the Apple earnings data",
    "text": "Trend and Seasonality in the Apple earnings data\n\napple_model1 &lt;- lm(Revenue ~ Period + Quarter, data = apple)\n\n...\n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  29.4695     3.0511    9.66      0.0000000000068 ***\nPeriod        1.3046     0.0892   14.62 &lt; 0.0000000000000002 ***\nQuarterQ2    -8.3919     3.1939   -2.63                0.012 *  \nQuarterQ3    -5.0707     3.1977   -1.59                0.121    \nQuarterQ4    22.4692     3.1939    7.03      0.0000000191875 ***\n...\n\n\n\\[\n\\hat Y_t = 29.5 + 1.3\\text{Period}_t - 8.4\\text{Q2}_t\n- 5.1\\text{Q3}_t\n+ 22.5\\text{Q4}_t\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#trend-seasonality-in-the-apple-earnings-data",
    "href": "fall_sta235_2025/week_01/week_01.html#trend-seasonality-in-the-apple-earnings-data",
    "title": "Data Science for Business Applications",
    "section": "Trend + Seasonality in the Apple earnings data",
    "text": "Trend + Seasonality in the Apple earnings data\n\\[\n\\hat Y_t = 29.5 + 1.3\\text{Period}_t - 8.4\\text{Q2}_t\n- 5.1\\text{Q3}_t\n+ 22.5\\text{Q4}_t\n\\]\n\nOn average, earnings in the same quarter increase \\(4\\times 1.3 = \\$5.2\\)b every year.\nAfter accounting for the trend, Q2 earnings are \\(\\$ 7.8\\)b lower than Q1 on average\nAfter accounting for the trend, Q3 earnings are ??? lower than Q1 on average\nAfter accounting for the trend, Q4 earnings are \\(\\$ 22.4\\)b higher than Q1 on average"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-assumptions",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nLet’s check on our modeling assumptions…\n\nWhich are violated?\n\nLinearity and Equal variance! (Independence TBD)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#whats-going-on",
    "href": "fall_sta235_2025/week_01/week_01.html#whats-going-on",
    "title": "Data Science for Business Applications",
    "section": "What’s going on?",
    "text": "What’s going on?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonlinearity + Increasing variance over time usually suggests taking log of \\(Y\\) to get a multiplicative model"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#a-multiplicative-model-is-more-appropriate",
    "href": "fall_sta235_2025/week_01/week_01.html#a-multiplicative-model-is-more-appropriate",
    "title": "Data Science for Business Applications",
    "section": "A multiplicative model is more appropriate",
    "text": "A multiplicative model is more appropriate\n\napple_model2 &lt;- lm(log(Revenue) ~ Period + Quarter, data = apple)\n\n...\n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.55729    0.03877   91.75 &lt; 0.0000000000000002 ***\nPeriod       0.02134    0.00113   18.82 &lt; 0.0000000000000002 ***\nQuarterQ2   -0.15339    0.04059   -3.78              0.00053 ***\nQuarterQ3   -0.10239    0.04063   -2.52              0.01595 *  \nQuarterQ4    0.32398    0.04059    7.98          0.000000001 ***\n...\n\n\n\\[\n\\widehat {\\log(Y_t)} = 3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t\n\\]\n\\[\n\\hat Y_t = e^{3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t}\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model",
    "href": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the multiplicative model",
    "text": "Interpreting the multiplicative model\n\\[\n\\hat Y_t = e^{3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t}\n\\]\n\nOn average, earnings in the same quarter increase by a factor of \\(\\exp(4\\times 0.02) = 1.083\\), or 8.3%, every year\nAfter accounting for the trend, on average Q2 earnings are lower than Q1 by a factor of \\(\\exp(-0.16) = 0.852\\), or \\(14.8\\%\\)\nAfter accounting for the trend, on average Q3 earnings are ??? than Q1 by a factor of ???, or ???%.\nAfter accounting for the trend, Q4 earnings are higher than Q1 by a factor of \\(\\exp(0.32) = 1.377\\), or \\(37.7\\%\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model-1",
    "href": "fall_sta235_2025/week_01/week_01.html#interpreting-the-multiplicative-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the multiplicative model",
    "text": "Interpreting the multiplicative model\n\\[\n\\hat Y_t = e^{3.56 + 0.02\\text{Period}_t - 0.15\\text{Q2}_t\n- 0.1\\text{Q3}_t\n+ 0.32\\text{Q4}_t}\n\\]\nOn average, earnings in the same quarter increase by a factor of \\(\\exp(4\\times 0.02) = 1.083\\), or 8.3%, every year. Why?\n\nFor example: Q3 of 2019 (Period = 33): \\[\\hat Y_{32} = \\exp(3.56 + 0.02(33) + 0.1)\\]\n\n\nQ3 of 2020 (Period = 37): \\[\n\\begin{align}\n\\hat Y_{37} &= \\exp(3.56 + 0.02(37) + 0.1)\\\\\n&= \\exp(3.56 + 0.02(33 + 4)  + 0.1)\\\\\n&= \\color{darkred}{\\exp(3.56 + 0.02(33)  + 0.1)}\\times\\color{darkblue}{\\exp(0.02(4))}\\\\\n&= \\color{darkred}{\\hat Y_{32}} \\times \\color{darkblue}{1.083}\n\\end{align}\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#much-better",
    "href": "fall_sta235_2025/week_01/week_01.html#much-better",
    "title": "Data Science for Business Applications",
    "section": "Much better!",
    "text": "Much better!\n\nautoplot(apple_model2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#but-wait",
    "href": "fall_sta235_2025/week_01/week_01.html#but-wait",
    "title": "Data Science for Business Applications",
    "section": "But wait…",
    "text": "But wait…\nWe’re consistently under predicting recent quarters!\n\nIs there time dependence in our residuals?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence",
    "text": "Checking for independence\n\nPrediction errors for consecutive quarters are correlated!\n\\[\n\\mathrm{Cor}(\\text{residual}_t, \\text{residual}_{t-1}) = 0.64\n\\] (This is the lag 1 autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence (ACF)",
    "text": "Checking for independence (ACF)\n\nacf(residuals(apple_model2))\n\n\n\\[\n\\color{darkred}{\n\\mathrm{Cor}(\\text{residual}_t, \\text{residual}_{t-1}) = 0.64\n}\n\\] (This is the lag 1 autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-1",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-1",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence (ACF)",
    "text": "Checking for independence (ACF)\n\nacf(residuals(apple_model2))\n\n\n\\[\n\\color{darkorange}{\n\\mathrm{Cor}(\\text{residual}_t, \\text{residual}_{t-2}) = 0.26\n}\n\\]\n(This is the lag 2 autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-2",
    "href": "fall_sta235_2025/week_01/week_01.html#checking-for-independence-acf-2",
    "title": "Data Science for Business Applications",
    "section": "Checking for independence (ACF)",
    "text": "Checking for independence (ACF)\nThe autocorrelation function (ACF) plot shows autocorrelation at many lags\n\nacf(residuals(apple_model2))\n\n\nUnder independence, about 95% of the time any given lag will be within the blue. Look out for large lag 1 values in particular!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#improving-our-model",
    "href": "fall_sta235_2025/week_01/week_01.html#improving-our-model",
    "title": "Data Science for Business Applications",
    "section": "Improving our model",
    "text": "Improving our model\nLet’s try to address the bias in recent predictions first.\n\nWhat may have changed mid-late 2020?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks",
    "href": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks",
    "title": "Data Science for Business Applications",
    "section": "Modeling external shocks",
    "text": "Modeling external shocks\nAdding a COVID level-shift:\n\n# Define PostCOVID to be 1 on or after 2020 Q4\n# (i.e., if the year is 2021+ or if we are in 2020 Q4)\napple &lt;- apple %&gt;%\n  mutate(PostCOVID = ifelse(\n    Year &gt;= 2021 | (Year == 2020 & Quarter == \"Q4\"),\n    1, 0)\n  )\n\n\n\n...\n   Period Year Quarter Revenue PostCOVID\n35     35 2020      Q1   58.31         0\n36     36 2020      Q2   59.68         0\n37     37 2020      Q3   64.70         0\n38     38 2020      Q4  111.44         1\n39     39 2021      Q1   89.58         1\n40     40 2021      Q2   81.43         1\n41     41 2021      Q3   83.36         1\n..."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks-1",
    "href": "fall_sta235_2025/week_01/week_01.html#modeling-external-shocks-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling external shocks",
    "text": "Modeling external shocks\nAdding a COVID level-shift:\n\napple_model3 &lt;- lm(log(Revenue) ~ Period + Quarter\n  + PostCOVID, data = apple)\n\n...\n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.59676    0.03500  102.77 &lt; 0.0000000000000002 ***\nPeriod       0.01830    0.00126   14.50 &lt; 0.0000000000000002 ***\nQuarterQ2   -0.15036    0.03500   -4.30              0.00012 ***\nQuarterQ3   -0.09328    0.03511   -2.66              0.01148 *  \nQuarterQ4    0.32094    0.03500    9.17       0.000000000036 ***\nPostCOVID    0.16701    0.04390    3.80              0.00050 ***\n...\n\n\n\nconfint(apple_model3)\n\n               2.5 %   97.5 %\n(Intercept)  3.52591  3.66762\nPeriod       0.01575  0.02086\nQuarterQ2   -0.22121 -0.07951\nQuarterQ3   -0.16436 -0.02219\nQuarterQ4    0.25009  0.39179\nPostCOVID    0.07813  0.25588"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#new-model-fit",
    "href": "fall_sta235_2025/week_01/week_01.html#new-model-fit",
    "title": "Data Science for Business Applications",
    "section": "New model fit",
    "text": "New model fit"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#new-models-residuals",
    "href": "fall_sta235_2025/week_01/week_01.html#new-models-residuals",
    "title": "Data Science for Business Applications",
    "section": "New model’s residuals",
    "text": "New model’s residuals\nThe last few quarters look better:\n\nBut is the time dependence gone?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#nope",
    "href": "fall_sta235_2025/week_01/week_01.html#nope",
    "title": "Data Science for Business Applications",
    "section": "Nope!",
    "text": "Nope!\n\nacf(residuals(apple_model3))\n\n\nWe still have time dependence in our residuals!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#adding-a-lag-autoregression",
    "href": "fall_sta235_2025/week_01/week_01.html#adding-a-lag-autoregression",
    "title": "Data Science for Business Applications",
    "section": "Adding a lag (autoregression)",
    "text": "Adding a lag (autoregression)\nLet’s create and add a lag 1 term (\\(\\log(Y_{t-1})\\), the log of last quarter’s earnings):\n\napple &lt;- apple %&gt;% mutate(lag1 = lag(Revenue))\napple_model4 &lt;- lm(log(Revenue) ~ Period + Quarter\n  + PostCOVID + log(lag1), data = apple)\n\n...\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 1.787927   0.484023    3.69       0.00073 ***\nPeriod      0.008745   0.002516    3.48       0.00135 ** \nQuarterQ2   0.000922   0.048638    0.02       0.98497    \nQuarterQ3   0.147878   0.065352    2.26       0.02978 *  \nQuarterQ4   0.521035   0.060089    8.67 0.00000000024 ***\nPostCOVID   0.118981   0.039507    3.01       0.00473 ** \nlog(lag1)   0.468291   0.123846    3.78       0.00057 ***\n..."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#independence-over-time-is-satisfied",
    "href": "fall_sta235_2025/week_01/week_01.html#independence-over-time-is-satisfied",
    "title": "Data Science for Business Applications",
    "section": "Independence (over time) is satisfied!",
    "text": "Independence (over time) is satisfied!\n\nacf(residuals(apple_model4))\n\n\nFor independence to hold:\n\nThe low-lag autocorrelations (1,2,3) should be within the blue, and we should see no clear patterns.\nOccasional values outside the blue (e.g. lags 5,11, etc here) are expected about 5% of the time even under independence"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#forecasting",
    "href": "fall_sta235_2025/week_01/week_01.html#forecasting",
    "title": "Data Science for Business Applications",
    "section": "Forecasting",
    "text": "Forecasting\nOur last observation is from Q2 2022; how can we forecast Q3 2022?\n\ntail(apple, 3)\n\n   Period Time Year Quarter Revenue PostCOVID   lag1\n42     42 2022 2021      Q4  123.94         1  83.36\n43     43 2022 2022      Q1   97.28         1 123.94\n44     44 2022 2022      Q2   82.96         1  97.28\n\n\nHow would the last row change?\n\n\nIncrement Period by 1 (43 -&gt; 44)\nChange Quarter from Q2 to Q3\nPostCOVID = 1 (still)\nlag1 = 82.96 (Revenue for Q2 2022)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#forecasting-1",
    "href": "fall_sta235_2025/week_01/week_01.html#forecasting-1",
    "title": "Data Science for Business Applications",
    "section": "Forecasting",
    "text": "Forecasting\n\nlogpred &lt;- predict(apple_model4,\n  newdata = list(Period = 44, Quarter = \"Q3\",\n    PostCOVID = 1, lag1 = 82.96),\n  interval = \"predict\")\nexp(logpred)\n\n   fit   lwr   upr\n1 90.8 78.26 105.3\n\n\nForecasting tips:\n\nDon’t forget to transform back if you took a log!\nA prediction interval is the right measure of uncertainty, since there will be only one Q3 of 2022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#model-building-strategy",
    "href": "fall_sta235_2025/week_01/week_01.html#model-building-strategy",
    "title": "Data Science for Business Applications",
    "section": "Model building strategy",
    "text": "Model building strategy\n\nStart with a an additive or multiplicative model with trend and seasonal components. (Plot your data! If the seasonal variation increases or decreases over time you’ll want a multiplicative model.)\nExamine the usual diagnostic plots, and plot your residuals as a function of time. Do you need a (different) nonlinear time trend? A transformation of \\(Y\\)?\nCheck your residuals for time dependence If it’s present, is it explained by external factors you can model?\nIf time dependence in the residuals remains, add appropriate lag terms to your model, one at a time."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data",
    "title": "Data Science for Business Applications",
    "section": "Explore the data",
    "text": "Explore the data"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#section-1",
    "href": "fall_sta235_2025/week_01/week_01.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The correlation \\(r\\) between two variables \\(X\\) and \\(Y\\) measures the strength of the linear relationship between them. Correlation ranges from \\(-1\\) (perfect negative relationship) to \\(0\\) (no relationship) to \\(1\\) (perfect positive relationship)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#correlation",
    "href": "fall_sta235_2025/week_01/week_01.html#correlation",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\nThe correlation \\(r\\) between two variables \\(X\\) and \\(Y\\) measures the strength of the linear relationship between them. Correlation ranges from \\(-1\\) (perfect negative relationship) to \\(0\\) (no relationship) to \\(1\\) (perfect positive relationship)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#correlation-1",
    "href": "fall_sta235_2025/week_01/week_01.html#correlation-1",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#correlation-2",
    "href": "fall_sta235_2025/week_01/week_01.html#correlation-2",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\ncor(profs$eval, profs$beauty)\n\n[1] 0.1890391\n\n\n\nHow can we interpret this?\nThe $ sign accesses the variables in the data set profs.csv."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model",
    "href": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Let’s Build a Simple Regression Model",
    "text": "Let’s Build a Simple Regression Model\n\n\\[\n\\text{eval} = \\beta_0 + \\beta_1 \\cdot \\text{beauty} + \\epsilon\n\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are known as coefficients (standard notations)\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope associated with beauty\nThe term \\(\\epsilon\\) (epsilon) accounts for unobserved factors that are not included in this model"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpreting-the-model",
    "href": "fall_sta235_2025/week_01/week_01.html#interpreting-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the Model",
    "text": "Interpreting the Model\n\neval is the response variable (\\(Y\\)); beauty is the predictor variable (\\(X\\)).\nSimple regression uses the best fit line to give us a linear equation to predict \\(Y\\) from \\(X\\):\n\\[\n\\widehat{\\text{eval}} = 3.998 + 0.133 \\cdot \\text{beauty}\n\\]\nWe can predict the evaluation score for someone based on their beauty score just by plugging into the equation.\nWhat do the coefficients mean?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpretation",
    "href": "fall_sta235_2025/week_01/week_01.html#interpretation",
    "title": "Data Science for Business Applications",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nIntercept\n\nWhen the beauty score is zero, the expected evaluation is 3.99 (almost 4).\n\nHere, beauty = 0 represents an “average beauty”.\nImportant, the intercept is evaluated always when the predictor variable is zero.\n\nSlope for Beauty\n\nFor every one-unit increase in the beauty score, there is a 0.133 increase in the professor’s expected evaluation.\n\nIn this context, “expected” refers to the average evaluation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#statistical-significance-of-the-model",
    "href": "fall_sta235_2025/week_01/week_01.html#statistical-significance-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Statistical Significance of the Model",
    "text": "Statistical Significance of the Model\n\nThe population regression line (the best fit line in the population) is \\(Y = \\beta_0 + \\beta_1 X\\) (we can’t know this).\nOur regression equation is the best fit line in the sample, or \\(\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\\) (this is what we get from our sample data).\nThe sample intercept and slope \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are our best estimates for the population intercept and slope \\(\\beta_0\\) and \\(\\beta_1\\).\nBut we need to get a sense of how close \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are to \\(\\beta_0\\) and \\(\\beta_1\\)!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#p-values",
    "href": "fall_sta235_2025/week_01/week_01.html#p-values",
    "title": "Data Science for Business Applications",
    "section": "P-values",
    "text": "P-values\n\n\nFor this model, the p-values associated with the coefficients, slope and intercept are close to zero:\n\n\n\n\n\nTerm\np-value - Pr(&gt;|t|)\nSignificance\n\n\n\n\nIntercept\n0.0000000000000002\n***\n\n\nBeauty\n0.0000425\n***\n\n\n\n\n\nIn this case, there’s evidence that beauty has an impact on a professor’s evaluation at a populational level.\nThus, we can conclude that the effect of beauty is statistically significant in relation to the professor’s evaluation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#confidence-intervals",
    "href": "fall_sta235_2025/week_01/week_01.html#confidence-intervals",
    "title": "Data Science for Business Applications",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nLet’s get confidence intervals for the slope and intercept to get a sense of the uncertainty in our estimates:\n\nconfint(model)\n\n                 2.5 %    97.5 %\n(Intercept) 3.94845765 4.0480866\nbeauty      0.06976869 0.1962342\n\n\n\nSlope: We are 95% confident that the incremental impact of each additional beauty point is between \\(0.07\\) and \\(0.20\\) student evaluation points.\nIntercept: We are 95% confident that the average student evaluation score for average-looking professors (beauty = 0) is between \\(3.95\\) and \\(4.05\\).\nRule of thumb: If zero is inside the CI, the effect is not statistically significant.\nP-values and confidence intervals (CIs) are connected. If the p-value is greater than 0.05, it is likely that zero will be included within the confidence interval."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for",
    "href": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for",
    "title": "Data Science for Business Applications",
    "section": "Rule of Thumb for",
    "text": "Rule of Thumb for"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for-p-values",
    "href": "fall_sta235_2025/week_01/week_01.html#rule-of-thumb-for-p-values",
    "title": "Data Science for Business Applications",
    "section": "Rule of Thumb for P-values",
    "text": "Rule of Thumb for P-values\n\n\nIf the p-value is smaller than 0.05, we can conclude that the effect is statistically significant.\nOtherwise, if the p-value is greater than 0.05, we conclude that the effect from the predictor is not statistically significant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions",
    "href": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\n\nInterval for a single prediction:\n\nWe are 95% confident that a single professor with a beauty score of 1 will get rated between 3.06 and 5.21.\n\n\npredict(model, list(beauty=1), interval=\"prediction\")\n\n       fit      lwr      upr\n1 4.131274 3.056375 5.206172\n\n\nInterval for an average prediction:\n\nWe are 95% confident that the average rating of all professors with beauty scores of 1 will be between 4.05 and 4.21.\n\n\npredict(model, list(beauty=1), interval=\"confidence\")\n\n       fit      lwr      upr\n1 4.131274 4.050776 4.211771"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions-1",
    "href": "fall_sta235_2025/week_01/week_01.html#confidence-intervals-for-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Confidence intervals for predictions",
    "text": "Confidence intervals for predictions\n\n\npredict(model, list(beauty=1), interval=\"confidence\")\n\n       fit      lwr      upr\n1 4.131274 4.050776 4.211771\n\n\n\nWe are 95% confident that the will be between 4.05 and 4.21."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#residuals-and-r-squared",
    "href": "fall_sta235_2025/week_01/week_01.html#residuals-and-r-squared",
    "title": "Data Science for Business Applications",
    "section": "Residuals and R-squared",
    "text": "Residuals and R-squared\n\n\nEach instructor has a residual: the difference between their actual and predicted scores (the prediction error)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#residual-standard-error",
    "href": "fall_sta235_2025/week_01/week_01.html#residual-standard-error",
    "title": "Data Science for Business Applications",
    "section": "Residual standard error",
    "text": "Residual standard error\n\n\nThe residual standard error is in the same units as the response variable:\n\nIn this case the RSE is: Residual standard error: 0.5455\nAll predictions made by this model will, on average, differ from the true values by approximately 0.5455, which represents one standard deviation of the residuals.\n\nWe can even get the 95% prediction interval for a single (beauty = 1) prediction as\n\nlower bound: \\(3.998 + 0.133 \\cdot 1 - 2\\times \\text{RSE}\\)\nupper bound: \\(3.998 + 0.133 \\cdot 1 + 2\\times \\text{RSE}\\)\n\n\n\n# lower bound \n3.998 + 0.133 - 2*0.5455\n\n[1] 3.04\n\n# upper bound\n3.998 + 0.133 + 2*0.5455\n\n[1] 5.222"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#r-squared-r2",
    "href": "fall_sta235_2025/week_01/week_01.html#r-squared-r2",
    "title": "Data Science for Business Applications",
    "section": "R-squared (\\(R^2\\))",
    "text": "R-squared (\\(R^2\\))\n\n\nThe \\(R^2\\) provides an understanding of the “fit” of the model in relation to the data.\n\nIf the \\(R^2\\) is close to one, the model has a good fit.\nIf the \\(R^2\\) is close to zero, the model does not provide a good fit for the data.\nMultiple R-squared: 0.03574\nThis indicates not a great fit.\n\nImportant interpretation:\n\nThe \\(R^2\\) represents the percentage of variation in the response variable that can be explained by the predictor.\nFor this model, 3.6% of the variation in evaluation scores can be explained by the beauty variable alone, while the remaining 96.4% is attributed to other unobserved factors."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#adding-more-predictors",
    "href": "fall_sta235_2025/week_01/week_01.html#adding-more-predictors",
    "title": "Data Science for Business Applications",
    "section": "Adding more predictors",
    "text": "Adding more predictors\n\n\nIs the professor’s evaluation explained only by it’s beauty or there might be other factor affecting their evalution?\nLet’s check if the variable age might help us better explain the relationship between beauty and evaluation.\nWe update our model as:\n\n\\[\n\\widehat{\\text{eval}} = \\beta_0 + \\beta_1 \\cdot \\text{beauty} + \\beta_2 \\cdot \\text{age} + \\epsilon\n\\] - We now have a multiple regression model. - Both beauty and age are numerical variables. - The term \\(\\epsilon\\) (epsilon) accounts for unobserved factors that are not included in this model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#multiple-regression-model",
    "href": "fall_sta235_2025/week_01/week_01.html#multiple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression model",
    "text": "Multiple regression model\n\n\n# Build a simple regression model\nmodel &lt;- lm(eval ~ beauty + age, data = profs)\nsummary(model)\n\n\nCall:\nlm(formula = eval ~ beauty + age, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80242 -0.36514  0.07407  0.39913  1.10206 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 3.9844013  0.1337296  29.794 &lt; 0.0000000000000002 ***\nbeauty      0.1340634  0.0337441   3.973            0.0000824 ***\nage         0.0002868  0.0027148   0.106                0.916    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.546 on 460 degrees of freedom\nMultiple R-squared:  0.03576,   Adjusted R-squared:  0.03157 \nF-statistic:  8.53 on 2 and 460 DF,  p-value: 0.0002305\n\n\n\nHow can we interpret this model?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#interpretation-of-model-coefficients",
    "href": "fall_sta235_2025/week_01/week_01.html#interpretation-of-model-coefficients",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of Model Coefficients",
    "text": "Interpretation of Model Coefficients\n\n\nIntercept\n\nWhen both the beauty score and age are zero, the expected evaluation is 3.98 (almost 4).\n\nHere, beauty = 0 represents an “average beauty,” and age = 0 is not meaningful in this context but is part of the model.\n\nImportant: The intercept is evaluated when all predictor variables (beauty and age) are zero.\n\nSlope for Beauty\n\nFor every one-unit increase in the beauty score, there is a 0.134 increase in the professor’s expected evaluation, holding age constant.\n\nSlope for Age\n\nFor every one-unit increase in age, there is a 0.0003 increase in the professor’s expected evaluation, holding beauty constant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#statistical-significance",
    "href": "fall_sta235_2025/week_01/week_01.html#statistical-significance",
    "title": "Data Science for Business Applications",
    "section": "Statistical significance",
    "text": "Statistical significance\n\n\nFor this model, the p-values associated with the coefficients (intercept, beauty, and age) are as follows:\n\n\n\n\nTerm\np-value - Pr(&gt;|t|)\nSignificance\n\n\n\n\nIntercept\n&lt; 0.0000000000000002\n***\n\n\nBeauty\n0.0000824\n***\n\n\nAge\n0.916\n\n\n\n\n\nIn this model, there is strong evidence that beauty has an impact on a professor’s evaluation at the population level, as its p-value is very close to zero.\nHowever, the p-value for age (0.916 &gt; 0.05) indicates that there is no statistically significant relationship between age and a professor’s evaluation.\nThus, we can conclude that the effect of beauty is statistically significant in relation to a professor’s evaluation, while the effect of age is not statistically significant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#rse-and-r2",
    "href": "fall_sta235_2025/week_01/week_01.html#rse-and-r2",
    "title": "Data Science for Business Applications",
    "section": "RSE and \\(R^2\\)",
    "text": "RSE and \\(R^2\\)\n\n\nThere are no significant changes in relation to the RSE and \\(R^2\\) in relation to the previous model.\nThis means that the quality of the fit and the accuracy of the predictions will be nearly identical to those of the previous model.\nFor this model, 3.6% of the variation in evaluation scores can be explained by the beauty and age variables alone, while the remaining 96.4% is attributed to other unobserved factors.\nWhat about predictions and confidence intervals?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#whats-the-impact-of-gender-on-student-evaluations",
    "href": "fall_sta235_2025/week_01/week_01.html#whats-the-impact-of-gender-on-student-evaluations",
    "title": "Data Science for Business Applications",
    "section": "What’s the Impact of Gender on Student Evaluations?",
    "text": "What’s the Impact of Gender on Student Evaluations?\n\n\nDo you see a difference between men (blue) and women (red)?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#for-for-the-weekend",
    "href": "fall_sta235_2025/week_01/week_01.html#for-for-the-weekend",
    "title": "Data Science for Business Applications",
    "section": "For for the Weekend",
    "text": "For for the Weekend\n\nRead the syllabus.\nDo the first homework assignment in Canvas (covering today’s material).\nDo the first pre-class assignment in Perusall (to prepare for next week’s class)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model-in-rstudio",
    "href": "fall_sta235_2025/week_01/week_01.html#lets-build-a-simple-regression-model-in-rstudio",
    "title": "Data Science for Business Applications",
    "section": "Let’s Build a Simple Regression Model in Rstudio",
    "text": "Let’s Build a Simple Regression Model in Rstudio\n\n\n# Build a simple regression model\nmodel &lt;- lm(eval ~ beauty, data = profs)\nsummary(model)\n\n\nCall:\nlm(formula = eval ~ beauty, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.99827    0.02535 157.727 &lt; 0.0000000000000002 ***\nbeauty       0.13300    0.03218   4.133            0.0000425 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 0.00004247"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-eval",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-eval",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: eval",
    "text": "Explore the data: eval"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_01.html#explore-the-data-beauty",
    "href": "fall_sta235_2025/week_01/week_01.html#explore-the-data-beauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: beauty",
    "text": "Explore the data: beauty"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#whats-the-impact-of-gender-on-student-evaluations",
    "href": "fall_sta235_2025/week_01/week_02.html#whats-the-impact-of-gender-on-student-evaluations",
    "title": "Data Science for Business Applications",
    "section": "What’s the impact of gender on student evaluations?",
    "text": "What’s the impact of gender on student evaluations?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model",
    "href": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Incorporating gender into the multiple regression model",
    "text": "Incorporating gender into the multiple regression model\n\nThe model is given by: \\[\n\\texttt{eval} = \\beta_0 + \\beta_1 \\cdot \\texttt{beauty} + \\beta_2 \\cdot \\texttt{gender} + \\epsilon\n\\] Where we have that:\n\n\\(\\texttt{eval}\\): is the response variable - (numerical)\n\\(\\texttt{beauty}\\): is a predictor - (numerical)\n\\(\\texttt{gender}\\): is a predictor - (categorical) - two groups:\n\nfemale\nmale"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model-1",
    "href": "fall_sta235_2025/week_01/week_02.html#incorporating-gender-into-the-multiple-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Incorporating gender into the multiple regression model",
    "text": "Incorporating gender into the multiple regression model\n\n\nGender is a categorical variable (male or female in this data set) so we can’t use it as-is as a predictor.\nIdea: Recode gender into the quantitative variable 1 = male, 0 = female. In practice this choice is arbitrary.\nR does this for us!\nIf you put a categorical variable into a model, R will alphabetically select the group that will associated with “0”, and the next to “1”."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model",
    "href": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Run the Regression Model",
    "text": "Run the Regression Model\n\n\noptions(scipen = 999)\nmodel1 &lt;- lm(eval ~ beauty + gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty + gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.88377    0.03866  100.47 &lt; 0.0000000000000002 ***\nbeauty       0.14859    0.03195    4.65           0.00000434 ***\ngendermale   0.19781    0.05098    3.88              0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 0.0000001407"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#regression-a-model-using-gender",
    "href": "fall_sta235_2025/week_01/week_02.html#regression-a-model-using-gender",
    "title": "Data Science for Business Applications",
    "section": "Regression a model using gender",
    "text": "Regression a model using gender\n\nA multiple regression predicting evaluation score from beauty and gender effectively fits two parallel regression lines:"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#is-there-a-generation-gap",
    "href": "fall_sta235_2025/week_01/week_02.html#is-there-a-generation-gap",
    "title": "Data Science for Business Applications",
    "section": "Is there a generation gap?",
    "text": "Is there a generation gap?\n\n\nThe variable generation is either silent (born before 1945), boomer (born 1945-1964), or genx (born after 1965).\nIs generation a significant predictor of evaluations above and beyond gender and beauty?\nTo answer this, we need a model that includes as predictors all of gender, beauty, and generation.\nBut we can’t just create a variable that is 0 for the silent generation, 1 for baby boomers, and 2 for gen X—why not?\nSolution is to pick a “reference category” and create dummy variables for the other categories."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#ok-boomer",
    "href": "fall_sta235_2025/week_01/week_02.html#ok-boomer",
    "title": "Data Science for Business Applications",
    "section": "OK boomer",
    "text": "OK boomer\n\nLet’s arbitrarily pick boomers as a reference category:\n\n\n\nCategory\ngenx\nsilent\n\n\n\n\nBoomers\n0\n0\n\n\nGen Xers\n1\n0\n\n\nSilent Gens\n0\n1\n\n\n\n\nR will do this automatically when you add a categorical variable with 3+ categories to a regression (it will arbitrarily pick a reference category)!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model-1",
    "href": "fall_sta235_2025/week_01/week_02.html#run-the-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Run the Regression Model",
    "text": "Run the Regression Model\n\n\nmodel2 &lt;- lm(eval ~ beauty + generation, data=profs)\nsummary(model2)\n\n\nCall:\nlm(formula = eval ~ beauty + generation, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.82584 -0.36581  0.06317  0.42027  1.07809 \n\nCoefficients:\n                 Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       4.02537    0.03201 125.742 &lt; 0.0000000000000002 ***\nbeauty            0.13491    0.03360   4.015            0.0000694 ***\ngenerationgenx   -0.06807    0.06181  -1.101                0.271    \ngenerationsilent -0.08225    0.07889  -1.043                0.298    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 459 degrees of freedom\nMultiple R-squared:  0.03983,   Adjusted R-squared:  0.03355 \nF-statistic: 6.346 on 3 and 459 DF,  p-value: 0.0003192"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_02.html#analysis",
    "href": "fall_sta235_2025/week_01/week_02.html#analysis",
    "title": "Data Science for Business Applications",
    "section": "Analysis",
    "text": "Analysis\n\nAll else equal (i.e., among professors of the same gender and beauty):\n\nGen X professors are predicted to get scores that are 0.07 points below those of boomers.\nSilent gen professors are predicted to get scores that are 0.08 points below those of boomers.\nOnly the boomer/silent generation difference is statistically significant; Gen X professors are not significantly different than boomers.\nIn other words: age only seems to matter if you are really old."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#section",
    "href": "fall_sta235_2025/week_01/week_03.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Last week, we predicted evaluation score from beauty and gender, and the model forced the lines to be parallel:\n\n\n\n\n\n\n\n\n\n\n\nWhat if we could build a more flexible model without forcing the lines to be parallel?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#what-is-an-interaction",
    "href": "fall_sta235_2025/week_01/week_03.html#what-is-an-interaction",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "An interaction is an additional term in a regression model that allows the slope of one variable to depend on the value of another."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#does-beauty-matter-more-for-men-or-for-women",
    "href": "fall_sta235_2025/week_01/week_03.html#does-beauty-matter-more-for-men-or-for-women",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "A categorical variable and a quantitative variable\nWe found that for the same level of attractiveness, male professors tend to get higher evaluation scores than female professors\nBut what if the effect of beauty depends on gender (is different for men vs women)?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The idea is to add a new variable that is itself the product of the two variables:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1(\\text{gender}) + \\hat\\beta_2(\\text{beauty}) + \\hat\\beta_3(\\text{gender})(\\text{beauty})\n\\]\n\nFor female professors, male = 0, so the \\(\\beta_1\\) and \\(\\beta_3\\) terms cancel out:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_2(\\text{beauty})\n\\]\n\nFor male professors, male = 1, so we get both a different intercept and a different slope for beauty:\n\n\\[\n\\hat Y = (\\hat\\beta_0 + \\hat\\beta_1) + (\\hat\\beta_2+\\hat\\beta_3)(\\text{beauty})\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender-1",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-beauty-and-gender-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "model1 &lt;- lm(eval ~ beauty*gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty * gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83820 -0.37387  0.04551  0.39876  1.06764 \n\nCoefficients:\n                  Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)        3.89085    0.03878 100.337 &lt; 0.0000000000000002 ***\nbeauty             0.08762    0.04706   1.862             0.063294 .  \ngendermale         0.19510    0.05089   3.834             0.000144 ***\nbeauty:gendermale  0.11266    0.06398   1.761             0.078910 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5361 on 459 degrees of freedom\nMultiple R-squared:  0.07256,   Adjusted R-squared:  0.0665 \nF-statistic: 11.97 on 3 and 459 DF,  p-value: 0.000000147"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#section-1",
    "href": "fall_sta235_2025/week_01/week_03.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Beauty seems to matter more for men than for women!\nThe gender gap is largest for good-looking professors"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#main-effects-and-interaction-effects",
    "href": "fall_sta235_2025/week_01/week_03.html#main-effects-and-interaction-effects",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "In a model with an interaction term \\(X_1X_2\\), you must also keep the main effects: the variables that are being interacted together.\nThe main effect of \\(X_1\\) represents the predicted increase in \\(Y\\) for a 1-unit change in \\(X_1\\), holding \\(X_2\\) constant at zero.\n\nThe main effect gendermale (0.20) represents the predicted advantage, but only for an average-looking professor (beauty = 0).\n\nThe main effect beauty (0.09) represents the predicted improvement in evaluation scores for each additional beauty point, but only among women (gendermale = 0).\n\nYou can also include other variables in the model that are not being interacted!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-1",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-2",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-2",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#regression-model",
    "href": "fall_sta235_2025/week_01/week_03.html#regression-model",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Let’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model",
    "href": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "How do we interpret this model?\nintercept (baseline), luxury = \\(\"no\"\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\"no\"\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\"yes\"\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model-1",
    "href": "fall_sta235_2025/week_01/week_03.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We also have the following interpretation:\nluxury = \\(\\text{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (0) - 0.16\\times \\text{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\text{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (1) - 0.16\\times \\text{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\text{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\text{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#section-2",
    "href": "fall_sta235_2025/week_01/week_03.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#when-should-you-use-interactions-in-a-model",
    "href": "fall_sta235_2025/week_01/week_03.html#when-should-you-use-interactions-in-a-model",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Interactions make a model more complex to analyze and explain, so it’s only worth doing so when you get a substantial bump in \\(R^2\\) by including the interaction.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions-neq-correlations-between-predictor-variables",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions-neq-correlations-between-predictor-variables",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Instead, interactions let us model a situation where the relationship of one predictor variable and \\(Y\\) is different depending on the value of another \\(X\\) variable:\n\nHow much attractiveness matters for student evaluation scores depends on gender."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#youll-experiment-with-this-in-the-lab",
    "href": "fall_sta235_2025/week_01/week_03.html#youll-experiment-with-this-in-the-lab",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Two categorical variables\nTwo numerical variables"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html",
    "href": "fall_sta235_2025/week_01/week_03.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Last week, we predicted evaluation score from beauty and gender, and the model forced the lines to be parallel:\n\n\n\n\n\n\n\n\n\n\n\nWhat if we could build a more flexible model without forcing the lines to be parallel?\n\n\n\n\n\nAn interaction is an additional term in a regression model that allows the slope of one variable to depend on the value of another.\n\n\n\n\n\nA categorical variable and a quantitative variable\nWe found that for the same level of attractiveness, male professors tend to get higher evaluation scores than female professors\nBut what if the effect of beauty depends on gender (is different for men vs women)?\n\n\n\n\n\nThe idea is to add a new variable that is itself the product of the two variables:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1(\\text{gender}) + \\hat\\beta_2(\\text{beauty}) + \\hat\\beta_3(\\text{gender})(\\text{beauty})\n\\]\n\nFor female professors, male = 0, so the \\(\\beta_1\\) and \\(\\beta_3\\) terms cancel out:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_2(\\text{beauty})\n\\]\n\nFor male professors, male = 1, so we get both a different intercept and a different slope for beauty:\n\n\\[\n\\hat Y = (\\hat\\beta_0 + \\hat\\beta_1) + (\\hat\\beta_2+\\hat\\beta_3)(\\text{beauty})\n\\]\n\n\n\n\n\nmodel1 &lt;- lm(eval ~ beauty*gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty * gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83820 -0.37387  0.04551  0.39876  1.06764 \n\nCoefficients:\n                  Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)        3.89085    0.03878 100.337 &lt; 0.0000000000000002 ***\nbeauty             0.08762    0.04706   1.862             0.063294 .  \ngendermale         0.19510    0.05089   3.834             0.000144 ***\nbeauty:gendermale  0.11266    0.06398   1.761             0.078910 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5361 on 459 degrees of freedom\nMultiple R-squared:  0.07256,   Adjusted R-squared:  0.0665 \nF-statistic: 11.97 on 3 and 459 DF,  p-value: 0.000000147\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeauty seems to matter more for men than for women!\nThe gender gap is largest for good-looking professors\n\n\n\n\n\nIn a model with an interaction term \\(X_1X_2\\), you must also keep the main effects: the variables that are being interacted together.\nThe main effect of \\(X_1\\) represents the predicted increase in \\(Y\\) for a 1-unit change in \\(X_1\\), holding \\(X_2\\) constant at zero.\n\nThe main effect gendermale (0.20) represents the predicted advantage, but only for an average-looking professor (beauty = 0).\n\nThe main effect beauty (0.09) represents the predicted improvement in evaluation scores for each additional beauty point, but only among women (gendermale = 0).\n\nYou can also include other variables in the model that are not being interacted!\n\n\n\n\n\n\nIs there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction.\n\n\n\n\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]\n\n\n\n\n\nLet’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\nHow do we interpret this model?\nintercept (baseline), luxury = \\(\"no\"\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\"no\"\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\"yes\"\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars.\n\n\n\n\n\nWe also have the following interpretation:\nluxury = \\(\\text{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (0) - 0.16\\times \\text{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\text{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (1) - 0.16\\times \\text{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\text{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\text{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope.\n\n\n\n\n\n\n\nThe lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions make a model more complex to analyze and explain, so it’s only worth doing so when you get a substantial bump in \\(R^2\\) by including the interaction.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them!\n\n\n\n\n\nInstead, interactions let us model a situation where the relationship of one predictor variable and \\(Y\\) is different depending on the value of another \\(X\\) variable:\n\nHow much attractiveness matters for student evaluation scores depends on gender.\n\n\n\n\n\n\nTwo categorical variables\nTwo numerical variables"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_03.html#interactions---luxury-cars",
    "href": "fall_sta235_2025/week_01/week_03.html#interactions---luxury-cars",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Is there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Linear models are useful:\n\nPrediction - given a new observations\nExplanatory power- which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems-1",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-assumptions-and-potential-problems-1",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#multiple-regression-assumptions",
    "href": "fall_sta235_2025/week_01/week_04.html#multiple-regression-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression assumptions",
    "text": "Multiple regression assumptions\nWe need four things to be true for regression to work properly:\n\nLinearity: \\(Y\\) is a linear function of the \\(X\\)’s (except for the prediction errors).\nIndependence: The prediction errors are independent.\nNormality: The prediction errors are normally distributed.\nEqual Variance: The variance of \\(Y\\) is the same for any value of \\(X\\) (“homoscedasticity”)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity",
    "text": "Non-Linearity\n\nWhat we would expect to observe in a regression where there is a linear relation?\n\n\nlibrary(tidyverse)\nggplot(linear_data, aes(x=X, y=Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#residuals",
    "href": "fall_sta235_2025/week_01/week_04.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nLet’s plot the residuals \\(r_i\\), such that \\[r_i = y_i − \\widehat{y}_i\\] where \\(\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\) vs \\(x_i\\)\nResiduals are basically the distances between the model and the points\nHopefully identify non-linear relationships\nWe are looking for patterns or trends in the residuals"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#residuals-1",
    "href": "fall_sta235_2025/week_01/week_04.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nPlot of the residuals\nHow can these residuals be useful for us?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plots",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plots",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plots",
    "text": "Regression diagnostic plots\nWe’ll use regression diagnostic plots to help us evaluate some of the assumptions.\nThe residuals vs fitted graph plots:\n\nResiduals on the \\(Y\\)-axis\nFitted values (predicted \\(Y\\) values) on the \\(X\\)-axis\n\nThis graph effectively subtracts out the linear trend between \\(Y\\) and the \\(X\\)’s, so we want to see no trend left in this graph."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check non-linearity we focus on the Residual vs. Fitted plot\n\n\nlibrary(ggfortify)\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-1",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nFrom the Residual vs. Fitted plot, we can observe that since the residuals are evenly distributed around zero in relation to the fitted values, we have that the linear regression model is a good fit for this data.\nThis means that we are learning the linear representation contained in this data."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nWhat we would expect to observe if the relation is non linear?\n\n\nggplot(nonlinear_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-1",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-1",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nLet’s look at the residuals for this model\n\n\n\n\n\n\n\n\n\n\n\nLet’s check the residual plot"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-2",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-2",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nlm2 = lm(Y ~ X, data = nonlinear_data)\nautoplot(lm2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-3",
    "href": "fall_sta235_2025/week_01/week_04.html#non-linearity-example-3",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nFrom the Residual vs. Fitted, we can observe that the residuals are not evenly distributed around zero.\nThis indicates that for lower and higher values of \\(x_i\\) our model is overpredicting and underpredicting in the mid values.\nWhat are the implications in this case?\nWorse predictions"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#independence",
    "href": "fall_sta235_2025/week_01/week_04.html#independence",
    "title": "Data Science for Business Applications",
    "section": "Independence",
    "text": "Independence\n\nIndependence means that knowing the prediction error for one observation doesn’t tell you anything about the error for another observation\nData collected over time are usually not independent\nWe can’t use regression diagnostics to decide the independence\nWe have to measure the autocorrelation of the residuals\nWe’ll get back to autocorrelation when we discuss Time Series models"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-assumption",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-assumption",
    "title": "Data Science for Business Applications",
    "section": "Normality assumption",
    "text": "Normality assumption\n\nWhen we’ve been interpreting residual standard error (RSE) , we’ve used the following interpretation:\n95% of our predictions will be accurate to within plus or minus \\(2\\times RSE\\).\nIn order for this to be true, the residuals have to be Normally distributed"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nWe can check the distribution of the residuals\n\n\nlinear_data = linear_data %&gt;% \n  mutate(resid = residuals(lm1))\n\nggplot(linear_data, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 0.2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example-1",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example-1",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nBut how can we judge if the residuals follows a Normal distribution?\nThe key is to look at the Normal Q-Q plot, which compares the distribution of our residuals to a perfect Normal distribution.\nIf the dots line up along an (approximately) straight line, then the Normality assumption is satisfied."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-2",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-2",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check for Normality we focus on the Normal Q-Q plot\n\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)\n\n\n\n\n\n\n\n\n\nIn this case the normality assumptions seem to be met"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example-2",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example-2",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nLet’s look at different data.\nIn this case the data has non Normal errors."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#normality-example-3",
    "href": "fall_sta235_2025/week_01/week_04.html#normality-example-3",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\n\nHistogram of the residuals (right skewed)\n\n\n\nlm3 = lm(Y ~ X, data = non_normal)\n\nnon_normal = non_normal %&gt;% \n  mutate(resid = residuals(lm3))\n\nggplot(non_normal, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 1)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-3",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-3",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nautoplot(lm3)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot",
    "href": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Normal Q-Q plot, we can observe that the residuals are not following the line that indicates the Normal quantiles\nThis means that our model results in non-normal residuals\nThis affects statistical tests, and confidence intervals"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#equal-variance",
    "href": "fall_sta235_2025/week_01/week_04.html#equal-variance",
    "title": "Data Science for Business Applications",
    "section": "Equal variance",
    "text": "Equal variance\n\nEqual variance is also known as “homoscedasticity”\nThe variance of \\(Y\\) should be about the same at any \\(X\\) value (or combination of values for the \\(X\\)’s).\nIn other words, the vertical spread of the points should be the same anywhere along the \\(X\\)-axis.\nIf there’s no equal variance then we might have heteroskedasticity.\nLower precision, estimates are further from the correct population value."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#equal-variance-example",
    "href": "fall_sta235_2025/week_01/week_04.html#equal-variance-example",
    "title": "Data Science for Business Applications",
    "section": "Equal variance example",
    "text": "Equal variance example\n\nThe vertical spread of the points is larger along the right side of the graph\n\n\nggplot(heter_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-4",
    "href": "fall_sta235_2025/week_01/week_04.html#regression-diagnostic-plot-4",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\n\nTo check for homoscidacity we focus on the Scale-Location plot\n\n\n\nlm4 = lm(Y ~ X, data = heter_data)\nautoplot(lm4)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot-1",
    "href": "fall_sta235_2025/week_01/week_04.html#interpretation-of-the-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Sacle-Location plot, we can observe that the residuals have a fan shape, indicating that there is heteroscedacity in the data.\nThis resulted in lower precision; thus, estimates are further from the correct population value."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#influential-observations",
    "href": "fall_sta235_2025/week_01/week_04.html#influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Influential observations",
    "text": "Influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\n\n\n\n\n\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#diagnostics-plot",
    "href": "fall_sta235_2025/week_01/week_04.html#diagnostics-plot",
    "title": "Data Science for Business Applications",
    "section": "Diagnostics Plot",
    "text": "Diagnostics Plot\n\nWe can observe the point with high residual on the Residual vs. Leverage plot\n\n\nlm5 = lm(Y ~ X, data = outlier_residual)\nautoplot(lm5)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#high-leverage",
    "href": "fall_sta235_2025/week_01/week_04.html#high-leverage",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can also have points with high leverage - when a point in \\(X\\) is distant from the average on \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\nThis point has low residual but high leverage. RSE = 0.2956"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#high-leverage-1",
    "href": "fall_sta235_2025/week_01/week_04.html#high-leverage-1",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can observe the point with high leverage on the Residual vs. Leverage plot\n\n\nlm6 = lm(Y ~ X, data = outlier_leverage)\nautoplot(lm6)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence",
    "href": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nPoints with high leverage and high residuals are known as influential points\n\n\n\n\n\n\n\n\n\n\n\nThis point has high residual but high leverage. RSE = 0.8281"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-1",
    "href": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-1",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWe can observe the point with high influence on the Residual vs. Leverage plot\n\n\nlm7 = lm(Y ~ X, data = outlier_influence)\nautoplot(lm7)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-2",
    "href": "fall_sta235_2025/week_01/week_04.html#points-with-high-influence-2",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWhen a case has a very unusual \\(X\\) value, it has leverage — the potential to have a big impact on the regression line\nIf the case is in line with the overall trend of the regression line, it won’t be a problem\nBut when that case also has a Y (high residual) value that is out of line\nWe need both a large residual and high leverage for an observation to be influential\nWe should be worried about these points\nThey affect the coefficents and predictions"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nThe data set utilities contains information on the utility bills for a house in Minnesota. We’ll focus on two variables:\n\ndailyspend is the average amount of money spent on utilities (e.g. heating) for each day during the month\ntemp is the average temperature outside for that month"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-1",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-1",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWhat problems do you see here?\n\nlibrary(tidyverse)\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point()"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-2",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-2",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm1 &lt;- lm(dailyspend ~ temp, data=utilities) \nsummary(lm1)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59 &lt;0.0000000000000002 ***\ntemp        -0.096432   0.003911  -24.66 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nLet’s interpret this relation\nFor one unit increase in temperature (Fahrenheit), there will be a 7-cent decrease in spending"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-3",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-3",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlibrary(ggfortify)\nautoplot(lm1)\n\n\n\nLinearity and homoscedasticity are violated"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-4",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-4",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe’ll use polynomial regression to fix problems\nIf a polynomial curve (e.g., quadratic, cubic, etc) would be a better fit for the data than a line, we can fit a curve to the data.\nThe way we do this is by adding \\(X^2\\) to the model as a second predictor variable.\nThis can “fix” the linearity problem because now \\(Y\\) is a linear function of \\(X\\) and \\(X^2\\), resulting in: \\[\nY = \\beta_0 + \\beta_1\\cdot X + \\beta\\cdot X^2 + e\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-5",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-5",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe add the term I(temp^2) in the regression equation:\n\n\nlm2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities) \nsummary(lm2)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239 &lt; 0.0000000000000002 ***\ntemp        -0.2115553  0.0191046 -11.074 &lt; 0.0000000000000002 ***\nI(temp^2)    0.0012476  0.0002037   6.124         0.0000000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nWe have that the new term is evaluated as an extra variable."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-6",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-6",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWriting out the equation: \\[\n\\widehat{\\texttt{dailyspend}} = 9.4723 −0.2116\\cdot \\texttt{temp} + 0.0012\\cdot \\texttt{temp}^2\n\\] The effect of the extra variable is statistically significant:\n\nconfint(lm2)\n\n                    2.5 %       97.5 %\n(Intercept)  8.6981381712 10.246438869\ntemp        -0.2494014032 -0.173709160\nI(temp^2)    0.0008440041  0.001651114\n\n\n\nThe residual standard error of the polynomial model is \\(\\texttt{0.75}\\).\nThe residual standard error of the linear model is \\(\\texttt{0.87}\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-7",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-7",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nAdding an \\(X^2\\) term fits a parabola to the data (orange line)\n\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm1)), col = \"lightblue\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-8",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-8",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nIt solves the linearity problem\n\nautoplot(lm2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-9",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-9",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWhat about a higher-order polynomial?\nWe could fit a cubic curve by adding an \\(X^3\\) term\nMaking the polynomial higher order will decrease the RSE\nWhy not go nuts and fit a 7th degree polynomial?\n\n\n\n\nDegree\nname\nRSE\n\n\n\n\n1\nlinear\n0.866\n\n\n2\nquadratic\n0.754\n\n\n3\ncubic\n0.755\n\n\n4\nquartic\n0.755\n\n\n5\nquintic\n0.758\n\n\n6\n\n0.761\n\n\n7\n\n0.761"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#polynomial-models-10",
    "href": "fall_sta235_2025/week_01/week_05.html#polynomial-models-10",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) +\nI(temp^5) + I(temp^6) + I(temp^7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#building-polynomial-models",
    "href": "fall_sta235_2025/week_01/week_05.html#building-polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Building polynomial models",
    "text": "Building polynomial models\nStart simple: only add higher-degree terms to the extent it gives you a substantial decrease in the RSE, or satisfies an assumption hold that wasn’t satisfied before\n\nYou must include lower-order terms: e.g., if you add \\(X^3\\), you must also include \\(X\\) and \\(X^2\\)\nBe careful about overfitting when adding higher-order terms!\nBe particularly careful about extrapolating beyond the range of the data!\nMind-bender: We can think about an \\(X^2\\) term as an interaction of \\(X\\) with itself: in a parabola, the slope depends on the value of \\(X\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#the-log-transformation",
    "href": "fall_sta235_2025/week_01/week_05.html#the-log-transformation",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nWe saw that we can use transformations to fix problems\nSometimes, a violation of regression assumptions can be fixed by transforming one or the other of the variables (or both).\nWhen we transform a variable, we have to also transform our interpretation of the equation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-1",
    "href": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-1",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\nThe log transformation is frequently useful in regression, because many nonlinear relationships are naturally exponential.\n\n\\(\\log_b x=y\\) when \\(b^y=x\\)\nFor example, \\(\\log_{10} 1000 = 3\\), \\(\\log_{10}100 = 2\\), and \\(\\log_{10}10 = 1\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-2",
    "href": "fall_sta235_2025/week_01/week_05.html#the-log-transformation-2",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!)\nSkewed data is also a good candidate for log"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#moores-law",
    "href": "fall_sta235_2025/week_01/week_05.html#moores-law",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nMoore’s Law was a prediction made by Gordon Moore in 1965 (!) that the number of transistors on computer chips would double every 2 years\nThis implies exponential growth, so a linear model won’t fit well (and neither will any polynomial)\n\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#moores-law-1",
    "href": "fall_sta235_2025/week_01/week_05.html#moores-law-1",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nlm_moore = lm(Transistor.count ~ Date.of.introduction, data = moores)\nautoplot(lm_moore)\n\n\n\nA linear model is a spectacular fail"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth",
    "href": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\nIf \\(Y = ae^{bX}\\), then\n\\[\\log(Y) = \\log(a)+ bX\\]\n\nIn other words, \\(\\log(Y)\\) is a linear function of \\(X\\) when \\(Y\\) is an exponential function of \\(X\\)\nSo if we think \\(Y\\) is an exponential function of \\(X\\), predict \\(\\log(Y)\\) as a linear function of \\(X\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-1",
    "href": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nTransistors does NOT have a linear relationship with year\n\\(\\log(\\texttt{Transistors})\\) does have a linear relationship with year\n\n\nggplot(moores, aes(x = Date.of.introduction, y = log(Transistor.count))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#log-linear-model",
    "href": "fall_sta235_2025/week_01/week_05.html#log-linear-model",
    "title": "Data Science for Business Applications",
    "section": "Log-linear Model",
    "text": "Log-linear Model\nLet’s run the regression model\n\noptions(scipen = 999)\nlm_moore = lm(log(Transistor.count) ~ Date.of.introduction, data = moores)\nsummary(lm_moore)\n\n\nCall:\nlm(formula = log(Transistor.count) ~ Date.of.introduction, data = moores)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1299 -0.3338  0.1767  0.5230  2.0626 \n\nCoefficients:\n                        Estimate  Std. Error t value            Pr(&gt;|t|)    \n(Intercept)          -681.212056   15.958165  -42.69 &lt;0.0000000000000002 ***\nDate.of.introduction    0.349154    0.007981   43.75 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.054 on 99 degrees of freedom\nMultiple R-squared:  0.9508,    Adjusted R-squared:  0.9503 \nF-statistic:  1914 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-2",
    "href": "fall_sta235_2025/week_01/week_05.html#modeling-exponential-growth-2",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nautoplot(lm_moore)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\nOur model is \\[\\widehat{\\log(\\texttt{Transistors})} = −681.21 + 0.35 \\cdot \\texttt{Year}\\]\nTwo interpretations of the slope coefficient:\n\nEvery year, the predicted log of transistors goes up by 0.35\nMore useful: \\(\\exp(0.35\\cdot1)-\\exp(0.35\\cdot0) = \\exp(0.35\\cdot1)-1 =  0.419\\)\nWe then have \\(0.419\\times 100 = 41.9\\%\\) increase for each year.\nA constant percentage increase every year is exponential growth!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-1",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nMaking predictions using the log-linear model\nWhen making predictions, we have to remember that our equation gives us predictions for \\(\\log(\\texttt{Transistors})\\), not Transistors!\n\nExample: To make a prediction for the number of transistors in 2022: \\[\n\\log(\\texttt{Transistors}) = −681.21 + 0.35\\cdot(2025) = 27.54\n\\] But our prediction is not 27.54:\n\\(e^{\\log(\\texttt{Transistors})} = \\exp(27.54) = e^{27.54} = 912,998,431,886\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-2",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_line(aes(x = Date.of.introduction, y = exp(predict(lm_moore))), col = \"orange\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-3",
    "href": "fall_sta235_2025/week_01/week_05.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\n\n\n\n\nModel\nEquation\nInterpretation\n\n\n\n\nLinear\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies \\(\\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-linear\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies ≈ \\(100 \\cdot (\\exp(\\widehat{\\beta}_1)-1) \\%\\) increase in \\(\\widehat{Y}\\)\n\n\nLinear-log\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(0.01 \\cdot \\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-log\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(\\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_05.html#conclusion",
    "href": "fall_sta235_2025/week_01/week_05.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhen is the log transformation useful?\nYou can transform \\(X \\rightarrow \\log(X)\\), \\(Y \\rightarrow \\log(Y)\\), or both\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!), try transforming it with a log\nIn this case, Transistors is skewed right so it is a good candidate for log\nYou may need to do a little bit of trial and error to see what works best\nOther transformations are possible!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_04.html",
    "href": "fall_sta235_2025/week_01/week_04.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Linear models are useful:\n\nPrediction - given a new observations\nExplanatory power- which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html",
    "href": "fall_sta235_2025/week_01/week_06.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Apple quarterly revenue (Billions of dollars)\nGoal: What is the pattern here, and how can we forecast future earnings?\n\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#basic-time-series-concepts",
    "href": "fall_sta235_2025/week_01/week_06.html#basic-time-series-concepts",
    "title": "Data Science for Business Applications",
    "section": "Basic time series concepts",
    "text": "Basic time series concepts\n\nApple quarterly revenue (Billions of dollars)\nGoal: What is the pattern here, and how can we forecast future earnings?\n\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#what-are-time-series",
    "href": "fall_sta235_2025/week_01/week_06.html#what-are-time-series",
    "title": "Data Science for Business Applications",
    "section": "What are time series?",
    "text": "What are time series?\n\nData where the cases represent time: data collected every day, month, year, etc.\nTime series are important for both explaining how variables change over time and forecasting the future\nExamples of time series data:\nGoogle’s closing daily stock price every day in 2020\nInventory levels of each item at a retail store at the end of every week in 2020\nNumber of new COVID cases in the US each day since the start of the pandemic\nApple’s quarterly revenue since 2009"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#anatomy-of-a-time-series",
    "href": "fall_sta235_2025/week_01/week_06.html#anatomy-of-a-time-series",
    "title": "Data Science for Business Applications",
    "section": "Anatomy of a time series",
    "text": "Anatomy of a time series\nSome notation:\n\n\\(t = 1,2,3,...\\), time index\n\\(Y_t\\), is the value: of the variable of interest at time \\(t\\)\n\\(Y_t\\) may be composed of one or more components:\nTrend\nSeasonal\nCyclical\nRandom"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#trend-component",
    "href": "fall_sta235_2025/week_01/week_06.html#trend-component",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nA trend is persistent upwards or downwards movement in the data (not necessarily linear)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#trend-component-1",
    "href": "fall_sta235_2025/week_01/week_06.html#trend-component-1",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nExample: Moore’s Law (accelerating increase of transistor count)\nExample: US population over time\nA time series with no trend is called stationary."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#seasonal-component",
    "href": "fall_sta235_2025/week_01/week_06.html#seasonal-component",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nSeasonal fluctuation occurs when predictable up or down movements occur over a regular interval."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#seasonal-component-1",
    "href": "fall_sta235_2025/week_01/week_06.html#seasonal-component-1",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nThe ups and downs must occur over a regular interval (e.g., every month, or every year)\nExample: Highway traffic volume is highest during rush hour every day\nExample: Supermarket sales may be highest every month right after common paydays like the 15th and 30th"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#cyclic-component",
    "href": "fall_sta235_2025/week_01/week_06.html#cyclic-component",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nCyclic fluctuations occur at unpredictable intervals, e.g. due to changing business or economic conditions."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#cyclic-component-1",
    "href": "fall_sta235_2025/week_01/week_06.html#cyclic-component-1",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nIn contrast to seasonal fluctuations, cyclic fluctuations do not occur at regular, predictable intervals\nIt may be possible to predict cyclic components based on some other (non-time) variable\nExample: Restaurant sales dropped dramatically in 2020 due to COVID, as people ate out less\nExample: Sales of bell bottoms rose in the 60s and 70s, declined by the 80s, and then had a resurgence in the 90s"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#remaindererror-component",
    "href": "fall_sta235_2025/week_01/week_06.html#remaindererror-component",
    "title": "Data Science for Business Applications",
    "section": "Remainder/Error component",
    "text": "Remainder/Error component\n\nAny real time series will always have random noise as well, which can’t be predicted or forecast."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#time-series-components",
    "href": "fall_sta235_2025/week_01/week_06.html#time-series-components",
    "title": "Data Science for Business Applications",
    "section": "Time Series Components",
    "text": "Time Series Components\n\nWhich component(s) you see in each of these time series?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#putting-these-together",
    "href": "fall_sta235_2025/week_01/week_06.html#putting-these-together",
    "title": "Data Science for Business Applications",
    "section": "Putting these together",
    "text": "Putting these together\nReal time series will usually include a combination of these four components. We will model the time series \\(Y_t\\) either additively:\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\] Or multiplicatively: \\[\nY_t = \\text{Trend}\\cdot\\text{Seasonal}\\cdot\\text{Random}= T_t \\cdot S_t \\cdot E_t\n\\] * (\\(E_t\\) consists of both the cyclic and error components, as both are unpredictable.) This model can be rewritten as a log model: \\[\n\\log{Y_t} = \\log(T_t) + \\log(S_t) + \\log(E_t)\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#additive-models",
    "href": "fall_sta235_2025/week_01/week_06.html#additive-models",
    "title": "Data Science for Business Applications",
    "section": "Additive models",
    "text": "Additive models\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\]\n\nMost appropriate when seasonal fluctuations are consistent (do not increase or decrease over time)\nThe trend component \\(T_t\\) is a function of t (e.g., linear or quadratic)\nThe seasonal component \\(S_t\\) is a set of dummy variable representing “seasons”\nSo we can estimate additive models using regular regression"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#additive-decomposition",
    "href": "fall_sta235_2025/week_01/week_06.html#additive-decomposition",
    "title": "Data Science for Business Applications",
    "section": "Additive decomposition",
    "text": "Additive decomposition\n\nRun a regression predicting \\(Y\\) as a function of:\n\n\n\\(t\\), \\(t^2\\), \\(\\log(t)\\) etc (the trend component \\(T_t\\))\nDummy variables for the seasons (the seasonal component \\(S_t\\))\n\n\nTo make a prediction for \\(Y\\), plug into the model!\nThe residuals of this model correspond to the error component \\(E_t\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-quarterly-revenue",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-quarterly-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple quarterly revenue",
    "text": "Apple quarterly revenue\n\nWhat components do you see here?\n\n\nlibrary(tidyverse)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model",
    "href": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nlm_additive = lm(Revenue ~ Period + Quarter, data=apple) \nsummary(lm_additive)\n\n\nCall:\nlm(formula = Revenue ~ Period + Quarter, data = apple)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.496  -5.135   1.280   4.923  17.928 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  33.93619    2.74731  12.353 &lt; 0.0000000000000002 ***\nPeriod        1.41324    0.05917  23.884 &lt; 0.0000000000000002 ***\nQuarterQ2   -20.62657    2.89298  -7.130    0.000000002310936 ***\nQuarterQ3   -27.44818    2.89480  -9.482    0.000000000000362 ***\nQuarterQ4   -24.20276    2.89298  -8.366    0.000000000022170 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.921 on 55 degrees of freedom\nMultiple R-squared:  0.9269,    Adjusted R-squared:  0.9216 \nF-statistic: 174.4 on 4 and 55 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#interpretation-of-the-model",
    "href": "fall_sta235_2025/week_01/week_06.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nThe trend that we can infer from the variable Period indicates a positive growth in revenue of US$ 1.4 billion for each increase in the periods.\nThe seasonal from the Quarter component indicates:\n\n\nQ2’s are expected to be $20.7 worse than Q1’s\nQ3’s are expected to be $27.4 worse than Q1’s\nQ4’s are expected to be $24.2 worse than Q1’s\nQ3’s are significantly worse than Q1’s\n\n\nThese effects are statistically significant (confint(lm_additive))\nThe RSE from this model is US$ 7.921 billions of dollars.\nHow can we interpret these results?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model-1",
    "href": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model-1",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line() +\n  geom_line(aes(x = Time, y = predict(lm_additive)), col = \"orange\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model-2",
    "href": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model-2",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nWhat does the final model predict from the Quarter component indicates: for Apple in 2024 Q3?\n\n\npredict(lm_additive, list(Period = 61, Quarter = \"Q3\"), interval = \"prediction\")\n\n       fit      lwr     upr\n1 92.69571 75.86745 109.524\n\n\n\nThe actual revenue was US$ 85.78 billions\nWhat does the final model predict from the Quarter component indicates: for Apple in 2030 Q1? (Should we trust that prediction?)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model-3",
    "href": "fall_sta235_2025/week_01/week_06.html#fitting-additive-model-3",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nThe residuals from this model show the “detrended and deasonalized” data (but there’s still some trend left!):\nWe hadn’t yet dealt with the time dependence\n\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line(aes(x = Time, y = residuals(lm_additive)))"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#autorgression-model",
    "href": "fall_sta235_2025/week_01/week_06.html#autorgression-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression model",
    "text": "Autorgression model\n\nHow we deal with the time dependence ? Key idea: Instead of predicting \\(Y_t\\) as a function of \\(t\\) (or other variables), predict \\(Y_t\\) as a function of \\(Y_{t-1}\\): \\[\nY_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\n\\]\n\\(Y_{t-1}\\) is called the “1st lag” of \\(Y\\)\nThis is called autoregressive (AR) because it predicts the values of a time series based on previous values\nThe model above is an AR(1) model\nWe can have AR(\\(p\\)) models, with lag \\(p\\)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#autocorrelation",
    "href": "fall_sta235_2025/week_01/week_06.html#autocorrelation",
    "title": "Data Science for Business Applications",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation, is the correlation of \\(Y_t\\) with each of its lags \\(Y_t, Y_{t−1},\\dots\\) \\[\nCor(Y_t, Y_{t−1}), Cor(Y_t, Y_{t−2}),\\dots\n\\]\nWe also have the autocorrelation of the residuals, \\(r_t\\)’s, which indicates that there’s a strong indication that the independence assumption is violated \\[\nCor(r_t, r_{t−1}), Cor(r_t, r_{t−2}),\\dots\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#ozone-example",
    "href": "fall_sta235_2025/week_01/week_06.html#ozone-example",
    "title": "Data Science for Business Applications",
    "section": "Ozone example",
    "text": "Ozone example\n\nCreating an AR(1) model: Daily ozone levels in Houston\n\n\nggplot(ozone, aes(x = day, y = ozone)) + \n  geom_line()"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#acf-plot",
    "href": "fall_sta235_2025/week_01/week_06.html#acf-plot",
    "title": "Data Science for Business Applications",
    "section": "ACF plot",
    "text": "ACF plot\n\nVisualizing the autocorrelation function (ACF)\n\n\nacf(ozone$ozone)\n\n\n\nAutocorrelations outside of the dashed blue lines are statistically significant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#autorgression-of-the-model",
    "href": "fall_sta235_2025/week_01/week_06.html#autorgression-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression of the model",
    "text": "Autorgression of the model\n\nWe use the lag function to create the lagged observations\n\n\nozone &lt;- ozone %&gt;% \n  mutate(lag1=lag(ozone)) \nozone.model = lm(ozone ~ lag1, data=ozone) \nsummary(ozone.model)\n\n\nCall:\nlm(formula = ozone ~ lag1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.192  -3.464  -1.108   2.679  16.679 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)  6.87446    1.06976   6.426 0.00000000276 ***\nlag1         0.40419    0.08381   4.823 0.00000419740 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.999 on 120 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1624,    Adjusted R-squared:  0.1554 \nF-statistic: 23.26 on 1 and 120 DF,  p-value: 0.000004197"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#assumptions-of-an-ar1-model",
    "href": "fall_sta235_2025/week_01/week_06.html#assumptions-of-an-ar1-model",
    "title": "Data Science for Business Applications",
    "section": "Assumptions of an AR(1) model",
    "text": "Assumptions of an AR(1) model\n\nLinearity, Normality, Equal Variance: Check using residual plot (linearity + homoscedasticity), Q-Q plot (normality), scale/location (homoscedasticity) like any other regression model\nIndependence: Since this is a time series, we can actually check this by looking at the autocorrelation of the residuals (we want no significant autocorrelation)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#autoplot",
    "href": "fall_sta235_2025/week_01/week_06.html#autoplot",
    "title": "Data Science for Business Applications",
    "section": "Autoplot",
    "text": "Autoplot\n\nLinearity, Normality, Equal Variance\n\n\nautoplot(ozone.model)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#acf-of-the-residuals",
    "href": "fall_sta235_2025/week_01/week_06.html#acf-of-the-residuals",
    "title": "Data Science for Business Applications",
    "section": "ACF of the residuals",
    "text": "ACF of the residuals\n\nacf(ozone.model$residuals)\n\n\n\nWe expect 5% of autocorrelations to be significant just by chance, so having just 1 out of the 20 lags flagged as significant indicates we are OK on independence!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#making-predictions-in-time-series",
    "href": "fall_sta235_2025/week_01/week_06.html#making-predictions-in-time-series",
    "title": "Data Science for Business Applications",
    "section": "Making predictions in time series",
    "text": "Making predictions in time series\n\n\n\n\n\n\n\n\nType\nModel\nPredicted \\(Y_t\\)\n\n\n\n\nWhite noise\n\\(Y_t = e_t\\)\n\\(0\\)\n\n\nRandom sample\n\\(Y_t = \\beta_0 + e_t\\)\n\\(\\widehat{\\beta}_0\\) (or average \\(Y\\))\n\n\nRandom walk\n\\(Y_t = \\beta_0 + Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + Y_{t-1}\\)\n\n\nGeneral AR(1)\n\\(Y_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 Y_{t-1}\\)\n\n\n\n\nUnit root occurs when \\(\\beta_1 = 1\\). This means:\nThe series is a random walk.\nThere’s no mean reversion, and any shocks will have a permanent effect.\nWhen \\(\\beta_1 = 1\\), the model is non-stationary, meaning the series tends to “drift” without stabilizing around a fixed mean.\nIf \\(|\\beta_1| &lt; 1\\), the series is mean-reverting, and shocks are temporary."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#statistical-analysis",
    "href": "fall_sta235_2025/week_01/week_06.html#statistical-analysis",
    "title": "Data Science for Business Applications",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nconfint(ozone.model)\n\n                2.5 %    97.5 %\n(Intercept) 4.7564110 8.9925161\nlag1        0.2382561 0.5701286\n\n\n\nThe coefficient \\(\\widehat{\\beta}_1\\) is associated with the variable lag1.\nIn this case, for the larger population, with 95% confidence, \\(\\widehat{\\beta}_1\\) lies between 0.24 and 0.57.\nThis means that \\(|\\beta_1| &lt; 1\\), indicating that the series is mean-reverting."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-revenue-acf-plot",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-revenue-acf-plot",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the additive model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-revenue",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nCombining decomposition and autoregression in a multiplicative model\n\n\\[\n\\log(\\texttt{Revenue}_t) = \\log(\\texttt{Period}_t) + \\texttt{Quarter}_t + \\log(\\texttt{Revenue}_{t-1})\n\\]\n\nWe need to create the lag variable.\nIt will have only one lag, and thus is an AR(1) model.\n\n\napple = apple %&gt;% \n  mutate(lag1 = lag(Revenue))"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-revenue-1",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-revenue-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nlog_apple = lm(log(Revenue) ~ log(Period) + Quarter + log(lag1), data = apple)\nsummary(log_apple)\n\n\nCall:\nlm(formula = log(Revenue) ~ log(Period) + Quarter + log(lag1), \n    data = apple)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.204851 -0.056602  0.005991  0.066084  0.193337 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.14400    0.17945   6.375  0.00000004558506504 ***\nlog(Period)  0.20622    0.06918   2.981              0.00433 ** \nQuarterQ2   -0.53559    0.04911 -10.906  0.00000000000000372 ***\nQuarterQ3   -0.47076    0.03397 -13.859 &lt; 0.0000000000000002 ***\nQuarterQ4   -0.31872    0.03346  -9.526  0.00000000000044727 ***\nlog(lag1)    0.63410    0.10109   6.273  0.00000006650534405 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09013 on 53 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9751,    Adjusted R-squared:  0.9728 \nF-statistic: 415.4 on 5 and 53 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-revenue-predictions",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-revenue-predictions",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nPredictions of multiplicative model"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-revenue-predictions-1",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-revenue-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nConfidence interval of the multiplicative model\n\n\nconfint(log_apple)\n\n                  2.5 %     97.5 %\n(Intercept)  0.78406737  1.5039420\nlog(Period)  0.06746219  0.3449861\nQuarterQ2   -0.63409896 -0.4370871\nQuarterQ3   -0.53888914 -0.4026276\nQuarterQ4   -0.38583509 -0.2516142\nlog(lag1)    0.43133359  0.8368601\n\n\n\nThe slope associated with lag is statistically significant, and its value is between minus and plus one; we have that this is a mean-reverting time series.\nWe also have a better fit (here we feed lag1 with prediction from the previous period, US$ 90.75 billions):\n\n\n exp(predict(log_apple, list(Period = 61, Quarter = \"Q3\", lag1 = 90.75), interval = \"prediction\"))\n\n       fit      lwr      upr\n1 79.80492 66.06926 96.39618\n\n\n\nThe confidence interval for the forecast is narrower, and the difference between what we observe and predict is smaller."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#apple-revenue-acf-plot-1",
    "href": "fall_sta235_2025/week_01/week_06.html#apple-revenue-acf-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the multiplicative model.\n\n\n\nThe independent assumptions look better, but it might be necessary to add more lags."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_06.html#time-series-strategy",
    "href": "fall_sta235_2025/week_01/week_06.html#time-series-strategy",
    "title": "Data Science for Business Applications",
    "section": "Time Series Strategy",
    "text": "Time Series Strategy\nTo building a time series model:\n\nStart with a an additive or multiplicative model with trend and seasonal components. (Plot your data! If the seasonal variation increases or decreases over time you’ll want a multiplicative model.)\nExamine the usual diagnostic plots, and plot your residuals as a function of time. Do you need a (different) nonlinear time trend? A transformation of \\(Y\\)?\nCheck your residuals for autocorrelation. If it’s present, add appropriate lag terms to your model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#how-should-we-evaluate-a-forecast",
    "href": "fall_sta235_2025/week_01/week-07.html#how-should-we-evaluate-a-forecast",
    "title": "Data Science for Business Applications",
    "section": "How should we evaluate a forecast?",
    "text": "How should we evaluate a forecast?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#section",
    "href": "fall_sta235_2025/week_01/week-07.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "apple &lt;- apple %&gt;% mutate(\n  COVID = ifelse(Period &gt;= 38 & Period &lt;= 45, 1, 0),\n  lag1 = lag(Revenue)\n)\nmodel4 &lt;- lm(log(Revenue) ~ Period + Quarter \n                  + COVID + log(lag1), data=apple)\nsummary(model4)\n\n\nCall:\nlm(formula = log(Revenue) ~ Period + Quarter + COVID + log(lag1), \n    data = apple)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.164955 -0.047524 -0.006454  0.044278  0.149002 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  2.472570   0.387615   6.379     0.00000007169178 ***\nPeriod       0.010331   0.002215   4.664     0.00002593255364 ***\nQuarterQ2   -0.471450   0.049927  -9.443     0.00000000000197 ***\nQuarterQ3   -0.487549   0.026755 -18.223 &lt; 0.0000000000000002 ***\nQuarterQ4   -0.352342   0.025851 -13.630 &lt; 0.0000000000000002 ***\nCOVID        0.109412   0.029754   3.677             0.000605 ***\nlog(lag1)    0.413292   0.111170   3.718             0.000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06482 on 47 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9716,    Adjusted R-squared:  0.968 \nF-statistic: 267.9 on 6 and 47 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#section-1",
    "href": "fall_sta235_2025/week_01/week-07.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "If you were an analyst, how impressed would your boss be if you claimed to be able to predict the past? (Not very — they want you to be able to predict the future!)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#how-should-we-evaluate-a-forecast-1",
    "href": "fall_sta235_2025/week_01/week-07.html#how-should-we-evaluate-a-forecast-1",
    "title": "Data Science for Business Applications",
    "section": "How should we evaluate a forecast?",
    "text": "How should we evaluate a forecast?\n\n\\(R^2\\) and the residual standard error are optimistic measures of predictive performance.\nThey measure the quality of predictions using the same data used to fit the model.\nBut we’re interested in how the model predicts into the future, not how well it fits the past!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#section-2",
    "href": "fall_sta235_2025/week_01/week-07.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We can mimic predicting on future data by holding back the most recent observations.\nFit the model to the “training” data only (blue) and test the model by forecast the most recent year of data (red)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#fitting-the-model-to-the-training-set",
    "href": "fall_sta235_2025/week_01/week-07.html#fitting-the-model-to-the-training-set",
    "title": "Data Science for Business Applications",
    "section": "Fitting the model to the training set",
    "text": "Fitting the model to the training set\nWe want to split our data into a test set (last 4 quarters) and a training set (everything else).\n\ntraining &lt;- apple[1:51,]\ntest &lt;- apple[52:55,]\n\nThen we want to fit the model to the training data and test it on the test data.\n\nmodel4 &lt;- lm(log(Revenue) ~ Period + Quarter \n                  + COVID + log(lag1), data=training)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#testing-the-model-on-the-test-set",
    "href": "fall_sta235_2025/week_01/week-07.html#testing-the-model-on-the-test-set",
    "title": "Data Science for Business Applications",
    "section": "Testing the model on the test set",
    "text": "Testing the model on the test set\nNow we can see how well the model predicts the test set.\n\npredict(model4, test)\n\n      52       53       54       55 \n4.377099 4.502806 4.918158 4.558995 \n\n\n\nThese are predictions for log revenue, so we need to convert them back to revenue:\n\nexp(predict(model4, test))\n\n       52        53        54        55 \n 79.60673  90.27005 136.75045  95.48747 \n\n\n\n\nHow do we compare this to the actual revenues of the last 4 quarters?\n\ntest.Rev = test$Revenue"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#measuring-prediction-error",
    "href": "fall_sta235_2025/week_01/week-07.html#measuring-prediction-error",
    "title": "Data Science for Business Applications",
    "section": "Measuring prediction error",
    "text": "Measuring prediction error\nMean squared prediction error (MSE or MSPE) lets us compare all 4 predictions to the actual revenues at once:\n\\[\n  \\text{MSPE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat Y_i)^2\n\\]\n(\\(n\\) is the size of the test set; \\(n=4\\) this case)\nTake the square root to get an interpretable number (in the units of \\(Y\\)):\n\\[\n  \\text{RMSPE} = \\sqrt{\\text{MSPE}}\n\\]\nInterpret RMSPE just like residual standard error (smaller is better!)!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#calculating-rmspe",
    "href": "fall_sta235_2025/week_01/week-07.html#calculating-rmspe",
    "title": "Data Science for Business Applications",
    "section": "Calculating RMSPE",
    "text": "Calculating RMSPE\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(Y_i\\)\n\\(\\hat Y_i\\)\n\\(Y_i - \\hat Y_i\\)\n\\((Y_i - \\hat Y_i)^2\\)\n\n\n\n\n\\(1\\)\n\\(85.8\\)\n\\(79.61\\)\n\\(6.19\\)\n\\(38.36\\)\n\n\n\\(2\\)\n\\(94.9\\)\n\\(90.27\\)\n\\(4.63\\)\n\\(21.44\\)\n\n\n\\(3\\)\n\\(124.3\\)\n\\(136.75\\)\n\\(-12.45\\)\n\\(155.01\\)\n\n\n\\(4\\)\n\\(95.4\\)\n\\(95.49\\)\n\\(-0.09\\)\n\\(830.16\\)\n\n\n\n\n\\[\n  \\text{RMSPE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^4 (Y_i - \\hat Y_i)^2} = 7.33\n\\]\n\n\nIn other words, we expect our model to be off by about\n\nround(sqrt(mean((test$Revenue - exp(predict(model4, test)))^2)), 2)\n\n[1] 7.33\n\n\nbillion dollars on average when forecasting the future."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#a-shortcut-for-calculating-rmspe",
    "href": "fall_sta235_2025/week_01/week-07.html#a-shortcut-for-calculating-rmspe",
    "title": "Data Science for Business Applications",
    "section": "A shortcut for calculating RMSPE",
    "text": "A shortcut for calculating RMSPE\n\n# exp here only because we're predicting log(Revenue) instead of Revenue\npredictions &lt;- exp(predict(model4, test))\nactuals &lt;- test$Revenue\nsqrt(mean((predictions - actuals)^2))\n\n[1] 7.328273"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#traintest-splits-to-measure-performance",
    "href": "fall_sta235_2025/week_01/week-07.html#traintest-splits-to-measure-performance",
    "title": "Data Science for Business Applications",
    "section": "Train/test splits to measure performance",
    "text": "Train/test splits to measure performance\n\n\n\n\n\nRandomly allocating observations makes training/testing sets representative of the full data set\n80/20% training/testing splits are common\nMore testing data \\(\\to\\) More accurate estimates of prediction error\nMore training data \\(\\to\\) Better representation of full-data fit"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#predicting-utility-expenditures",
    "href": "fall_sta235_2025/week_01/week-07.html#predicting-utility-expenditures",
    "title": "Data Science for Business Applications",
    "section": "Predicting utility expenditures",
    "text": "Predicting utility expenditures"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#creating-a-traintest-split",
    "href": "fall_sta235_2025/week_01/week-07.html#creating-a-traintest-split",
    "title": "Data Science for Business Applications",
    "section": "Creating a train/test split",
    "text": "Creating a train/test split\nLet’s do the same thing for the utilities data, but by randomly splitting the data into a training and test set.\n\nlibrary(tidyverse)\nset.seed(9529)\n\n# Ask R to shuffle all 117 observations\nrandomized &lt;- slice_sample(utilities, n = 117)\n\n# 24 is about 20% of 117, so we'll use the first 24 rows as our test set\nutilities_test &lt;- randomized[1:24,]\n\n# The rest are our training set\nutilities_train &lt;- randomized[25:117,]\n\nThe set.seed(9529) command ensures reproducible “random” splits. (I chose this number because it’s my EID number; use your EID number!)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#creating-a-traintest-split-1",
    "href": "fall_sta235_2025/week_01/week-07.html#creating-a-traintest-split-1",
    "title": "Data Science for Business Applications",
    "section": "Creating a train/test split",
    "text": "Creating a train/test split\nBlack = training data, Red = testing data\nRandom splitting is important! (What would happen if we trained on fall-spring and tested on summer?)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#using-trainingtest-splits-to-pick-between-competing-models",
    "href": "fall_sta235_2025/week_01/week-07.html#using-trainingtest-splits-to-pick-between-competing-models",
    "title": "Data Science for Business Applications",
    "section": "Using training/test splits to pick between competing models",
    "text": "Using training/test splits to pick between competing models\nWhen comparing models, we want to pick the one that minimizes prediction error on the test set — we want to avoid overfitting to the training data!\nWhen we fit to the entire data set, we might overfit to the training data and get an overly optimistic estimate of prediction error on new data."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#a-linear-1st-degree-polynomial-model",
    "href": "fall_sta235_2025/week_01/week-07.html#a-linear-1st-degree-polynomial-model",
    "title": "Data Science for Business Applications",
    "section": "A linear (1st-degree) polynomial model",
    "text": "A linear (1st-degree) polynomial model\n\n\n\nmodel1 &lt;- lm(dailyspend ~ temp, data=utilities)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59   &lt;2e-16 ***\ntemp        -0.096432   0.003911  -24.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#a-quadratic-2nd-degree-polynomial-model",
    "href": "fall_sta235_2025/week_01/week-07.html#a-quadratic-2nd-degree-polynomial-model",
    "title": "Data Science for Business Applications",
    "section": "A quadratic (2nd-degree) polynomial model",
    "text": "A quadratic (2nd-degree) polynomial model\n\n\n\nmodel2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239      &lt; 2e-16 ***\ntemp        -0.2115553  0.0191046 -11.074      &lt; 2e-16 ***\nI(temp^2)    0.0012476  0.0002037   6.124 0.0000000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#a-9th-degree-polynomial-model",
    "href": "fall_sta235_2025/week_01/week-07.html#a-9th-degree-polynomial-model",
    "title": "Data Science for Business Applications",
    "section": "A 9th-degree polynomial model",
    "text": "A 9th-degree polynomial model\n\n\n\nmodel9 &lt;- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) \n              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),\n              data=utilities)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) + \n    I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9), \n    data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.66386 -0.24156 -0.01734  0.25996  2.48031 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.144e+01  4.599e+01  -0.466    0.642\ntemp         8.103e+00  1.460e+01   0.555    0.580\nI(temp^2)   -8.468e-01  1.905e+00  -0.445    0.658\nI(temp^3)    4.153e-02  1.349e-01   0.308    0.759\nI(temp^4)   -9.600e-04  5.764e-03  -0.167    0.868\nI(temp^5)    4.563e-06  1.551e-04   0.029    0.977\nI(temp^6)    2.603e-07  2.644e-06   0.098    0.922\nI(temp^7)   -5.943e-09  2.770e-08  -0.215    0.831\nI(temp^8)    5.173e-11  1.627e-10   0.318    0.751\nI(temp^9)   -1.676e-13  4.095e-13  -0.409    0.683\n\nResidual standard error: 0.7501 on 107 degrees of freedom\nMultiple R-squared:  0.889, Adjusted R-squared:  0.8797 \nF-statistic: 95.26 on 9 and 107 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#building-the-models",
    "href": "fall_sta235_2025/week_01/week-07.html#building-the-models",
    "title": "Data Science for Business Applications",
    "section": "Building the models",
    "text": "Building the models\nWe can avoid overfitting to the training data by fitting the models to the training data and then testing them on the test data.\nFit the models using the training set:\n\nmodel1 &lt;- lm(dailyspend ~ temp, data=utilities_train)\nmodel2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities_train)\nmodel9 &lt;- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) \n              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),\n              data=utilities_train)\n\nGet their predictions on the test set:\n\nactuals &lt;- utilities_test$dailyspend\npredictions1 &lt;- predict(model1, utilities_test)\npredictions2 &lt;- predict(model2, utilities_test)\npredictions9 &lt;- predict(model9, utilities_test)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#measuring-performance",
    "href": "fall_sta235_2025/week_01/week-07.html#measuring-performance",
    "title": "Data Science for Business Applications",
    "section": "Measuring performance",
    "text": "Measuring performance\nNow let’s calculate RMSPE on the testing data:\n\nsqrt(mean((actuals - predictions1)^2))\n\n[1] 1.013597\n\nsqrt(mean((actuals - predictions2)^2))\n\n[1] 0.8534912\n\nsqrt(mean((actuals - predictions9)^2))\n\n[1] 0.9482653\n\n\n\n\n\n\n\n\n\n\n\nModel\nRSE (all data)\nRMSPE (training/test split)\n\n\n\n\nLinear\n\\(0.83\\)\n\\(1.01\\)\n\n\nQuadratic\n\\(0.73\\)\n\\(0.85\\)\n\n\n9th Degree\n\\(0.71\\)\n\\(0.95\\)\n\n\n\n\n\n\nThese are all worse than the predictions of the models fit to the entire data set!\nIt shows that the quadratic model is the best of the three for making predictions on new data"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#the-problem-with-this-approach",
    "href": "fall_sta235_2025/week_01/week-07.html#the-problem-with-this-approach",
    "title": "Data Science for Business Applications",
    "section": "The problem with this approach",
    "text": "The problem with this approach\n\nWe might get “unlucky,” and our train/test split might not give us a representative estimate of prediction error!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#k-fold-cross-validation",
    "href": "fall_sta235_2025/week_01/week-07.html#k-fold-cross-validation",
    "title": "Data Science for Business Applications",
    "section": "\\(k\\)-fold Cross-Validation",
    "text": "\\(k\\)-fold Cross-Validation\n\n\n\n\nAverages results over \\(k\\) random train/test splits, to avoid getting bad results from possible bad luck of a single train/test split.\n\\(k=n\\) is called leave-one-out cross validation"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#lets-work-through-an-example-with-k3",
    "href": "fall_sta235_2025/week_01/week-07.html#lets-work-through-an-example-with-k3",
    "title": "Data Science for Business Applications",
    "section": "Let’s work through an example with \\(k=3\\)",
    "text": "Let’s work through an example with \\(k=3\\)\nFirst we need to split the data into 3 random groups of roughly the same size (\\(39\\times 3=117\\)):\n\nfold1 &lt;- randomized[1:39,]\nfold2 &lt;- randomized[40:78,]\nfold3 &lt;- randomized[79:117,]\n\n\nNow we need to build the model 3 times:\n\nModel A: test on fold1, train on fold2 and fold3\nModel B: test on fold2, train on fold1 and fold3\nModel C: test on fold3, train on fold1 and fold2"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#building-the-models-1",
    "href": "fall_sta235_2025/week_01/week-07.html#building-the-models-1",
    "title": "Data Science for Business Applications",
    "section": "Building the models",
    "text": "Building the models\n\nmodelA &lt;- lm(dailyspend ~ temp, data=rbind(fold2, fold3))\nmodelB &lt;- lm(dailyspend ~ temp, data=rbind(fold1, fold3))\nmodelC &lt;- lm(dailyspend ~ temp, data=rbind(fold1, fold2))"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#getting-the-predictions",
    "href": "fall_sta235_2025/week_01/week-07.html#getting-the-predictions",
    "title": "Data Science for Business Applications",
    "section": "Getting the predictions",
    "text": "Getting the predictions\n\npredictionsA &lt;- predict(modelA, fold1)\npredictionsB &lt;- predict(modelB, fold2)\npredictionsC &lt;- predict(modelC, fold3)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#calculating-rmspe-1",
    "href": "fall_sta235_2025/week_01/week-07.html#calculating-rmspe-1",
    "title": "Data Science for Business Applications",
    "section": "Calculating RMSPE",
    "text": "Calculating RMSPE\nWe now get 3 different RMSPE estimates:\n\nactualsA &lt;- fold1$dailyspend\nsqrt(mean((actualsA - predictionsA)^2))\n\n[1] 0.9898423\n\nactualsB &lt;- fold2$dailyspend\nsqrt(mean((actualsB - predictionsB)^2))\n\n[1] 0.745035\n\nactualsC &lt;- fold3$dailyspend\nsqrt(mean((actualsC - predictionsC)^2))\n\n[1] 0.8463284\n\n\nOur final estimate of RMSPE is the average of these 3 estimates:\n\\[\n  \\text{RMSPE} = \\frac{ 0.99 + 0.75 + 0.85 }{3} = 0.86\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#re-doing-our-model-comparisons",
    "href": "fall_sta235_2025/week_01/week-07.html#re-doing-our-model-comparisons",
    "title": "Data Science for Business Applications",
    "section": "Re-doing our model comparisons",
    "text": "Re-doing our model comparisons\n\n\n\n\n\n\n\n\n\nModel\nRSE (all data)\nRMSPE (3 Fold CV)\nRMSPE (Single Train/Test Split)\n\n\n\n\nLinear\n\\(0.83\\)\n\\(0.86\\)\n\\(1.01\\)\n\n\nQuadratic\n\\(0.73\\)\n\\(0.77\\)\n\\(0.85\\)\n\n\n9th Degree\n\\(0.71\\)\n\\(0.91\\)\n\\(0.95\\)\n\n\n\n\nComparing RSE and CV:\n\nRSE misses overfitting\nFor every model, RSE is too optimistic about how well the model will perform on new data\n\nOur initial train/test split was unlucky! It over-estimated the RMSE"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week-07.html#isnt-there-a-faster-way",
    "href": "fall_sta235_2025/week_01/week-07.html#isnt-there-a-faster-way",
    "title": "Data Science for Business Applications",
    "section": "Isn’t there a faster way?",
    "text": "Isn’t there a faster way?\nIn practice, you don’t have to do this all manually! The tidymodels package makes it easy to do cross-validation even for larger \\(k\\).\n\nlibrary(tidymodels)\n\n# Randomly split the data into 5 folds\nfolds &lt;- vfold_cv(utilities, v=5)\n\n# Build a tidymodels \"workflow\" for a linear model\n# with the formula dailyspend ~ temp, using RMSE as the target metric\nworkflow() %&gt;% \n  add_model(linear_reg()) %&gt;%\n  add_formula(dailyspend ~ temp) %&gt;%\n  fit_resamples(folds, metrics=metric_set(rmse)) %&gt;%\n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   0.871     5  0.0857 pre0_mod0_post0"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#how-should-we-evaluate-a-forecast",
    "href": "fall_sta235_2025/week_01/week_07.html#how-should-we-evaluate-a-forecast",
    "title": "Data Science for Business Applications",
    "section": "How should we evaluate a forecast?",
    "text": "How should we evaluate a forecast?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#section",
    "href": "fall_sta235_2025/week_01/week_07.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "apple &lt;- apple %&gt;% mutate(\n  COVID = ifelse(Period &gt;= 38 & Period &lt;= 45, 1, 0),\n  lag1 = lag(Revenue)\n)\nmodel4 &lt;- lm(log(Revenue) ~ Period + Quarter \n                  + COVID + log(lag1), data=apple)\nsummary(model4)\n\n\nCall:\nlm(formula = log(Revenue) ~ Period + Quarter + COVID + log(lag1), \n    data = apple)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.164955 -0.047524 -0.006454  0.044278  0.149002 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  2.472570   0.387615   6.379     0.00000007169178 ***\nPeriod       0.010331   0.002215   4.664     0.00002593255364 ***\nQuarterQ2   -0.471450   0.049927  -9.443     0.00000000000197 ***\nQuarterQ3   -0.487549   0.026755 -18.223 &lt; 0.0000000000000002 ***\nQuarterQ4   -0.352342   0.025851 -13.630 &lt; 0.0000000000000002 ***\nCOVID        0.109412   0.029754   3.677             0.000605 ***\nlog(lag1)    0.413292   0.111170   3.718             0.000535 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06482 on 47 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9716,    Adjusted R-squared:  0.968 \nF-statistic: 267.9 on 6 and 47 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#section-1",
    "href": "fall_sta235_2025/week_01/week_07.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "If you were an analyst, how impressed would your boss be if you claimed to be able to predict the past? (Not very — they want you to be able to predict the future!)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#how-should-we-evaluate-a-forecast-1",
    "href": "fall_sta235_2025/week_01/week_07.html#how-should-we-evaluate-a-forecast-1",
    "title": "Data Science for Business Applications",
    "section": "How should we evaluate a forecast?",
    "text": "How should we evaluate a forecast?\n\n\\(R^2\\) and the residual standard error are optimistic measures of predictive performance.\nThey measure the quality of predictions using the same data used to fit the model.\nBut we’re interested in how the model predicts into the future, not how well it fits the past!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#section-2",
    "href": "fall_sta235_2025/week_01/week_07.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "We can mimic predicting on future data by holding back the most recent observations.\nFit the model to the “training” data only (blue) and test the model by forecast the most recent year of data (red)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#fitting-the-model-to-the-training-set",
    "href": "fall_sta235_2025/week_01/week_07.html#fitting-the-model-to-the-training-set",
    "title": "Data Science for Business Applications",
    "section": "Fitting the model to the training set",
    "text": "Fitting the model to the training set\nWe want to split our data into a test set (last 4 quarters) and a training set (everything else).\n\ntraining &lt;- apple[1:51,]\ntest &lt;- apple[52:55,]\n\nThen we want to fit the model to the training data and test it on the test data.\n\nmodel4 &lt;- lm(log(Revenue) ~ Period + Quarter \n                  + COVID + log(lag1), data=training)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#testing-the-model-on-the-test-set",
    "href": "fall_sta235_2025/week_01/week_07.html#testing-the-model-on-the-test-set",
    "title": "Data Science for Business Applications",
    "section": "Testing the model on the test set",
    "text": "Testing the model on the test set\nNow we can see how well the model predicts the test set.\n\npredict(model4, test)\n\n      52       53       54       55 \n4.377099 4.502806 4.918158 4.558995 \n\n\n\nThese are predictions for log revenue, so we need to convert them back to revenue:\n\nexp(predict(model4, test))\n\n       52        53        54        55 \n 79.60673  90.27005 136.75045  95.48747 \n\n\n\n\nHow do we compare this to the actual revenues of the last 4 quarters?\n\ntest.Rev = test$Revenue"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#measuring-prediction-error",
    "href": "fall_sta235_2025/week_01/week_07.html#measuring-prediction-error",
    "title": "Data Science for Business Applications",
    "section": "Measuring prediction error",
    "text": "Measuring prediction error\nMean squared prediction error (MSE or MSPE) lets us compare all 4 predictions to the actual revenues at once:\n\\[\n  \\text{MSPE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat Y_i)^2\n\\]\n(\\(n\\) is the size of the test set; \\(n=4\\) this case)\nTake the square root to get an interpretable number (in the units of \\(Y\\)):\n\\[\n  \\text{RMSPE} = \\sqrt{\\text{MSPE}}\n\\]\nInterpret RMSPE just like residual standard error (smaller is better!)!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#calculating-rmspe",
    "href": "fall_sta235_2025/week_01/week_07.html#calculating-rmspe",
    "title": "Data Science for Business Applications",
    "section": "Calculating RMSPE",
    "text": "Calculating RMSPE\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(Y_i\\)\n\\(\\hat Y_i\\)\n\\(Y_i - \\hat Y_i\\)\n\\((Y_i - \\hat Y_i)^2\\)\n\n\n\n\n\\(1\\)\n\\(85.8\\)\n\\(79.61\\)\n\\(6.19\\)\n\\(38.36\\)\n\n\n\\(2\\)\n\\(94.9\\)\n\\(79.61\\)\n\\(4.63\\)\n\\(21.44\\)\n\n\n\\(3\\)\n\\(124.3\\)\n\\(136.75\\)\n\\(44.69\\)\n\\(1997.49\\)\n\n\n\\(4\\)\n\\(95.4\\)\n\\(95.49\\)\n\\(-0.09\\)\n\\(0.01\\)\n\n\n\n\n\\[\n  \\text{RMSPE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^4 (Y_i - \\hat Y_i)^2} = 7.33\n\\]\n\n\nIn other words, we expect our model to be off by about\n\nround(sqrt(mean((test$Revenue - exp(predict(model4, test)))^2)), 2)\n\n[1] 7.33\n\n\nbillion dollars on average when forecasting the future."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#a-shortcut-for-calculating-rmspe",
    "href": "fall_sta235_2025/week_01/week_07.html#a-shortcut-for-calculating-rmspe",
    "title": "Data Science for Business Applications",
    "section": "A shortcut for calculating RMSPE",
    "text": "A shortcut for calculating RMSPE\n\n# exp here only because we're predicting log(Revenue) instead of Revenue\npredictions &lt;- exp(predict(model4, test))\nactuals &lt;- test$Revenue\nsqrt(mean((predictions - actuals)^2))\n\n[1] 7.328273"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#traintest-splits-to-measure-performance",
    "href": "fall_sta235_2025/week_01/week_07.html#traintest-splits-to-measure-performance",
    "title": "Data Science for Business Applications",
    "section": "Train/test splits to measure performance",
    "text": "Train/test splits to measure performance\n\n\n\n\n\nRandomly allocating observations makes training/testing sets representative of the full data set\n80/20% training/testing splits are common\nMore testing data \\(\\to\\) More accurate estimates of prediction error\nMore training data \\(\\to\\) Better representation of full-data fit"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#predicting-utility-expenditures",
    "href": "fall_sta235_2025/week_01/week_07.html#predicting-utility-expenditures",
    "title": "Data Science for Business Applications",
    "section": "Predicting utility expenditures",
    "text": "Predicting utility expenditures"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#creating-a-traintest-split",
    "href": "fall_sta235_2025/week_01/week_07.html#creating-a-traintest-split",
    "title": "Data Science for Business Applications",
    "section": "Creating a train/test split",
    "text": "Creating a train/test split\nLet’s do the same thing for the utilities data, but by randomly splitting the data into a training and test set.\n\nlibrary(tidyverse)\nset.seed(9529)\n\n# Ask R to shuffle all 117 observations\nrandomized &lt;- slice_sample(utilities, n = 117)\n\n# 24 is about 20% of 117, so we'll use the first 24 rows as our test set\nutilities_test &lt;- randomized[1:24,]\n\n# The rest are our training set\nutilities_train &lt;- randomized[25:117,]\n\nThe set.seed(9529) command ensures reproducible “random” splits. (I chose this number because it’s my EID number; use your EID number!)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#creating-a-traintest-split-1",
    "href": "fall_sta235_2025/week_01/week_07.html#creating-a-traintest-split-1",
    "title": "Data Science for Business Applications",
    "section": "Creating a train/test split",
    "text": "Creating a train/test split\nBlack = training data, Red = testing data\nRandom splitting is important! (What would happen if we trained on fall-spring and tested on summer?)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#using-trainingtest-splits-to-pick-between-competing-models",
    "href": "fall_sta235_2025/week_01/week_07.html#using-trainingtest-splits-to-pick-between-competing-models",
    "title": "Data Science for Business Applications",
    "section": "Using training/test splits to pick between competing models",
    "text": "Using training/test splits to pick between competing models\nWhen comparing models, we want to pick the one that minimizes prediction error on the test set — we want to avoid overfitting to the training data!\nWhen we fit to the entire data set, we might overfit to the training data and get an overly optimistic estimate of prediction error on new data."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#a-linear-1st-degree-polynomial-model",
    "href": "fall_sta235_2025/week_01/week_07.html#a-linear-1st-degree-polynomial-model",
    "title": "Data Science for Business Applications",
    "section": "A linear (1st-degree) polynomial model",
    "text": "A linear (1st-degree) polynomial model\n\n\n\nmodel1 &lt;- lm(dailyspend ~ temp, data=utilities)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59   &lt;2e-16 ***\ntemp        -0.096432   0.003911  -24.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#a-quadratic-2nd-degree-polynomial-model",
    "href": "fall_sta235_2025/week_01/week_07.html#a-quadratic-2nd-degree-polynomial-model",
    "title": "Data Science for Business Applications",
    "section": "A quadratic (2nd-degree) polynomial model",
    "text": "A quadratic (2nd-degree) polynomial model\n\n\n\nmodel2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239      &lt; 2e-16 ***\ntemp        -0.2115553  0.0191046 -11.074      &lt; 2e-16 ***\nI(temp^2)    0.0012476  0.0002037   6.124 0.0000000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#a-9th-degree-polynomial-model",
    "href": "fall_sta235_2025/week_01/week_07.html#a-9th-degree-polynomial-model",
    "title": "Data Science for Business Applications",
    "section": "A 9th-degree polynomial model",
    "text": "A 9th-degree polynomial model\n\n\n\nmodel9 &lt;- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) \n              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),\n              data=utilities)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) + \n    I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9), \n    data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.66386 -0.24156 -0.01734  0.25996  2.48031 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -2.144e+01  4.599e+01  -0.466    0.642\ntemp         8.103e+00  1.460e+01   0.555    0.580\nI(temp^2)   -8.468e-01  1.905e+00  -0.445    0.658\nI(temp^3)    4.153e-02  1.349e-01   0.308    0.759\nI(temp^4)   -9.600e-04  5.764e-03  -0.167    0.868\nI(temp^5)    4.563e-06  1.551e-04   0.029    0.977\nI(temp^6)    2.603e-07  2.644e-06   0.098    0.922\nI(temp^7)   -5.943e-09  2.770e-08  -0.215    0.831\nI(temp^8)    5.173e-11  1.627e-10   0.318    0.751\nI(temp^9)   -1.676e-13  4.095e-13  -0.409    0.683\n\nResidual standard error: 0.7501 on 107 degrees of freedom\nMultiple R-squared:  0.889, Adjusted R-squared:  0.8797 \nF-statistic: 95.26 on 9 and 107 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#building-the-models",
    "href": "fall_sta235_2025/week_01/week_07.html#building-the-models",
    "title": "Data Science for Business Applications",
    "section": "Building the models",
    "text": "Building the models\nWe can avoid overfitting to the training data by fitting the models to the training data and then testing them on the test data.\nFit the models using the training set:\n\nmodel1 &lt;- lm(dailyspend ~ temp, data=utilities_train)\nmodel2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities_train)\nmodel9 &lt;- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) \n              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),\n              data=utilities_train)\n\nGet their predictions on the test set:\n\nactuals &lt;- utilities_test$dailyspend\npredictions1 &lt;- predict(model1, utilities_test)\npredictions2 &lt;- predict(model2, utilities_test)\npredictions9 &lt;- predict(model9, utilities_test)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#measuring-performance",
    "href": "fall_sta235_2025/week_01/week_07.html#measuring-performance",
    "title": "Data Science for Business Applications",
    "section": "Measuring performance",
    "text": "Measuring performance\nNow let’s calculate RMSPE on the testing data:\n\nsqrt(mean((actuals - predictions1)^2))\n\n[1] 1.013597\n\nsqrt(mean((actuals - predictions2)^2))\n\n[1] 0.8534912\n\nsqrt(mean((actuals - predictions9)^2))\n\n[1] 0.9482653\n\n\n\n\n\n\n\n\n\n\n\nModel\nRSE (all data)\nRMSPE (training/test split)\n\n\n\n\nLinear\n\\(0.83\\)\n\\(1.01\\)\n\n\nQuadratic\n\\(0.73\\)\n\\(0.85\\)\n\n\n9th Degree\n\\(0.71\\)\n\\(0.95\\)\n\n\n\n\n\n\nThese are all worse than the predictions of the models fit to the entire data set!\nIt shows that the quadratic model is the best of the three for making predictions on new data"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#the-problem-with-this-approach",
    "href": "fall_sta235_2025/week_01/week_07.html#the-problem-with-this-approach",
    "title": "Data Science for Business Applications",
    "section": "The problem with this approach",
    "text": "The problem with this approach\n\nWe might get “unlucky,” and our train/test split might not give us a representative estimate of prediction error!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#k-fold-cross-validation",
    "href": "fall_sta235_2025/week_01/week_07.html#k-fold-cross-validation",
    "title": "Data Science for Business Applications",
    "section": "\\(k\\)-fold Cross-Validation",
    "text": "\\(k\\)-fold Cross-Validation\n\n\n\n\nAverages results over \\(k\\) random train/test splits, to avoid getting bad results from possible bad luck of a single train/test split.\n\\(k=n\\) is called leave-one-out cross validation"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#lets-work-through-an-example-with-k3",
    "href": "fall_sta235_2025/week_01/week_07.html#lets-work-through-an-example-with-k3",
    "title": "Data Science for Business Applications",
    "section": "Let’s work through an example with \\(k=3\\)",
    "text": "Let’s work through an example with \\(k=3\\)\nFirst we need to split the data into 3 random groups of roughly the same size (\\(39\\times 3=117\\)):\n\nfold1 &lt;- randomized[1:39,]\nfold2 &lt;- randomized[40:78,]\nfold3 &lt;- randomized[79:117,]\n\n\nNow we need to build the model 3 times:\n\nModel A: test on fold1, train on fold2 and fold3\nModel B: test on fold2, train on fold1 and fold3\nModel C: test on fold3, train on fold1 and fold2"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#building-the-models-1",
    "href": "fall_sta235_2025/week_01/week_07.html#building-the-models-1",
    "title": "Data Science for Business Applications",
    "section": "Building the models",
    "text": "Building the models\n\nmodelA &lt;- lm(dailyspend ~ temp, data=rbind(fold2, fold3))\nmodelB &lt;- lm(dailyspend ~ temp, data=rbind(fold1, fold3))\nmodelC &lt;- lm(dailyspend ~ temp, data=rbind(fold1, fold2))"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#getting-the-predictions",
    "href": "fall_sta235_2025/week_01/week_07.html#getting-the-predictions",
    "title": "Data Science for Business Applications",
    "section": "Getting the predictions",
    "text": "Getting the predictions\n\npredictionsA &lt;- predict(modelA, fold1)\npredictionsB &lt;- predict(modelB, fold2)\npredictionsC &lt;- predict(modelC, fold3)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#calculating-rmspe-1",
    "href": "fall_sta235_2025/week_01/week_07.html#calculating-rmspe-1",
    "title": "Data Science for Business Applications",
    "section": "Calculating RMSPE",
    "text": "Calculating RMSPE\nWe now get 3 different RMSPE estimates:\n\nactualsA &lt;- fold1$dailyspend\nsqrt(mean((actualsA - predictionsA)^2))\n\n[1] 0.9898423\n\nactualsB &lt;- fold2$dailyspend\nsqrt(mean((actualsB - predictionsB)^2))\n\n[1] 0.745035\n\nactualsC &lt;- fold3$dailyspend\nsqrt(mean((actualsC - predictionsC)^2))\n\n[1] 0.8463284\n\n\nOur final estimate of RMSPE is the average of these 3 estimates:\n\\[\n  \\text{RMSPE} = \\frac{ 0.99 + 0.75 + 0.85 }{3} = 0.86\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#re-doing-our-model-comparisons",
    "href": "fall_sta235_2025/week_01/week_07.html#re-doing-our-model-comparisons",
    "title": "Data Science for Business Applications",
    "section": "Re-doing our model comparisons",
    "text": "Re-doing our model comparisons\n\n\n\n\n\n\n\n\n\nModel\nRSE (all data)\nRMSPE (3 Fold CV)\nRMSPE (Single Train/Test Split)\n\n\n\n\nLinear\n\\(0.83\\)\n\\(0.86\\)\n\\(1.01\\)\n\n\nQuadratic\n\\(0.73\\)\n\\(0.77\\)\n\\(0.85\\)\n\n\n9th Degree\n\\(0.71\\)\n\\(0.91\\)\n\\(0.95\\)\n\n\n\n\nComparing RSE and CV:\n\nRSE misses overfitting\nFor every model, RSE is too optimistic about how well the model will perform on new data\n\nOur initial train/test split was unlucky! It over-estimated the RMSE"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_07.html#isnt-there-a-faster-way",
    "href": "fall_sta235_2025/week_01/week_07.html#isnt-there-a-faster-way",
    "title": "Data Science for Business Applications",
    "section": "Isn’t there a faster way?",
    "text": "Isn’t there a faster way?\nIn practice, you don’t have to do this all manually! The tidymodels package makes it easy to do cross-validation even for larger \\(k\\).\n\nlibrary(tidymodels)\n\n# Randomly split the data into 5 folds\nfolds &lt;- vfold_cv(utilities, v=5)\n\n# Build a tidymodels \"workflow\" for a linear model\n# with the formula dailyspend ~ temp, using RMSE as the target metric\nworkflow() %&gt;% \n  add_model(linear_reg()) %&gt;%\n  add_formula(dailyspend ~ temp) %&gt;%\n  fit_resamples(folds, metrics=metric_set(rmse)) %&gt;%\n  collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   0.871     5  0.0857 pre0_mod0_post0"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#quick-review-of-our-class",
    "href": "fall_sta235_2025/week_01/week_08.html#quick-review-of-our-class",
    "title": "Data Science for Business Applications",
    "section": "Quick review of our Class",
    "text": "Quick review of our Class\nThis is what we covered in previous classes:\n\nSimple and Multiple Regression\nCategorical Variables and Interactions\nResidual Analysis\nTime Series\nModel Selection\n\nToday we will introduce a new model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#the-okcupid-data-set",
    "href": "fall_sta235_2025/week_01/week_08.html#the-okcupid-data-set",
    "title": "Data Science for Business Applications",
    "section": "The OkCupid data set",
    "text": "The OkCupid data set\n\n\n\nThe OkCupid data set contains information about 59826 profiles from users of the OkCupid online dating service.\nWe have data on user age, height, sex, income , sexual orientation, education level]{style=“color:darkorange;”}, body type , ethnicity, and more.\nLet’s see if we can predict the sex of the user based on their height. (In this data set, everyone is classified as male or female.)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#lets-build-the-model",
    "href": "fall_sta235_2025/week_01/week_08.html#lets-build-the-model",
    "title": "Data Science for Business Applications",
    "section": "Let’s build the model",
    "text": "Let’s build the model\n\nWhat’s wrong with this regression?\n\n\\[\n\\widehat{\\text{sex}} = \\widehat{\\beta}_{0} + \\widehat{\\beta}_{0} \\cdot \\text{height}\n\\]\n\nThe \\(Y\\) variable here is categorical (two levels—everyone in this data set is either labeled male or female), so regular linear regression won’t work here.\nBut what if we just do it anyway?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#binary-variable",
    "href": "fall_sta235_2025/week_01/week_08.html#binary-variable",
    "title": "Data Science for Business Applications",
    "section": "Binary Variable",
    "text": "Binary Variable\n\nLet’s first create a dummy variable male to convert sex to a quantitative dummy variable:\n\n\nlibrary(tidyverse)\nokcupid = okcupid %&gt;% \n  mutate(male = ifelse(okcupid$sex == \"m\", 1, 0))\n\n\nWe could do this with 1 representing either male or female (it wouldn’t matter)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#regular-linear-regression",
    "href": "fall_sta235_2025/week_01/week_08.html#regular-linear-regression",
    "title": "Data Science for Business Applications",
    "section": "Regular Linear Regression",
    "text": "Regular Linear Regression\n\nggplot(okcupid, aes(x=height, y = male)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", se = F)\n\n\n\nA line is not a great fit to this data—it’s not even close to linear. And what does it mean to predict that male = 0.7 (or 1.2)?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#logistic-regression",
    "href": "fall_sta235_2025/week_01/week_08.html#logistic-regression",
    "title": "Data Science for Business Applications",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nInstead of predicting whether someone is male, let’s predict the probability that they are male\nIn logistic regression, one level of \\(Y\\) is always called “success” and the other called “failure.” Since \\(Y = 1\\) for males, in our setup we have designated males as “success.” (You could also set \\(Y = 1\\) for females and call females “success.”)\nLet’s fit a curve that is always between 0 and 1."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#odds-and-probabilty",
    "href": "fall_sta235_2025/week_01/week_08.html#odds-and-probabilty",
    "title": "Data Science for Business Applications",
    "section": "Odds and Probabilty",
    "text": "Odds and Probabilty\n\nTo fit the Logistic regression model we need to know the difference between odds and probability and how they relate.\nWhen something has “even (1/1) odds,” the probability of success is 1/2.\nWhen something has “2/1 odds,” the probability of success is 2/3.\nWhen something has “3/2 odds,” the probability of success is 3/5.\nIn general, the odds of something happening are \\(p/(1 − p)\\).\nWhere \\(p\\) is the probability defined bewteen zero and one.\nYou can transform odds to probability: \\[\n\\text{Odds} = \\frac{3}{2} = \\frac{3/(3+2)}{2/(3+2)} = \\frac{3/5}{2/5}  = \\frac{p}{1-p}\n\\]\nIf the odds are between zero and one they are not in your favor, \\((1-p)&gt;p\\)\nLet’s the explore this relation!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#probability-vs-odds-vs-log-odds",
    "href": "fall_sta235_2025/week_01/week_08.html#probability-vs-odds-vs-log-odds",
    "title": "Data Science for Business Applications",
    "section": "Probability vs odds vs log odds",
    "text": "Probability vs odds vs log odds\n\n\n\nProbability \\(p\\)\nOdds \\(p/(1 − p)\\)\nLog odds \\(\\log(p/(1 − p))\\)\n\n\n\n\n0\n0\n\\(-\\infty\\)\n\n\n0.25\n0.33\n−1.10\n\n\n0.5\n1\n0\n\n\n0.75\n3\n1.10\n\n\n0.8\n4\n1.39\n\n\n0.9\n9\n2.20\n\n\n0.95\n19\n2.94\n\n\n1\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\n\nProbability is between zero and one.\nOdds are strictly positive (greater than zero).\nLog odds ranges the whole real line."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#the-logistic-regression-model",
    "href": "fall_sta235_2025/week_01/week_08.html#the-logistic-regression-model",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nLogistic regression models the log odds of success \\(p\\) as a linear function of \\(X\\): \\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot X + e\n\\]\nThis fits an “S-shaped” curve to the data\nWe’ll see what it looks like later\nBy making this choice, we have a series of benefits.\nLet’s try it!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#the-logistic-regression-model-1",
    "href": "fall_sta235_2025/week_01/week_08.html#the-logistic-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nWe need a different function - glm() (generalized linear models)\n\n\nmodel &lt;- glm(male ~ height, data = okcupid, family = binomial)\nsummary(model)\n\n\nCall:\nglm(formula = male ~ height, family = binomial, data = okcupid)\n\nCoefficients:\n              Estimate Std. Error z value            Pr(&gt;|z|)    \n(Intercept) -44.448609   0.357510  -124.3 &lt;0.0000000000000002 ***\nheight        0.661904   0.005293   125.1 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 80654  on 59825  degrees of freedom\nResidual deviance: 44637  on 59824  degrees of freedom\nAIC: 44641\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nHow ca we interpret this model?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#the-logistic-regression-model-2",
    "href": "fall_sta235_2025/week_01/week_08.html#the-logistic-regression-model-2",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nThe logistic regression output tells us that our prediction is \\[\n\\log(\\text{odds}) = \\log\\left(\\frac{\\widehat{p(\\text{male})}}{1-\\widehat{p(\\text{male})}}\\right) = −44.45 + 0.66 \\cdot \\text{height}\n\\]\nTo get the probability we have to solve in terms \\(\\widehat{p(\\text{male})}\\)\nThe probability of being male given height: \\[\n\\widehat{p(\\text{male})} = \\frac{\\exp(−44.45 + 0.66 \\cdot \\text{height})}{1+ \\exp(−44.45 + 0.66 \\cdot \\text{height})}\n\\] where \\(\\exp()\\) is the exponential function \\(e^x\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#lets-show-this",
    "href": "fall_sta235_2025/week_01/week_08.html#lets-show-this",
    "title": "Data Science for Business Applications",
    "section": "Let’s show this",
    "text": "Let’s show this\nLet \\(\\widehat{p} = \\widehat{p(\\text{male})}\\), and \\(\\exp(X\\widehat{\\beta}) = \\exp(−44.45 + 0.66 \\cdot \\text{height})\\):\n\\[\n\\begin{eqnarray}\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right) &=& X\\widehat{\\beta} \\\\\n\\exp\\left(\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\\right) &=& \\exp(X\\widehat{\\beta}) \\\\\n\\frac{\\widehat{p}}{1-\\widehat{p}} &=& \\exp(X\\widehat{\\beta})\\\\\n\\widehat{p} &=& \\exp(X\\widehat{\\beta})\\cdot (1-\\widehat{p})\\\\\n\\widehat{p} &=&\\exp(X\\widehat{\\beta}) - \\exp(X\\widehat{\\beta}) \\cdot \\widehat{p} \\\\\n\\widehat{p} &=& \\frac{\\exp(X\\widehat{\\beta})}{1 + \\exp(X\\widehat{\\beta})}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#visualizing-the-model",
    "href": "fall_sta235_2025/week_01/week_08.html#visualizing-the-model",
    "title": "Data Science for Business Applications",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\n\nHow to interpret this curve?\nThe blue line is \\(\\widehat{p(\\text{male})}\\), given the height."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#interpreting-the-coefficients",
    "href": "fall_sta235_2025/week_01/week_08.html#interpreting-the-coefficients",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\nOur prediction equation is:\n\\[\n\\log(\\text{odds}) = \\log\\left(\\frac{\\widehat{p(\\text{male})}}{1-\\widehat{p(\\text{male})}}\\right) = −44.45 + 0.66 \\cdot \\text{height}\n\\]\nLet’s start with some basic, but not particularly useful, interpretations:\n\nWhen height = 0, we predict that the log odds will be -44.45 , so the probability of male is predicted to be very close to 0%.\nWhen height increases by 1 inch, we predict that the log odds of being male will increase by 0.66.\nInstead of log odds is better to have the interpretation in odds."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#interpreting-the-coefficients-1",
    "href": "fall_sta235_2025/week_01/week_08.html#interpreting-the-coefficients-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nLet’s rewrite the prediction equation as:\nPredicted odds of male, \\(\\exp(−44.45 + 0.66 \\cdot \\text{height})\\).\nIncreasing height by 1 inch will multiply the odds by \\(\\exp(0.66) = 1.94\\); i.e., increase the odds by 94%.\nIn summary, \\[(\\exp(\\widehat{\\beta}) - 1)\\times 100 = \\text{percentage change in odds}.\\]\nIncreasing height by 2 inches will multiply the odds by \\(\\exp(2\\cdot0.66) = 3.76\\); i.e., increase the odds by 276%.\nOdds equal to 1 indicate an one-to-one chance."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#making-predictions",
    "href": "fall_sta235_2025/week_01/week_08.html#making-predictions",
    "title": "Data Science for Business Applications",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is the probability of being male given we have a height of 69.\n\n\npredict(model, list(height=69), type=\"response\")\n\n        1 \n0.7725447 \n\n\n\nUsing the probability equation in R:\n\n\\[\n\\widehat{p(\\text{male})} = \\frac{\\exp(−44.45 + 0.66 \\cdot 69)}{1+ \\exp(−44.45 + 0.66 \\cdot 69)} = 0.77\n\\]\n\nexp(-44.448609 + 0.661904*69)/(1+exp(-44.45 + 0.66*69))\n\n[1] 0.8546399"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#adding-more-predictors",
    "href": "fall_sta235_2025/week_01/week_08.html#adding-more-predictors",
    "title": "Data Science for Business Applications",
    "section": "Adding more predictors",
    "text": "Adding more predictors\n\nAdding another predictor: can we do better?\nJust like with a linear regression model, we can add additional predictors to the model.\nOur interpretation of the coefficients in multiple logistic regression is similar to multiple linear regression, in the sense that each coefficient represents the predicted effect of one \\(X\\) on \\(Y\\), holding the other \\(X\\) variables constant."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#how-good-is-our-model",
    "href": "fall_sta235_2025/week_01/week_08.html#how-good-is-our-model",
    "title": "Data Science for Business Applications",
    "section": "How good is our model?",
    "text": "How good is our model?\n\nUnfortunately, the typical root mean squared error, RSE metric isn’t available for logistic regression.\nHowever, there are many metrics that indicate model fit.\nBut: most of these metrics are difficult to interpret, so we’ll focus on something simpler to interpret and communicate."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#accuracy-of-the-model",
    "href": "fall_sta235_2025/week_01/week_08.html#accuracy-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\n\nWe could use our model to make a prediction of sex based on the probability.\nSuppose we say that our prediction is: \\[\n\\text{Prediction} = \\begin{cases}\n\\text{male}, & \\text{if $\\widehat{P(\\text{male})} \\geq 0.5$}, \\\\\n\\text{female}, & \\text{if $\\widehat{P(\\text{male})} &lt; 0.5$}. \\\\\n\\end{cases}\n\\]\nGiven a threshold of 0.5, now we can compute the fraction of individuals whose sex we correctly predicted.\nFor males and females.\nThis is known as the accuracy of the model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#accuracy-of-the-model-1",
    "href": "fall_sta235_2025/week_01/week_08.html#accuracy-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\n\nWe can use the xtabs function to get the accuracy:\nWe add we the number of correctly predicted groups for both male and female, and divide by the total of observations.\n\n\nokcupid = okcupid %&gt;% \n  mutate(predict.sex = ifelse(predict(model, type=\"response\") &gt;= 0.5,\"m\",\"f\"))\nxtabs(~ predict.sex + sex,okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nCorrectly predicted that is female - 19466\nCorrectly predicted that is male - 30243\nTotal number of individuals in the sample - 59826\nThe accuracy is (19466 + 30243)/59826 = 0.831, or 83%"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#confusion-matrix",
    "href": "fall_sta235_2025/week_01/week_08.html#confusion-matrix",
    "title": "Data Science for Business Applications",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\nThe table from xtabs is also called a Confusion matrix\n\n\n\n\n\nActual failure\nActual success\n\n\n\n\nModel predicts failure\nTrue negative\nFalse negative\n\n\nModel predicts success\nFalse positive\nTrue positive\n\n\n\n\nTrue positives: predicting male for someone that is male\nTrue negatives: predicting female for someone that is female\nFalse positives: predicting male for someone that is female\nFalse negatives: predicting female for someone that is male\nIf we had designated female as 1 and male as 0, these would have switched\nSo Accuracy = (True negative + True positive)/(Total cases)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#accuracy-of-the-model-2",
    "href": "fall_sta235_2025/week_01/week_08.html#accuracy-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Accuracy of the model",
    "text": "Accuracy of the model\nSuppose that the Amazon is trying to build a model to predict which costumers buy a certain product:\n\nSuppose that 0.01% of people are costumers of this product (the product is really expensive / high revenue)\nA “null” or “no-brainer” model that predicts that no one is a costumer will be 99.99% accurate.\nThe revenue coming from our model would be zero.\nBut the model could make two different kinds of prediction errors:\nFalse positive: predicting someone is a customer when they really are not\nFalse negative: predicting someone is not a customer when they really are\nThese two measures give us a better idea of the predictive power of our model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#false-positive-rate",
    "href": "fall_sta235_2025/week_01/week_08.html#false-positive-rate",
    "title": "Data Science for Business Applications",
    "section": "False positive rate",
    "text": "False positive rate\nThe false positive rate is the proportion of actual failures where the model predicted success.\n\nxtabs(~ predict.sex + sex, okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nFalse Positives - predicting someone is a male when they really are female\nActual failure - number of cases that are female\nFalse positive rate = False positives/ Actual failure\nIn our model, the false positive rate is 4623/24089 = 0.19"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#false-negative-rate",
    "href": "fall_sta235_2025/week_01/week_08.html#false-negative-rate",
    "title": "Data Science for Business Applications",
    "section": "False negative rate",
    "text": "False negative rate\nThe false negative rate is the proportion of actual successes where the model predicted failure.\n\nxtabs(~ predict.sex + sex, okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   19466  5494 24960\n        m    4623 30243 34866\n        Sum 24089 35737 59826\n\n\n\nFalse Negatives - predicting someone is a female when they really are male\nActual success - number of cases that are male\nFalse negative rate = False negatives/ Actual success\nIn our model, the false positive rate is 5494/35737 = 0.15"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#changing-rates",
    "href": "fall_sta235_2025/week_01/week_08.html#changing-rates",
    "title": "Data Science for Business Applications",
    "section": "Changing rates",
    "text": "Changing rates\nHow do we reduce false positive/negative rates?\n\nInstead of using 50% as a cutoff probability to decide when to predict success, use a higher (or lower) probability.\nFor example, we could have the model predict that someone is male only if \\(\\widehat{p(\\text{male})} \\geq 0.8\\), instead of 0.5:\n\n\nokcupid = okcupid %&gt;% \n  mutate(predict.sex = ifelse(predict(model, type=\"response\") &gt;= 0.8,\"m\",\"f\"))\nxtabs(~ predict.sex + sex,okcupid) %&gt;%\n  addmargins()\n\n           sex\npredict.sex     f     m   Sum\n        f   22650 12938 35588\n        m    1439 22799 24238\n        Sum 24089 35737 59826\n\n\n\nAccuracy = (21425+26753)/59826 = 0.76\nFalse positive rate = 2664/24089 = 0.06\nFalse negative rate = 8984/59826 = 0.36"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#prediction-trade-off",
    "href": "fall_sta235_2025/week_01/week_08.html#prediction-trade-off",
    "title": "Data Science for Business Applications",
    "section": "Prediction Trade off",
    "text": "Prediction Trade off\n\nWe can decrease the false positive rate, but at the expense of increasing the false negative rate.\nOr we can decrease the false negative rate, but at the expense of increasing the false positive rate.\nWe might choose a cutoff probability other than 50% based on our assessment of the relative costs of the two different kinds of errors."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_08.html#summary",
    "href": "fall_sta235_2025/week_01/week_08.html#summary",
    "title": "Data Science for Business Applications",
    "section": "Summary",
    "text": "Summary\nIn a logistic regression model, the response variable is binary, taking values of either zero or one.\n\nThe model estimates the log odds of the event associated with a response of one.\nThe model’s effects are interpreted in terms of odds.\nPredictions are expressed as probabilities.\nThe performance of the model is evaluated based on its prediction accuracy."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#decision-trees",
    "href": "fall_sta235_2025/week_01/week_10.html#decision-trees",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Another predictive model\nDecision Trees: Classification and Regression Trees (CART)\nTrees are:\n\nFlexible at capturing non-linearity and interactions\nDon’t require scaling of variables\nHandle categorical and numerical data\nFast\nInterpretable"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#the-idea-behind-decision-trees",
    "href": "fall_sta235_2025/week_01/week_10.html#the-idea-behind-decision-trees",
    "title": "Data Science for Business Applications",
    "section": "The Idea Behind Decision Trees",
    "text": "The Idea Behind Decision Trees\n\nCreate a flow chart for making decisions\n\nHow do we classify an individual?\n\nBut there are many decisions!\n\nHow many variables do we use?\nHow do we sort them? In what order do we place them?\nHow do we split them?\nHow deep do we go?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#the-idea-behind-decision-trees-1",
    "href": "fall_sta235_2025/week_01/week_10.html#the-idea-behind-decision-trees-1",
    "title": "Data Science for Business Applications",
    "section": "The Idea Behind Decision Trees",
    "text": "The Idea Behind Decision Trees\n\n\n\nThe decision tree is always binary\nSplits the data in two parts given a decision rule\nStructure:\n\nRoot node (decision rule)\nInternal nodes (decision rule)\nLeaves (prediction)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#types-of-decision-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#types-of-decision-tree",
    "title": "Data Science for Business Applications",
    "section": "Types of Decision Tree",
    "text": "Types of Decision Tree\n\nWe can use decision trees to perform:\n\nClassification tasks (target variable is categorical)\n\nSimilar to logistic regression\n\nRegression (target variable is numerical)\n\nWorks well on nonlinear data"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#creating-a-decision-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#creating-a-decision-tree",
    "title": "Data Science for Business Applications",
    "section": "Creating a Decision Tree",
    "text": "Creating a Decision Tree\n\nDetermine which variable and criteria we can use to split the data into two groups (binary) so that the two parts of the data are as different as possible from each other\nWithin each group, repeat step 1\nStop this process when we run out of variables, or splitting no longer helps us make better predictions"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#creating-a-decision-tree-basically",
    "href": "fall_sta235_2025/week_01/week_10.html#creating-a-decision-tree-basically",
    "title": "Data Science for Business Applications",
    "section": "Creating a Decision Tree (Basically)",
    "text": "Creating a Decision Tree (Basically)\n\nStart at the root node\nSplit by a variable that provides the most differentiation\nStop splitting if you get pure leaves or pure enough (one class)\nRepeat for each node\nAssign the majority classification (or average outcome in regression)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#example---disney",
    "href": "fall_sta235_2025/week_01/week_10.html#example---disney",
    "title": "Data Science for Business Applications",
    "section": "Example - Disney+",
    "text": "Example - Disney+\n\nDisney+ data:\n\ncity: Whether the customer lives in a big city or not\nfemale: Whether the customer is female or not\nage: Customer’s age (in years)\nlogins: Number of logins to the platform in the past week\nmandalorian: Whether the person has watched the Mandalorian or not\nunsubscribe: Whether they canceled their subscription or not\n\nLet’s try to predict who will cancel their subscription"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#predicting-subscription",
    "href": "fall_sta235_2025/week_01/week_10.html#predicting-subscription",
    "title": "Data Science for Business Applications",
    "section": "Predicting subscription",
    "text": "Predicting subscription\n\nSince our outcome is binary, this is a classification task\nWe’ll start with two variables, city and mandalorian\nHere’s a frequency table:\n\n\nunsub_df = disney %&gt;%\n  filter(unsubscribe == \"unsubscribe\")\n\nxtabs(~mandalorian + city, data = unsub_df) %&gt;% \n  addmargins()\n\n           city\nmandalorian   no  yes  Sum\n        no    22  152  174\n        yes  183  995 1178\n        Sum  205 1147 1352\n\nstay_df = disney %&gt;%\n  filter(unsubscribe == \"stay\")\n\nxtabs(~mandalorian + city, data = stay_df) %&gt;% \n  addmargins()\n\n           city\nmandalorian   no  yes  Sum\n        no   190 1101 1291\n        yes  317 2040 2357\n        Sum  507 3141 3648"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#section",
    "href": "fall_sta235_2025/week_01/week_10.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "For mandalorian:\n\n\n\n\nMandalorian\nSubscribers\nUnsubscribers\nTotal\n% Unsubscribed\n\n\n\n\nno\n1291\n174\n1465\n11.9%\n\n\nyes\n2357\n1178\n3535\n33.3%\n\n\n\n\nThere’s a big difference in unsubscribe rates between those who watched and didn’t watch The Mandalorian.\nThis suggests mandalorian is a strong predictor of unsubscribe.\nFor city:\n\n\n\n\nCity\nSubscribers\nUnsubscribers\nTotal\n% Unsubscribed\n\n\n\n\nno\n507\n205\n712\n28.8%\n\n\nyes\n3141\n1147\n4288\n26.7%\n\n\n\n\nThere’s also a difference, but it’s smaller than for mandalorian."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#section-1",
    "href": "fall_sta235_2025/week_01/week_10.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The variable that most reduces impurity, that makes each resulting group more “pure” in terms of unsubscribe/stay gets chosen for the first split in this case mandalorian.\n\n\n\n\nWe can then classify based on the majority percentage in each leaf node\nIn this case very intersting sincethe prediction will always be subscribe"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#adding-city",
    "href": "fall_sta235_2025/week_01/week_10.html#adding-city",
    "title": "Data Science for Business Applications",
    "section": "Adding city",
    "text": "Adding city\n\nLet’s add city in both nodes\n\n\n\n\nCity\nType\nMandalorian No\nMandalorian Yes\n\n\n\n\nNo\nUnsubscribers\n22\n183\n\n\nNo\nSubscribers\n190\n317\n\n\nYes\nUnsubscribers\n152\n995\n\n\nYes\nSubscribers\n1101\n2040"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#results-in-the-follwing-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#results-in-the-follwing-tree",
    "title": "Data Science for Business Applications",
    "section": "Results in the follwing tree",
    "text": "Results in the follwing tree\n\n\nThis tree is not really good since it always predict subscription."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#example-with-data",
    "href": "fall_sta235_2025/week_01/week_10.html#example-with-data",
    "title": "Data Science for Business Applications",
    "section": "Example with data",
    "text": "Example with data\n\nCan We Build a Model to Predict Who Survived the Titanic?\nWe’ll make a classification tree to predict survival:\n\nadult: If the passenger was as or older than 18\n\nsex: passenger’s gender; male or female\npassengerClass: Class in which the passenger traveled: 1st, 2nd, 3rd\nsurvided: Indicates if the passenger survived or not: yes, no"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#build-the-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#build-the-tree",
    "title": "Data Science for Business Applications",
    "section": "Build the tree",
    "text": "Build the tree\n\nBefore we can build the tree we should define the categorical variables as factors.\nThis is not necessary when the variables are binary or dummy (0 or 1).\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rpart.plot)\n\ntitanic_age$survived = as.factor(titanic_age$survived)\ntitanic_age$sex = as.factor(titanic_age$sex)\ntitanic_age$passengerClass = as.factor(titanic_age$passengerClass)\ntitanic_age$adult = as.factor(titanic_age$adult)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#build-the-tree-1",
    "href": "fall_sta235_2025/week_01/week_10.html#build-the-tree-1",
    "title": "Data Science for Business Applications",
    "section": "Build the tree",
    "text": "Build the tree\n\n# Create a decision tree model specification\ntree_spec &lt;- decision_tree(mode = \"classification\",engine = \"rpart\")\n\n# Fit the model to the training data\ntree_fit &lt;- tree_spec %&gt;%\n  fit(survived ~ adult + sex + passengerClass, data = titanic_age)\n\n### Model Results\n# Print out the logic the decision tree decided on\nrules &lt;- rpart.rules(tree_fit$fit)\nprint(rules)\n\n survived                                                  \n     0.21 when sex is   male                               \n     0.47 when sex is female & passengerClass is        3rd\n     0.93 when sex is female & passengerClass is 1st or 2nd\n\n\n\nThis decision tree shows us that:\n\n0.21, or 21%, of the men on the Titanic survived\n0.47, or 47%, of the females in the third class survived\n0.93, or 93%, of the females in the 1st or 2nd survived\n\nIn some ways the tree returns the probability of surviving given the conditions displayed on the decision rules."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#plot-the-decision-rules-and-classification",
    "href": "fall_sta235_2025/week_01/week_10.html#plot-the-decision-rules-and-classification",
    "title": "Data Science for Business Applications",
    "section": "Plot the decision rules and classification",
    "text": "Plot the decision rules and classification\n\n# Plot the decision tree\nrpart.plot(tree_fit$fit, type = 4, extra = 101, \n           under = TRUE, cex = 0.8, box.palette = \"auto\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#how-to-interpret-the-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#how-to-interpret-the-tree",
    "title": "Data Science for Business Applications",
    "section": "How to interpret the tree",
    "text": "How to interpret the tree\n\nThe nodes indicates which group of the target variable is more common in the split. If it indicates no it means that there’s a higher chance of not surviving than surviving.\nSo in the top node we have no and it indicates that from a total of 1046 (100%), 619 survived, and 427 did not.\nWe then use the first decision rule that splits the data into gender. It indicates that 63% (658) of the passengers are male and 37% (388) female. From those who are male, 523 died and 135 survived; that is 135/658 = 0.21, or 21%. In this case 523/658 = 0.79, or 79%, died. That’s the reason the node is no on the node.\nFrom the 388 females, 152 were in the 3rd class, from which 80/152 = 0.53, or 53%, died, and 72/152 = 0.47, or 47%, survived. Of the female passengers in the 3rd class, 16/236 = 0.067, or 6.7%, died, and 220/236 = 0.93, or 93%, survived. In this case we can see that the node indicates an yes.\nOverall the female node indicates that 96/388 = 24% of the female passengers died and 292/388 = 75% survived."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#using-the-model-to-make-predictions",
    "href": "fall_sta235_2025/week_01/week_10.html#using-the-model-to-make-predictions",
    "title": "Data Science for Business Applications",
    "section": "Using the model to make predictions",
    "text": "Using the model to make predictions\n\n# Make predictions on the testing data\npredictions &lt;- tree_fit %&gt;%\n  predict(titanic_age) %&gt;%\n  pull(.pred_class)\n\ntitanic_age = titanic_age %&gt;% \n  mutate(pred.survival = predictions)\n\nxtabs(~pred.survival+survived,data=titanic_age) %&gt;% \n  addmargins()\n\n             survived\npred.survival   no  yes  Sum\n          no   603  207  810\n          yes   16  220  236\n          Sum  619  427 1046\n\n\n\nSince this is classification model we can measure the accuracy of the model\n\nTrue positives (TP) - 220\nTrue Negatives (TN) - 603\nFalse Positives (FP) - 16\nFalse Negatives (FN) - 207\n\nAccuracy is equal to (TP+TN)/Total = (220+603)/1046 = 0.78, or 78%"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#regression-trees",
    "href": "fall_sta235_2025/week_01/week_10.html#regression-trees",
    "title": "Data Science for Business Applications",
    "section": "Regression Trees",
    "text": "Regression Trees\n\nWe can also use decision trees for regression.\nWe’ll use our Boston data for predicting median home value (MEDV), measured in thousands of dollars, from a socioeconomic indicator LSTAT that was defined as the proportion of adults without some high school education and proportion of male workers classified as laborers). There are 506 observations in this data.\nIn this case, both the target variable and the predictor are numerical, and the nodes will show the average value of this variable (average of the median home value).\nThe decision on the node (rule) of the tree will now depend on a numerical cutoff determined by the data, which will split the predictor into areas from which a prediction will be made by obtaining the average of the response in this area."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#running-the-model",
    "href": "fall_sta235_2025/week_01/week_10.html#running-the-model",
    "title": "Data Science for Business Applications",
    "section": "Running the model",
    "text": "Running the model\n\n# Make predictions on the testing data\n# Create a decision tree model specification\ntree_spec &lt;- decision_tree(mode = \"regression\",engine = \"rpart\")\n\n# Fit the model to the training data\ntree_fit &lt;- tree_spec %&gt;%\n  fit(MEDV ~ LSTAT, data = tree_housing)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#regression-tree-plot",
    "href": "fall_sta235_2025/week_01/week_10.html#regression-tree-plot",
    "title": "Data Science for Business Applications",
    "section": "Regression tree plot",
    "text": "Regression tree plot\n\nrpart.plot(tree_fit$fit, type = 4, extra = 101, \n           under = TRUE, cex = 0.8, box.palette = \"auto\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#regression-tree-interpretation",
    "href": "fall_sta235_2025/week_01/week_10.html#regression-tree-interpretation",
    "title": "Data Science for Business Applications",
    "section": "Regression tree interpretation",
    "text": "Regression tree interpretation\n\nWe start at the top, or root node of the tree. At each interior node we have a rule that tells us which split to take, based on the value of our predictor variable and the cutoff.\nWe keep going until we reach the bottom, or leaf node.\nWithin each partition, the target values are averaged to give us the prediction within that partition. These are also the values seen at the leaf nodes on the left\nTo predict a new point, we can just “drop” an observation down the tree until we end up at a leaf node."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#regression-tree-model",
    "href": "fall_sta235_2025/week_01/week_10.html#regression-tree-model",
    "title": "Data Science for Business Applications",
    "section": "Regression tree model",
    "text": "Regression tree model\n\nWhat does the regression tree look like applied to the data?\nIt is basically a step function in which the splits are determined by the tree structure."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#how-we-evaluate-the-regression-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#how-we-evaluate-the-regression-tree",
    "title": "Data Science for Business Applications",
    "section": "How we evaluate the regression tree",
    "text": "How we evaluate the regression tree\n\nWe can also split the data into testing and training, or use cross validation and obtain the RMSE.\nIn this case we’ll only obtain the RMSE and the R-squared of this model on the original data as\n\n\n# Make predictions on the testing data\npredictions &lt;- tree_fit %&gt;%\n  predict(tree_housing) %&gt;%\n  pull(.pred)\n\n# Calculate RMSE and R-squared\nmetrics &lt;- metric_set(rmse, rsq)\nmodel_performance &lt;- tree_housing %&gt;%\n  mutate(predictions = predictions) %&gt;%\n  metrics(truth = MEDV, estimate = predictions)\n\nprint(model_performance)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       5.07 \n2 rsq     standard       0.695"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#model-results",
    "href": "fall_sta235_2025/week_01/week_10.html#model-results",
    "title": "Data Science for Business Applications",
    "section": "Model Results",
    "text": "Model Results\n\nOur median home estimates will have a prediction error of +/- 5.071 thousand dollars.\nOur model captures 69.5% of the variation in age using the LSTAT variable."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#more-predictors-on-the-regression-tree",
    "href": "fall_sta235_2025/week_01/week_10.html#more-predictors-on-the-regression-tree",
    "title": "Data Science for Business Applications",
    "section": "More predictors on the regression tree",
    "text": "More predictors on the regression tree\n\nWhat if we had a tree with more than one explanatory variable?\nWe add distance to employment centers (DIS) to our model, and the tree can now use either LSTAT or DIS.\n\n\n# Make predictions on the testing data\n# Create a decision tree model specification\ntree_spec &lt;- decision_tree(mode = \"regression\",engine = \"rpart\")\n\n# Fit the model to the training data\ntree_fit &lt;- tree_spec %&gt;%\n  fit(MEDV ~ LSTAT + as.numeric(DIS), data = tree_housing)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#regression-tree-plot-1",
    "href": "fall_sta235_2025/week_01/week_10.html#regression-tree-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Regression tree plot",
    "text": "Regression tree plot\n\nrpart.plot(tree_fit$fit, type = 4, extra = 101, \n           under = TRUE, cex = 0.8, box.palette = \"auto\")"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#model-performance",
    "href": "fall_sta235_2025/week_01/week_10.html#model-performance",
    "title": "Data Science for Business Applications",
    "section": "Model performance",
    "text": "Model performance\n\n# Make predictions on the testing data\npredictions &lt;- tree_fit %&gt;%\n  predict(tree_housing) %&gt;%\n  pull(.pred)\n\n# Calculate RMSE and R-squared\nmetrics &lt;- metric_set(rmse, rsq)\nmodel_performance &lt;- tree_housing %&gt;%\n  mutate(predictions = predictions) %&gt;%\n  metrics(truth = MEDV, estimate = predictions)\n\nprint(model_performance)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       4.85 \n2 rsq     standard       0.721\n\n\n\nIn this case there is a slight increase in the R-squared and an increase in the RMSE, indicating a better fit compared to the previous model."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html#when-to-use-decision-trees",
    "href": "fall_sta235_2025/week_01/week_10.html#when-to-use-decision-trees",
    "title": "Data Science for Business Applications",
    "section": "When to use decision trees?",
    "text": "When to use decision trees?\n\nMain Advantages:\n\nEasy to interpret and explain (you can plot them!)\nMirrors human decision-making\nCan handle qualitative predictors (without need for dummies)\n\nMain disadvantages:\n\nAccuracy not as high as other methods\nVery sensitive to training data (e.g. overfitting)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#interactions-in-logistic-regression-models",
    "href": "fall_sta235_2025/week_01/week_09.html#interactions-in-logistic-regression-models",
    "title": "Data Science for Business Applications",
    "section": "Interactions in Logistic Regression Models",
    "text": "Interactions in Logistic Regression Models\nA bank that extends lines of credit has a sample of 100 customers with information about whether each customer’s loan account is in good standing, along with information about each customer:\n\nStatus: Good or Bad depending on whether the account is in good standing\n\nAge: The age of the customer\n\nDependents: The number of children the customer has\n\nSex: The sex of the customer\n\nJob_Years: How many years the customer has held their current job\n\nHome_Years: How many years the customer has lived in their current home\n\nOther_Credit: Whether the customer has any other lines of credit, (Yes,No)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#predicting-bad-status",
    "href": "fall_sta235_2025/week_01/week_09.html#predicting-bad-status",
    "title": "Data Science for Business Applications",
    "section": "Predicting bad status",
    "text": "Predicting bad status\n\nThe bank is looking to build a model to predict Status, because they would like to be able to predict which future customers are most likely to later have accounts in bad standing (so they can avoid approving those customers for lines of credit!).\nStatus is a categorical variable - transform it into a dummy variable by creating a new variable Bad that is 1 if Status is Bad and 0 if Status is Good.\n\n\nbanco = banco %&gt;% \n  mutate(Bad = ifelse(Status== \"Bad\", 1, 0))\n\n\nWhy 1 for Bad? It doesn’t really matter, but does make sense here because the bank is looking to predict which customers will later have accounts in bad standing."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#the-logistic-regression-model",
    "href": "fall_sta235_2025/week_01/week_09.html#the-logistic-regression-model",
    "title": "Data Science for Business Applications",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nWe’ll use two variables for predicting the status, Age and Other_Credit.\nThe two variables are added as an interaction: \\[\n\\log\\left(\\frac{p(\\text{Bad})}{1-p(\\text{Bad})}\\right) = \\beta_0 + \\beta_1  \\text{Age} + \\beta_2 \\text{Other_Credit} + \\beta_3  \\text{Age} \\times \\text{Other_Credit}\n\\] where odds of having a Bad status is defined as \\[\n\\text{odds}(\\text{Bad}) = \\frac{p(\\text{Bad})}{1-p(\\text{Bad})},\n\\]\n\nand where \\(p(\\text{Bad})\\) is probability of having a Bad status."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#run-the-model-in-r",
    "href": "fall_sta235_2025/week_01/week_09.html#run-the-model-in-r",
    "title": "Data Science for Business Applications",
    "section": "Run the model in R",
    "text": "Run the model in R\n\nglm1 = glm(Bad ~ Age*Other_Credit, data  = banco, family = \"binomial\")\nsummary(glm1)\n\n\nCall:\nglm(formula = Bad ~ Age * Other_Credit, family = \"binomial\", \n    data = banco)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          18.1579     4.8547   3.740 0.000184 ***\nAge                  -0.5871     0.1586  -3.701 0.000215 ***\nOther_CreditYes     -12.1921     5.2995  -2.301 0.021414 *  \nAge:Other_CreditYes   0.4076     0.1712   2.382 0.017241 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.63  on 99  degrees of freedom\nResidual deviance:  62.96  on 96  degrees of freedom\nAIC: 70.96\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#interpreting-the-model",
    "href": "fall_sta235_2025/week_01/week_09.html#interpreting-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the model",
    "text": "Interpreting the model\nWe have the following equation for predicting the log odds of having a Bad status:\n\\[\n\\log\\left(\\frac{p(\\text{Bad})}{1-p(\\text{Bad})}\\right) = 18.16  -0.59 \\cdot \\text{Age} -12.1921\\cdot \\text{Other_Credit(Yes)} + \\\\ 0.4076 \\cdot \\text{Age} \\times \\text{Other_Credit(Yes)}\n\\] The odds model is given by \\[\n\\text{odds}(\\text{Bad}) = \\frac{p(\\text{Bad})}{1-p(\\text{Bad})} = \\exp(18.16  -0.59 \\cdot \\text{Age} -12.19\\cdot \\text{Other_Credit(Yes)} + \\\\ 0.41 \\cdot \\text{Age} \\times \\text{Other_Credit(Yes)})\n\\] Next, we give the interpretation of these coefficients."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#section",
    "href": "fall_sta235_2025/week_01/week_09.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Intercept: Setting Age = 0, and Other_Credit(No) = 0 (baseline), results in \\(\\log(\\text{odds}) = 18.16\\). Thus, the odds of having a bad status under these conditions are \\(\\text{odds} = \\exp(18.16) = 77,052,688\\). This result has no practical meaning since we cannot have bank account holders with zero age.\nAge: To interpret the effect of Age alone, we have to set Other_Credit(No) = 0. Thus, for a one unit change in Age (for each year the account holder gets older), there will be a 44.56% decrease (\\((\\exp(-0.59)-1)\\cdot 100 = -44.56\\)) in the odds of having a bad status when the account holder doesn’t have other credit.\nOther_Credit: When Age = 0, the additional effect of having other credit, Other_Credit(No) = 1, compared to account members without other credit (baseline) is \\((\\exp(-12.19)-1)\\cdot100 = -99\\). Or a 99% decrease in the odds of having a bad status compared to account holders that have zero credit with zero Age.\nAge\\(\\times\\)Other_Credit: For a unit increase in Age there will be an extra increase in the odds of having a Bad status among account members with other credit of about \\((\\exp(0.41)-1)\\cot 100 = 50.68\\), or 50.68%, compared to account members without other credit."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#section-1",
    "href": "fall_sta235_2025/week_01/week_09.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "In total, the effect for one unit increase in Age for account holders with other credit is given by \\((\\exp(-0.59+0.41)-1)\\cdot100 = -16.47\\), or a decrease of 16.43% on the odds of having a Bad credit compared to account holders without other credit.\nThis, in turn, means that the rate of decrease in the odds of having bad credit with an increase in age is greater for account holders without other credit compared to those who do have other credit.\nAs with the linear model we end up with two models. One odds model for account holders with other credit Other_Credit(Yes) = 1.\n\n\\[\n\\text{odds}(\\text{Bad}) = \\exp(5.97  -0.18 \\cdot \\text{Age})\n\\] and when Other_Credit(Yes) = 0 (no other credit) \\[\n\\text{odds}(\\text{Bad}) = \\exp(18.16  -0.59 \\cdot \\text{Age})\n\\]"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#visualizing-the-model",
    "href": "fall_sta235_2025/week_01/week_09.html#visualizing-the-model",
    "title": "Data Science for Business Applications",
    "section": "Visualizing the model",
    "text": "Visualizing the model\n\nggplot(banco, aes(x = Age, y = Bad, col = Other_Credit)) +\n  geom_point() +\n  geom_line(aes(x = Age, y = predict(glm1, type = \"response\")))"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#making-predictions",
    "href": "fall_sta235_2025/week_01/week_09.html#making-predictions",
    "title": "Data Science for Business Applications",
    "section": "Making predictions",
    "text": "Making predictions\n\nSuppose we have a new account from which we don’t know the status. The only information we have is the account member’s age, Age = 35, and that this person has another credit. What are the predicted log odds, odds, and probability of this person having a bad status?\nPredicted log odds:\n\n\\[\n\\log\\left(\\text{odds}(\\text{Bad})\\right) = 5.97  -0.18 \\cdot 35 = -0.33\n\\] - Using the predict function:\n\npredict(glm1, list(Age = 35, Other_Credit = \"Yes\"))\n\n        1 \n-0.315811 \n\n\n\nThe difference in this case is due to the rounding on the coefficients of the equation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#section-2",
    "href": "fall_sta235_2025/week_01/week_09.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Predicted odds:\n\n\\[\n\\text{odds}(\\text{Bad}) = \\exp(5.97  -0.18 \\cdot 35) = 0.72\n\\] - Using the predict function:\n\nexp(predict(glm1, list(Age = 35, Other_Credit = \"Yes\")))\n\n        1 \n0.7291973 \n\n\n\nIn this case the odds are smaller than one, which indicates that the odds are not in favor of the group of interest (having a bad status) but on the base group, which is having a bad status. Thus, it is more likely that a person with this age and credit condition does not have a bad status compared to having good credit."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#section-3",
    "href": "fall_sta235_2025/week_01/week_09.html#section-3",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Predicted Probabily of having a bad status: \\[\np(\\text{Bad}) = \\frac{\\exp(5.97  -0.18 \\cdot 35)}{1+\\exp(5.97  -0.18 \\cdot 35)} = 0.42\n\\]\n\n\npredict(glm1, list(Age = 35, Other_Credit = \"Yes\"), type = \"response\")\n\n       1 \n0.421697 \n\n\n\nThe predicted the probability of account holder with 35 years of age having a Bad status is 0.42, or around 42%.\nHow do we know if this model is effective at making predictions in this case, since we don’t have access to common measures used for this goal, as the RSE, \\(R^2\\), and the RMSE?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#measuring-the-models-accuracy",
    "href": "fall_sta235_2025/week_01/week_09.html#measuring-the-models-accuracy",
    "title": "Data Science for Business Applications",
    "section": "Measuring the model’s Accuracy",
    "text": "Measuring the model’s Accuracy\n\nFirst we obtain the predictions (classification) given the predicted probabilities\nWe’ll use a threshold of 0.5 for the classification\n\n\nbanco = banco %&gt;% \n  mutate(predict.Status = ifelse(predict(glm1, type = \"response\")&gt;=0.5,\"Bad\",\"No\"))\n\n\nWe next obtain the ``confusion Matrix’’:\n\n\nxtabs(~predict.Status+Status, data = banco) %&gt;% \n  addmargins()\n\n              Status\npredict.Status Bad Good Sum\n           Bad  43    9  52\n           No    7   41  48\n           Sum  50   50 100"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#section-4",
    "href": "fall_sta235_2025/week_01/week_09.html#section-4",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "First we obtain the TP, TN, FP, and FN\n\n(TP) True positives: 43\n(TN) True negatives: 41\n(FP) False positives: 9\n(FN) False Negatives: 7\n\nThe accuracy is given by: \\[\n\\frac{\\text{(TP+TN)}}{\\text{Total}} = \\frac{43+41}{100} = 0.86\n\\]\nWe evaluate the accuracy from this model by comparing it’s accuracy to the ``no brainer’’ method:\n\n\nxtabs(~Status, data = banco) %&gt;% \n  prop.table()\n\nStatus\n Bad Good \n 0.5  0.5"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_09.html#section-5",
    "href": "fall_sta235_2025/week_01/week_09.html#section-5",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "There’s an increase in the accuracy of 86% from the 50% from the group that is the most common on Status. In this case, either group is valid.\nTo measure the model’s accuracy on out sample data, methods we can use cross validation."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_10.html",
    "href": "fall_sta235_2025/week_01/week_10.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Another predictive model\nDecision Trees: Classification and Regression Trees (CART)\nTrees are:\n\nFlexible at capturing non-linearity and interactions\nDon’t require scaling of variables\nHandle categorical and numerical data\nFast\nInterpretable"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#review-on-decision-trees",
    "href": "fall_sta235_2025/week_01/week_11.html#review-on-decision-trees",
    "title": "Data Science for Business Applications",
    "section": "Review on Decision Trees",
    "text": "Review on Decision Trees\n\nWhat kinds of models do we know how to make?\nClassification or Regression\nWe can use decision trees for either kind of \\(Y\\) variable."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#pros-cons-of-decision-trees",
    "href": "fall_sta235_2025/week_01/week_11.html#pros-cons-of-decision-trees",
    "title": "Data Science for Business Applications",
    "section": "Pros & Cons of Decision Trees",
    "text": "Pros & Cons of Decision Trees\n\nMain Advantages:\n\nEasy to interpret and explain (you can plot them!)\nMirrors human decision-making\nCan handle qualitative predictors (without need for dummies)\n\nMain disadvantages:\n\nAccuracy not as high as other methods\nVery sensitive to training data (e.g. overfitting)\nBiased toward dominant classes"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#what-we-can-do-to-improve",
    "href": "fall_sta235_2025/week_01/week_11.html#what-we-can-do-to-improve",
    "title": "Data Science for Business Applications",
    "section": "What we can do to improve?",
    "text": "What we can do to improve?\n\nImprove accuracy?\nLess sensitive to random changes in the data?\nMore relevance for low-frequency classifications?\nSolution: Bagging - Bootstrap Aggregation\n\nBootstrap - quantify sampling variability by resampling from the sample.\nIn Bagginig we basically we use bootstrap on the data and then we aggregate the results."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#bootstrap-resampling",
    "href": "fall_sta235_2025/week_01/week_11.html#bootstrap-resampling",
    "title": "Data Science for Business Applications",
    "section": "Bootstrap & Resampling",
    "text": "Bootstrap & Resampling\n\nSampling with replacement: each case selected for the sample is then replaced.\nEvery bootstrapped sample may have its own pattern of ties and omissions."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#bagging",
    "href": "fall_sta235_2025/week_01/week_11.html#bagging",
    "title": "Data Science for Business Applications",
    "section": "Bagging",
    "text": "Bagging\n\nAggregate the results from the method applied to each bootstrap sample.\nThis is also known as an ensemble, or combining the results of multiple models."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#bagging-ensembling",
    "href": "fall_sta235_2025/week_01/week_11.html#bagging-ensembling",
    "title": "Data Science for Business Applications",
    "section": "Bagging & Ensembling",
    "text": "Bagging & Ensembling\n\nYou want to predict on new test data.\nFor classification, take the majority vote!\nFor regression, take the mean or mode!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#random-forests",
    "href": "fall_sta235_2025/week_01/week_11.html#random-forests",
    "title": "Data Science for Business Applications",
    "section": "Random Forests",
    "text": "Random Forests\n\nRandom sampling of my training data rows helps to reduce variation in my predictions that are due to randomness.\nWe saw last week: sometimes the importance of individual variables can change from dataset to dataset, and therefore from model to model.\nWhat if we random sample the columns (variables) as well as the rows?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#random-forests-1",
    "href": "fall_sta235_2025/week_01/week_11.html#random-forests-1",
    "title": "Data Science for Business Applications",
    "section": "Random Forests",
    "text": "Random Forests\n\nConsidering the Titanic data\nWe again predict if the passenger survived (yes, no) - Categorical Variable\nClassification model\nBefore, I had sex as the variable defining the first split.\nIn my new trees, what if I only give the option for passengerClass and adult?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#random-forests-2",
    "href": "fall_sta235_2025/week_01/week_11.html#random-forests-2",
    "title": "Data Science for Business Applications",
    "section": "Random Forests",
    "text": "Random Forests"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#lets-build-a-random-forest-rf-in-r",
    "href": "fall_sta235_2025/week_01/week_11.html#lets-build-a-random-forest-rf-in-r",
    "title": "Data Science for Business Applications",
    "section": "Let’s build a Random Forest (RF) in R",
    "text": "Let’s build a Random Forest (RF) in R\n\nlibrary(randomForest)\n# Run the model\nmodel1 = randomForest(survived ~ adult + sex + passengerClass, data = titanic_age)\n# Show details on the model\nprint(model1)\n\n\nCall:\n randomForest(formula = survived ~ adult + sex + passengerClass,      data = titanic_age) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 20.55%\nConfusion matrix:\n     no yes class.error\nno  580  39  0.06300485\nyes 176 251  0.41217799\n\n\n\nIn an instant, we built 500 trees.\nWe only had 3 variables, so the RF only needed to try one at each split, and grow the tree.\nOOB (Out-of-Bag) error rate: 20.46%\n(580+252)/1046 = 0.795"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#classification-error-estimates",
    "href": "fall_sta235_2025/week_01/week_11.html#classification-error-estimates",
    "title": "Data Science for Business Applications",
    "section": "Classification Error estimates",
    "text": "Classification Error estimates\n\nplot(model1)\n\n\n\nThe plot shows how the error change given more trees are generated.\n\nOOB (black line)\nAmong passengers who did not survive, the error in prediction (red line)\nAmong passengers who did survive, the error in prediction (green line)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#feature-importance",
    "href": "fall_sta235_2025/week_01/week_11.html#feature-importance",
    "title": "Data Science for Business Applications",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nFeature (or variable) importance is an output from random forests.\nIn this case sex was by far most helpful as the first divide in the trees.\nAdult was the least important variable in terms of predicting survival.\n\n\nimportance(model1)\n\n               MeanDecreaseGini\nadult                  6.241061\nsex                   93.624871\npassengerClass        34.567394\n\n\n\nThe Mean Decrease in Gini: measures how much each variable contributes to reducing uncertainty when the trees are built in the random forest.\nA higher value means the variable is more important for making accurate classifications."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#feature-importance-1",
    "href": "fall_sta235_2025/week_01/week_11.html#feature-importance-1",
    "title": "Data Science for Business Applications",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nThe importance of the variable on the split can also be displayed in a plot.\n\n\nvarImpPlot(model1, sort = TRUE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#random-forest-for-regression",
    "href": "fall_sta235_2025/week_01/week_11.html#random-forest-for-regression",
    "title": "Data Science for Business Applications",
    "section": "Random Forest for Regression",
    "text": "Random Forest for Regression\n\nIn this model we’ll now predict the Age of the passengers given class, gender, if they survived or not.\n\n\n# Run the model\nmodel2 = randomForest(age ~ sex + passengerClass + survived, data = titanic_age)\n# Show details on the model\nprint(model2)\n\n\nCall:\n randomForest(formula = age ~ sex + passengerClass + survived,      data = titanic_age) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 172.4933\n                    % Var explained: 16.89\n\n\n\nThe MSE is given by 173.036, or RMSE = \\(\\sqrt{173.036}\\) = 13.15\nVariation explained 16.63 (similar to the \\(R^2\\))"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#regression-error-estimates",
    "href": "fall_sta235_2025/week_01/week_11.html#regression-error-estimates",
    "title": "Data Science for Business Applications",
    "section": "Regression Error estimates",
    "text": "Regression Error estimates\n\nThe plot displays the MSE in relation to the number of trees generated.\n\n\nplot(model2)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#random-forest-for-regression-1",
    "href": "fall_sta235_2025/week_01/week_11.html#random-forest-for-regression-1",
    "title": "Data Science for Business Applications",
    "section": "Random Forest for Regression",
    "text": "Random Forest for Regression\n\nVariance importance for this model\n\n\nvarImpPlot(model2, sort = TRUE)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_11.html#take-aways-of-the-random-forests",
    "href": "fall_sta235_2025/week_01/week_11.html#take-aways-of-the-random-forests",
    "title": "Data Science for Business Applications",
    "section": "Take aways of the Random Forests",
    "text": "Take aways of the Random Forests\n\nImprove accuracy on new data\nLess sensitive to random changes in the data.\nMore relevance for low-frequency classifications.\nRandom forests give me a lot of the benefit of decision\ntrees with fewer downsides!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#understanding-vs-predicting",
    "href": "fall_sta235_2025/week_01/week_12.html#understanding-vs-predicting",
    "title": "Data Science for Business Applications",
    "section": "Understanding vs predicting",
    "text": "Understanding vs predicting\n\nIn a purely predictive model, we should include terms that reduce out-of-sample prediction error.\n\nWith lots of variables, many models will have essentially identical out-of-sample predictive error, and we can use any of them.\n\nIf we are trying to understand the relationships between a subset of the \\(X\\) variables and \\(Y\\), the model form is much more important."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#variables-in-the-study",
    "href": "fall_sta235_2025/week_01/week_12.html#variables-in-the-study",
    "title": "Data Science for Business Applications",
    "section": "Variables in the study",
    "text": "Variables in the study\n\ntotal_sleep_time (TST): Average daily sleep time, recorded early in the term.\n\ncumulative_gpa: GPA at the beginning of the term.\n\nmidterm: Midterm grade, recorded midway through the term.\n\nterm_gpa: GPA at the end of the term.\n\nstudy: Four studies using different cohorts at different universities:\n\nStudy 2, 3: “large public university”\n\nStudy 4: “private Catholic university”\n\nStudy 5: “private, STEM-focused university”\n\n\nrace: 1 if either parent from an under-represented group (non-White/Asian).\n\nfirst_gen: 1 if neither parent attended any college.\n\ngender: 1 if reported male, 0 otherwise.\n\nheight: height in inches."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#model-1-predicting-term-gpa-from-total-sleep-time",
    "href": "fall_sta235_2025/week_01/week_12.html#model-1-predicting-term-gpa-from-total-sleep-time",
    "title": "Data Science for Business Applications",
    "section": "Model 1: Predicting term GPA from total sleep time",
    "text": "Model 1: Predicting term GPA from total sleep time\n\nmodel1 = lm(term_gpa ~ total_sleep_time, data=sleep)\nsummary(model1)\n\n\nCall:\nlm(formula = term_gpa ~ total_sleep_time, data = sleep)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.99561 -0.21082  0.09999  0.36996  0.71862 \n\nCoefficients:\n                 Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       2.80814    0.17241  16.287 &lt; 0.0000000000000002 ***\ntotal_sleep_time  0.09619    0.02570   3.742             0.000201 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4844 on 552 degrees of freedom\nMultiple R-squared:  0.02474,   Adjusted R-squared:  0.02298 \nF-statistic:    14 on 1 and 552 DF,  p-value: 0.0002015\n\nconfint(model1)\n\n                      2.5 %    97.5 %\n(Intercept)      2.46947087 3.1468058\ntotal_sleep_time 0.04570081 0.1466792"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#building-a-model-for-understanding",
    "href": "fall_sta235_2025/week_01/week_12.html#building-a-model-for-understanding",
    "title": "Data Science for Business Applications",
    "section": "Building a model for understanding",
    "text": "Building a model for understanding\n\nWe would like to be able to conclude that more sleep causes higher GPAs, but this model will not let us do that.\n\nWe can rule out potential confounders by adding them to the model (along with other predictive variables) and avoid causal language.\n\nHow do we decide which variables to include?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#how-do-we-decide-which-variables-to-include",
    "href": "fall_sta235_2025/week_01/week_12.html#how-do-we-decide-which-variables-to-include",
    "title": "Data Science for Business Applications",
    "section": "How do we decide which variables to include?",
    "text": "How do we decide which variables to include?\n\nTheory-driven, often “pre-registered” to avoid \\(p\\)-hacking.\n\\(p\\)-hacking: running statistical tests on a set of data until some statistically significant results arise.\nThat can be done in a few different ways, for example: by stopping the collection of data once you get a \\(p\\)&lt;0.05, analyzing many outcomes, but only reporting those with \\(p\\)&lt;0.05, using covariates, excluding participants, etc.\nData-driven, but be careful — we don’t want to overfit!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#variables-to-always-include",
    "href": "fall_sta235_2025/week_01/week_12.html#variables-to-always-include",
    "title": "Data Science for Business Applications",
    "section": "Variables to always include",
    "text": "Variables to always include\n\nConfounders are variables that causally influence both total sleep time and term GPA\n\nBecause of their collinearity with sleep time, the sleep time coefficient will be different if omitted from the model.\n\nTo ensure we are measuring the effect of sleep and not the confounder, they must be in the model!\n\nVariables that are strongly predictive of term GPA and are (nearly) uncorrelated with total sleep time.\n\nBy reducing the residual standard error, these variables reduce the size of confidence intervals for the regression coefficient of interest."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#variables-you-may-want-to-include",
    "href": "fall_sta235_2025/week_01/week_12.html#variables-you-may-want-to-include",
    "title": "Data Science for Business Applications",
    "section": "Variables you may want to include",
    "text": "Variables you may want to include\n\nVariables that are weakly predictive of term GPA and (nearly) uncorrelated with total sleep time.\n\nIs the more complex model worth the extra predictive power?\n\nVariables that someone might think are confounders, even if you don’t agree (or they don’t appear to be after seeing the data).\n\nThis lends your analysis some credibility."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#variables-you-should-never-include",
    "href": "fall_sta235_2025/week_01/week_12.html#variables-you-should-never-include",
    "title": "Data Science for Business Applications",
    "section": "Variables you should NEVER include",
    "text": "Variables you should NEVER include\n\nVariables that are causally influenced by the outcome \\(Y\\) (term GPA).\n\nThese variables can introduce bias, especially if they’re also causally influenced by total sleep time (a collider).\n\nThis can happen by accident when constructing the final dataset (e.g., if we excluded students who dropped out at the end of the term).\n\nVariables that are causally influenced by the \\(X\\) variable of interest (total sleep time).\n\nAdjusting for these variables removes a part of the effect we’re interested in!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#report-findings",
    "href": "fall_sta235_2025/week_01/week_12.html#report-findings",
    "title": "Data Science for Business Applications",
    "section": "Report findings:",
    "text": "Report findings:\n\nModels with few or no other variables.\n\nModels that adjust for theory-driven confounders and predictors.\n\nModels that add data-driven confounders and predictors — specifically note these!\n\nModels that add or remove subsets of potential confounders, to assess how robust the findings are."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#so-what-did-we-find",
    "href": "fall_sta235_2025/week_01/week_12.html#so-what-did-we-find",
    "title": "Data Science for Business Applications",
    "section": "So, what did we find?",
    "text": "So, what did we find?\n\nAfter adjusting for cohort, cumulative GPA, and demographic variables, each additional hour of sleep is associated with an increase in predicted GPA of about 0.05 points, with a 95% CI of about [0.03, 0.11].\n\nThe effect estimate is robust to the specific adjustment for demographic variables hypothesized to correlate with sleep patterns and academic performance — this strengthens our argument!\n\nIn each case, our findings are statistically significant, and the estimated effects (and their plausible values based on the CIs) are all positive.\n\nIn other words, you should probably get more sleep!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#multicollinearity",
    "href": "fall_sta235_2025/week_01/week_12.html#multicollinearity",
    "title": "Data Science for Business Applications",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nMulticollinearity exists whenever 2+ predictors in a regression model are moderately or highly correlated.\n\nIf two predictors \\(X_1\\) and \\(X_2\\) are highly correlated, it is hard to estimate the effect of changing \\(X_1\\) while keeping \\(X_2\\) constant.\n\nThis means the coefficients for \\(X_1\\) and \\(X_2\\) could be misleading: we will get wide confidence intervals for \\(X_1\\) and/or \\(X_2\\), and the coefficients won’t be stable (adding new data or predictors to the model could drastically change them).\n\nThat’s a problem if we’re using the size of a coefficient to understand the relationship between \\(X_1\\) (or \\(X_2\\)) and \\(Y\\).\n\nCorrelation between the response and the predictors is good, but correlation between the predictors is not!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#extreme-examples",
    "href": "fall_sta235_2025/week_01/week_12.html#extreme-examples",
    "title": "Data Science for Business Applications",
    "section": "Extreme examples",
    "text": "Extreme examples\n\nPredicting your height based on the amount you weighed on one scale (\\(X_1\\)) and the amount you weighed on a second scale (\\(X_2\\)): very hard to disentangle the effects of \\(X_1\\) and \\(X_2\\).\n\nPredicting your height based on your weight in pounds (\\(X_1\\)) and your weight in kilograms (\\(X_2\\)): perfect multicollinearity — impossible to disentangle the effects of \\(X_1\\) vs \\(X_2\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#example-math-sat-predicts-graduation-rate-above-and-beyond-acceptance-rate",
    "href": "fall_sta235_2025/week_01/week_12.html#example-math-sat-predicts-graduation-rate-above-and-beyond-acceptance-rate",
    "title": "Data Science for Business Applications",
    "section": "Example: Math SAT predicts graduation rate above and beyond acceptance rate",
    "text": "Example: Math SAT predicts graduation rate above and beyond acceptance rate\n\nmodel1 = lm(Graduation.rate ~ Average.math.SAT + Acceptance.rate, data = colleges)\nsummary(model1)\n\n\nCall:\nlm(formula = Graduation.rate ~ Average.math.SAT + Acceptance.rate, \n    data = colleges)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.018 -10.076  -0.467   9.891  74.037 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)      -18.680894   6.703758  -2.787              0.00547 ** \nAverage.math.SAT   0.155968   0.009455  16.496 &lt; 0.0000000000000002 ***\nAcceptance.rate    0.016856   0.040725   0.414              0.67908    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.45 on 725 degrees of freedom\n  (574 observations deleted due to missingness)\nMultiple R-squared:  0.3104,    Adjusted R-squared:  0.3085 \nF-statistic: 163.2 on 2 and 725 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#but-not-after-adding-verbal-sat",
    "href": "fall_sta235_2025/week_01/week_12.html#but-not-after-adding-verbal-sat",
    "title": "Data Science for Business Applications",
    "section": "But not after adding verbal SAT?!?",
    "text": "But not after adding verbal SAT?!?\n\nmodel2 = lm(Graduation.rate ~ Average.math.SAT + Average.verbal.SAT + Acceptance.rate, data = colleges)\nsummary(model2)\n\n\nCall:\nlm(formula = Graduation.rate ~ Average.math.SAT + Average.verbal.SAT + \n    Acceptance.rate, data = colleges)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.487  -9.230  -0.139   8.968  73.071 \n\nCoefficients:\n                    Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)        -27.93587    6.67769  -4.183 0.000032235072 ***\nAverage.math.SAT     0.03063    0.02145   1.428          0.154    \nAverage.verbal.SAT   0.15736    0.02433   6.468 0.000000000183 ***\nAcceptance.rate      0.02072    0.03963   0.523          0.601    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.03 on 724 degrees of freedom\n  (574 observations deleted due to missingness)\nMultiple R-squared:  0.3481,    Adjusted R-squared:  0.3454 \nF-statistic: 128.9 on 3 and 724 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#how-can-we-detect-multicollinearity",
    "href": "fall_sta235_2025/week_01/week_12.html#how-can-we-detect-multicollinearity",
    "title": "Data Science for Business Applications",
    "section": "How can we detect multicollinearity?",
    "text": "How can we detect multicollinearity?\n\nThe VIF for variable \\(X_j\\) is:\n\n\\[\\text{VIF}(\\beta_j) = \\frac{1}{1 - R_j^2},\\]\nwhere \\(R_j^2\\) is the \\(R^2\\) in a regression predicting \\(X_j\\) from the other \\(X\\) variables.\n\n\\(\\text{VIF}(\\beta_j) = 1\\) when \\(R_j^2 = 0\\); i.e., the \\(j\\)th predictor variable is completely independent from the others.\n\nHigher VIF for variable \\(X_j\\) means that variable is more closely related to the other \\(X\\) variables (more multicollinearity)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#how-can-we-detect-multicollinearity-1",
    "href": "fall_sta235_2025/week_01/week_12.html#how-can-we-detect-multicollinearity-1",
    "title": "Data Science for Business Applications",
    "section": "How can we detect multicollinearity?",
    "text": "How can we detect multicollinearity?\nTo calculate VIF for each predictor, you would have to run one regression for each predictor. But you don’t have to run all of these regressions by hand!\n\nmodel = lm(Graduation.rate ~ Average.math.SAT + Average.verbal.SAT + Acceptance.rate, data = colleges)\n# install.packages(\"car\")\nlibrary(car)\nvif(model)\n\n  Average.math.SAT Average.verbal.SAT    Acceptance.rate \n          6.660778           6.469362           1.225301"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#dealing-with-multicollinearity",
    "href": "fall_sta235_2025/week_01/week_12.html#dealing-with-multicollinearity",
    "title": "Data Science for Business Applications",
    "section": "Dealing with multicollinearity",
    "text": "Dealing with multicollinearity\nThere are two general strategies for dealing with multicollinearity:\n\nDrop one of the variables with a high VIF factor, and rerun to see if VIFs have improved. (Just like we drop one of the dummy variables when putting a categorical variable in the model!)\n\nCombine the variables that correlate into a composite variable. (Combined SAT score = Math + Verbal)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#dealing-with-multicollinearity-1",
    "href": "fall_sta235_2025/week_01/week_12.html#dealing-with-multicollinearity-1",
    "title": "Data Science for Business Applications",
    "section": "Dealing with multicollinearity",
    "text": "Dealing with multicollinearity\nWhen we combine math and verbal SAT into a single variable, VIFs are now close to 1.\n\nmodel = lm(Graduation.rate ~ Average.combined.SAT + Acceptance.rate, data = colleges)\nvif(model)\n\nAverage.combined.SAT      Acceptance.rate \n            1.219782             1.219782"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_12.html#when-is-multicollinearity-not-a-problem",
    "href": "fall_sta235_2025/week_01/week_12.html#when-is-multicollinearity-not-a-problem",
    "title": "Data Science for Business Applications",
    "section": "When is multicollinearity not a problem?",
    "text": "When is multicollinearity not a problem?\n\nWhen there is high collinearity in \\(X\\)’s that are strictly for adjustment, not interpretation, and the coefficient we want to interpret corresponds to a variable with low multicollinearity.\n\nWhen multicollinearity comes from how we construct \\(X\\): e.g., adding polynomial terms (like last week!), turning categories into dummy variables, or adding interactions.\n\nWhen we are just trying to predict using relatively few \\(X\\)’s, and only predicting at “typical” \\(X\\) values."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#causal-conclusion",
    "href": "fall_sta235_2025/week_01/week_13.html#causal-conclusion",
    "title": "Data Science for Business Applications",
    "section": "Causal Conclusion",
    "text": "Causal Conclusion\nIf we run a regression predicting \\(Y\\) from \\(X\\) and find that \\(X\\) is a significant predictor of \\(Y\\), we would like to conclude that \\(X\\) causes \\(Y\\). But it might be the case that:\n\n\\(Y\\) actually causes \\(X\\).\n\\(X\\) and \\(Y\\) are not actually related in the population; they happen to be correlated in the sample just by chance.\nA common variable \\(Z\\) (a confounder) causes both \\(X\\) and \\(Y\\).\nA common variable \\(W\\) (a collider) is caused by both \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#confounders-and-colliders",
    "href": "fall_sta235_2025/week_01/week_13.html#confounders-and-colliders",
    "title": "Data Science for Business Applications",
    "section": "Confounders and Colliders",
    "text": "Confounders and Colliders\n\nA confounder is a third variable that causes both \\(X\\) and \\(Y\\) and explains the observed correlation between X and Y.\nA collider is a third variable that is caused by both \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#randomization",
    "href": "fall_sta235_2025/week_01/week_13.html#randomization",
    "title": "Data Science for Business Applications",
    "section": "Randomization",
    "text": "Randomization\nOne way to make sure the causal conclusion holds is to do it by design:\n\nRandomize the assignment of the treatment \\(Z\\)\ni.e. Some units will randomly be chosen to be in the treatment group and others to be in the control group.\nWhat does randomization buy us?\nControl for unforeseen factors (confounders)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#confounders",
    "href": "fall_sta235_2025/week_01/week_13.html#confounders",
    "title": "Data Science for Business Applications",
    "section": "Confounders",
    "text": "Confounders\n\nConfounder is a variable that affects both the treatment and the outcome"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#randomization-1",
    "href": "fall_sta235_2025/week_01/week_13.html#randomization-1",
    "title": "Data Science for Business Applications",
    "section": "Randomization",
    "text": "Randomization\n\nDue to randomization, we know that the treatment is not affected by a confounder\n\n\n\nThis would be the causal effect of the treatment"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#randomized-controlled-trials-rcts",
    "href": "fall_sta235_2025/week_01/week_13.html#randomized-controlled-trials-rcts",
    "title": "Data Science for Business Applications",
    "section": "Randomized controlled trials (RCTs)",
    "text": "Randomized controlled trials (RCTs)\n\nOften called the gold standard for establishing causality.\nRandomly assign the \\(X\\), treatment, to participants\nNow, any observed relationship between \\(X\\) and \\(Y\\) must be due to \\(X\\), since the only reason an individual had a particular value of \\(X\\) was the random assignment."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#randomized-controlled-trials-rcts-1",
    "href": "fall_sta235_2025/week_01/week_13.html#randomized-controlled-trials-rcts-1",
    "title": "Data Science for Business Applications",
    "section": "Randomized controlled trials (RCTs)",
    "text": "Randomized controlled trials (RCTs)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#rct---steps",
    "href": "fall_sta235_2025/week_01/week_13.html#rct---steps",
    "title": "Data Science for Business Applications",
    "section": "RCT - Steps",
    "text": "RCT - Steps\n\nRandomize\nCheck for balance - (balance between the treated and untreated)\nCalculate difference in sample means between treatment and control group"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#problem-with-causal-inference",
    "href": "fall_sta235_2025/week_01/week_13.html#problem-with-causal-inference",
    "title": "Data Science for Business Applications",
    "section": "Problem with causal inference",
    "text": "Problem with causal inference\n\nSuppose you have a headache, and you take an asprin. Then you don’t have a headache. Did the asprin work?\n\n\n\n\n\nPerson\nTook aspirin\nDidn’t take aspirin\n\n\n\n\n1\nno headache\n?\n\n\n2\nno headache\n?\n\n\n3\nno headache\n?\n\n\n4\nno headache\n?\n\n\n5\n?\nno headache\n\n\n6\n?\nheadache\n\n\n7\n?\nheadache\n\n\n8\n?\nheadache\n\n\n\n\n\nFor any given person, we can only observe one outcome or the other, depending on whether the person took an asprin or not:"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#problem-with-causal-inference-1",
    "href": "fall_sta235_2025/week_01/week_13.html#problem-with-causal-inference-1",
    "title": "Data Science for Business Applications",
    "section": "Problem with causal inference",
    "text": "Problem with causal inference\n\nThe best we can do is compute an average treatment effect (ATE): the difference in the proportion of the treatment vs control group \\[\n\\begin{aligned}\n\\text{ATE}\n&= (\\% \\text{ headache among aspirin–takers}) \\\\\n&\\;\\;-\\; (\\% \\text{ headache among non–aspirin–takers}) \\\\\n&= \\frac{0}{4} - \\frac{3}{4} \\\\\n&= -0.75\n\\end{aligned}\n\\]\nHeadaches decreased in 75% among those who took aspirin compared to those who didn’t take aspirin.\nWe can only make this conclusion if the treatment was randomly assigned."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#example-1-vaccine-trial",
    "href": "fall_sta235_2025/week_01/week_13.html#example-1-vaccine-trial",
    "title": "Data Science for Business Applications",
    "section": "Example 1: Vaccine Trial",
    "text": "Example 1: Vaccine Trial\n\nPhase 3 Clinical Trial for the Moderna COVID-19 vaccine\n\\(X\\) = got the vaccine, \\(Y\\) = got COVID-19\nRandomly assign study participants to get either the vaccine (an treatment group of 14,134 people) or a placebo (a control group of 14,073 people)\n11 vaccine recipients got COVID; 185 of placebo recipients got COVID"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#issues-with-rct",
    "href": "fall_sta235_2025/week_01/week_13.html#issues-with-rct",
    "title": "Data Science for Business Applications",
    "section": "Issues with RCT",
    "text": "Issues with RCT\n\nInternal validity is the ability of an experiment to establish cause-and-effect of the treatment within the sample studied.\nExamples of threats to internal validity:\n\nFailure to randomize.\nFailure to follow the treatment protocol/attrition.\nSmall sample sizes"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#issues-with-rct-1",
    "href": "fall_sta235_2025/week_01/week_13.html#issues-with-rct-1",
    "title": "Data Science for Business Applications",
    "section": "Issues with RCT",
    "text": "Issues with RCT\n\nExternal validity is the ability of an experimental result to generalize to a larger context or population.\nExamples of threats to external validity:\n\nNon representative samples.\nNon representative protocol/policy."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nRandomization works on average but we only get one opportunity at creating treatment and control groups, and there might be imbalances in nuisance variables that could affect the outcome.\nFor example, what will happen if the treatment group for the Moderna trial happens to get younger people in it than the control group?\nWe can solve this by blocking or stratifying: randomly assigning to treatment/control within groups."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-1",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-1",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nUnbalanced sample - Males and Females"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-2",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-2",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nBlocking or stratification sample"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-in-vaccine-trial",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-in-vaccine-trial",
    "title": "Data Science for Business Applications",
    "section": "Blocking in vaccine trial",
    "text": "Blocking in vaccine trial\n\nIn the Moderna vaccine trial, they identified two possible variables that could impact COVID outcomes:\nAge (65+ vs under 65)\nUnderlying health condition"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-in-vaccine-trial-1",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-in-vaccine-trial-1",
    "title": "Data Science for Business Applications",
    "section": "Blocking in vaccine trial",
    "text": "Blocking in vaccine trial"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#experiments-using-regression",
    "href": "fall_sta235_2025/week_01/week_13.html#experiments-using-regression",
    "title": "Data Science for Business Applications",
    "section": "Experiments using regression",
    "text": "Experiments using regression\n\nNon-blocked design: use a simple regression \\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 T,\n\\]\nwhere \\(T\\) is a dummy variable that is \\[\nT =\n\\begin{cases}\n  1, & \\text{for the treatment group}, \\\\\n  0, & \\text{for the control group}\n\\end{cases}\n\\]\n\\(\\widehat{\\beta}_1\\) represents the estimated average treatment effect. The regression needs to be logistic if \\(Y\\) is categorical!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#experiments-using-regression-1",
    "href": "fall_sta235_2025/week_01/week_13.html#experiments-using-regression-1",
    "title": "Data Science for Business Applications",
    "section": "Experiments using regression",
    "text": "Experiments using regression\n\nBlocked design: use a regression that controls for the blocking variable \\(B\\):\n\n\\[\n\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 T + \\widehat{\\beta}_2 B,\n\\] where \\(B\\) is the fixed effect of each strata, that are interactions between categories.\n\nImportant: the regression needs to be logistic if \\(Y\\) is categorical."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#get-out-the-vote-gotv",
    "href": "fall_sta235_2025/week_01/week_13.html#get-out-the-vote-gotv",
    "title": "Data Science for Business Applications",
    "section": "Get Out The Vote (GOTV)",
    "text": "Get Out The Vote (GOTV)\n\nIn 2002, researchers at Temple and Yale conducted a large phone banking experiment to see calling voters helps:\nFrom among about 381,062 phone numbers of voters in Iowa and Michigan they randomly contacted about 12000 voters\nThe outcome Y of interest is whether each voter actually voted."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#no-blocking",
    "href": "fall_sta235_2025/week_01/week_13.html#no-blocking",
    "title": "Data Science for Business Applications",
    "section": "No blocking",
    "text": "No blocking\nEstimating the average treatment effect with logistic regression:\n\nglm = glm(vote02 ~ treatment,data = GOTV, family = \"binomial\")\nsummary(glm)\n\n\nCall:\nglm(formula = vote02 ~ treatment, family = \"binomial\", data = GOTV)\n\nCoefficients:\n                   Estimate Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        0.184717   0.003306  55.870 &lt;0.0000000000000002 ***\ntreatmenttreatment 0.170824   0.018843   9.066 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 524839  on 381061  degrees of freedom\nResidual deviance: 524756  on 381060  degrees of freedom\nAIC: 524760\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nThe coefficients are in log odds."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#no-blocking-1",
    "href": "fall_sta235_2025/week_01/week_13.html#no-blocking-1",
    "title": "Data Science for Business Applications",
    "section": "No blocking",
    "text": "No blocking\n\nThe average treatment effect will be of approximately 19%\n\n\n(exp(0.17)-1)*100\n\n[1] 18.53049\n\nconfint(glm)\n\n                       2.5 %    97.5 %\n(Intercept)        0.1782378 0.1911978\ntreatmenttreatment 0.1339278 0.2077954\n\n\n\nReceiving a phone call increases the likelihood of voting by 19% compared to those who did not receive a call.\nConfidence interval for the treatment\n\n\nconfint(glm)\n\n                       2.5 %    97.5 %\n(Intercept)        0.1782378 0.1911978\ntreatmenttreatment 0.1339278 0.2077954"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-3",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-3",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nThe researchers actually used a blocking design with two variables that they thought could impact voting rates (separately from the phone calls):\nThe state of the voter (Iowa or Michigan)\nWhether the voter was in a “competitive” district (one where there was likely to be a close election)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-4",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-4",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-5",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-5",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nGOTV = GOTV %&gt;%\n       mutate(block = interaction(state, competiv))\nglm_vote = glm(vote02 ~ treatment + block, data = GOTV, family = 'binomial')\nsummary(glm_vote)\n\n\nCall:\nglm(formula = vote02 ~ treatment + block, family = \"binomial\", \n    data = GOTV)\n\nCoefficients:\n                   Estimate Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        0.043236   0.004146   10.43 &lt;0.0000000000000002 ***\ntreatmenttreatment 0.028542   0.019279    1.48               0.139    \nblock1.1           0.351686   0.015168   23.19 &lt;0.0000000000000002 ***\nblock0.2           0.196691   0.008866   22.18 &lt;0.0000000000000002 ***\nblock1.2           0.603739   0.009515   63.45 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 524839  on 381061  degrees of freedom\nResidual deviance: 520331  on 381057  degrees of freedom\nAIC: 520341\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#blocking-6",
    "href": "fall_sta235_2025/week_01/week_13.html#blocking-6",
    "title": "Data Science for Business Applications",
    "section": "Blocking",
    "text": "Blocking\n\nThe effect of the treatment is not significant.\nWhat if some callers didn’t stick to the script?\nMany people didn’t answer the phone!\nWhat about voters outside of the Midwest?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html#the-limitations-of-rcts",
    "href": "fall_sta235_2025/week_01/week_13.html#the-limitations-of-rcts",
    "title": "Data Science for Business Applications",
    "section": "The limitations of RCTs",
    "text": "The limitations of RCTs\n\nAlthough powerful for inferring causation, RCTs are difficult to apply.\nThey can be incredibly expensive.\nCompliance with the treatment protocol isn’t perfect (e.g., mask-wearing, picking up the phone)\nIt can be hard to generalize beyond the participants involved in the study.\nThey can be impractical or (e.g., effect of education on performance) or unethical to conduct (e.g., seatbelts, parachutes, even medical trials)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#the-limitations-of-randomized-controlled-trials-rcts",
    "href": "fall_sta235_2025/week_01/week_14.html#the-limitations-of-randomized-controlled-trials-rcts",
    "title": "Data Science for Business Applications",
    "section": "The limitations of Randomized Controlled Trials (RCTs)",
    "text": "The limitations of Randomized Controlled Trials (RCTs)\nAlthough they are powerful for inferring causation, RCTs are hard to pull off:\n\nThey can be incredibly expensive (e.g., Phase 3 clinical trial)\nCompliance with the treatment protocol isn’t perfect (e.g., low-calorie diet, picking up the phone)\nIt can be hard to generalize beyond the participants involved in the study if they aren’t representative (e.g., psychology experiments conducted on college students)\nThey can be impractical (e.g., effect of education on later earnings) or even unethical (e.g., seatbelts, parachutes, medical trials)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#faking-randomization",
    "href": "fall_sta235_2025/week_01/week_14.html#faking-randomization",
    "title": "Data Science for Business Applications",
    "section": "“Faking” randomization",
    "text": "“Faking” randomization\n\nKey idea: Find a comparison group that is effectively “the same as” the treatment group to create a: quasi-experiment or natural experiment."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#does-serving-in-the-military-affect-long-term-earnings",
    "href": "fall_sta235_2025/week_01/week_14.html#does-serving-in-the-military-affect-long-term-earnings",
    "title": "Data Science for Business Applications",
    "section": "Does serving in the military affect long-term earnings?",
    "text": "Does serving in the military affect long-term earnings?\n\nDoes serving in the military have an impact upon your long-term earnings after discharge?\nWhy this won’t work: Compare the wages of people who served in the US military in Afghanistan or Iraq, 10 years after discharge, to the wages of the general public."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#a-natural-experiment-on-the-effect-of-military-service-on-earnings",
    "href": "fall_sta235_2025/week_01/week_14.html#a-natural-experiment-on-the-effect-of-military-service-on-earnings",
    "title": "Data Science for Business Applications",
    "section": "A natural experiment on the effect of military service on earnings",
    "text": "A natural experiment on the effect of military service on earnings\n\nAngrist (1990) wanted to determine what effect military service had on future earnings\n“Treatment” group: men selected by lottery to serve in Vietnam\n“Control” group: men eligible to be drafted but not selected to serve\nWe effectively have (almost) random assignment\nThis is called a natural experiment because we have discovered something close to an RCT “in the wild”\nFor white men, earnings in the 1980s were 15% lower in the treatment group; military service in Vietnam causally reduced long-term earning power"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#quasi-experiments-natural-experiments",
    "href": "fall_sta235_2025/week_01/week_14.html#quasi-experiments-natural-experiments",
    "title": "Data Science for Business Applications",
    "section": "Quasi-experiments / Natural experiments",
    "text": "Quasi-experiments / Natural experiments\n\nThese are called quasi-experiments or natural experiments because participants are not randomly assigned to treatment and control groups, but groups are selected in such a way that the assignment can be thought of as effectively random."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#a-natural-experiment-of-the-minimum-wage",
    "href": "fall_sta235_2025/week_01/week_14.html#a-natural-experiment-of-the-minimum-wage",
    "title": "Data Science for Business Applications",
    "section": "A natural experiment of the minimum wage",
    "text": "A natural experiment of the minimum wage\n\nWhy can’t we just compare the unemployment rate in places with a low minimum wage (e.g., Texas) to places with a high minimum wage (e.g., California)?\nWhy can’t we just do a randomized controlled trial to study the impact of raising the minimum wage?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#a-natural-experiment-of-the-minimum-wage-1",
    "href": "fall_sta235_2025/week_01/week_14.html#a-natural-experiment-of-the-minimum-wage-1",
    "title": "Data Science for Business Applications",
    "section": "A natural experiment of the minimum wage",
    "text": "A natural experiment of the minimum wage\n\nIn 1992, New Jersey’s minimum wage went from $4.25 to $5.05\nThe minimum wage in Pennsylvania remained at $4.25\nResearchers measured employment at 410 fast food restaurants in NJ and PA both before and after the change\nThis is a natural experiment because the two groups arose naturally (rather than being assigned by the researchers)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#nj-vs-pa-comparison",
    "href": "fall_sta235_2025/week_01/week_14.html#nj-vs-pa-comparison",
    "title": "Data Science for Business Applications",
    "section": "NJ vs PA comparison",
    "text": "NJ vs PA comparison\n\n\n\n\nAfter\n\n\n\n\nPennsylvania\n21.17\n\n\nNew Jersey\n21.03\n\n\nDifference\n-0.14\n\n\n\n\nAfter the policy change, employment was 0.14 employees per store less in NJ than in PA. Can we interpret this as a causal effect?\nNo! We cannot distinguish the effect of the minimum wage increase from other differences between PA and NJ."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#difference-in-differences",
    "href": "fall_sta235_2025/week_01/week_14.html#difference-in-differences",
    "title": "Data Science for Business Applications",
    "section": "Difference-in-differences",
    "text": "Difference-in-differences\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nPennsylvania\n23.33\n21.17\n-2.16\n\n\nNew Jersey\n20.44\n21.03\n0.59\n\n\nDifference\n-2.89\n-0.14\n2.76\n\n\n\n\nThe difference of the differences (-0.14-(-2.89) or 0.59-(-2.16)) gives us the causal effect of the policy change."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#ways-to-create-natural-experiments",
    "href": "fall_sta235_2025/week_01/week_14.html#ways-to-create-natural-experiments",
    "title": "Data Science for Business Applications",
    "section": "Ways to create natural experiments",
    "text": "Ways to create natural experiments\n\nGeographic boundaries (e.g., NJ vs PA minimum wage example)\nPolicy changes (e.g., financial aid policy change example)\nLotteries (e.g., Vietnam draft lottery example)\nArbitrary cutoffs"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#do-flagship-state-university-grads-earn-more-money",
    "href": "fall_sta235_2025/week_01/week_14.html#do-flagship-state-university-grads-earn-more-money",
    "title": "Data Science for Business Applications",
    "section": "Do flagship state university grads earn more money?",
    "text": "Do flagship state university grads earn more money?\n\nWhy can’t we answer this question by comparing average income or wealth of (say) Texas Exes to non-Texas Exes?\nWhy can’t we do a Randomized Controlled Trial?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#regression-discontinuity-designs",
    "href": "fall_sta235_2025/week_01/week_14.html#regression-discontinuity-designs",
    "title": "Data Science for Business Applications",
    "section": "Regression discontinuity designs",
    "text": "Regression discontinuity designs\n\nHoekstra (2009) studied admission to a state flagship university with an SAT cutoff for admission\nKey idea: Compare earnings 15 years after graduation for students that just made the admissions cutoff (and were accepted)\nto those that just missed it (and were rejected)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#regression-discontinuity-design-example",
    "href": "fall_sta235_2025/week_01/week_14.html#regression-discontinuity-design-example",
    "title": "Data Science for Business Applications",
    "section": "Regression discontinuity design example",
    "text": "Regression discontinuity design example\n\nBuild two regressions predicting Y = earnings measure from X = SAT score: one for students below the cutoff and one for students above\nThe length of the red line between the curves is the causal effect of admission"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#is-there-a-benefit-to-small-class-sizes",
    "href": "fall_sta235_2025/week_01/week_14.html#is-there-a-benefit-to-small-class-sizes",
    "title": "Data Science for Business Applications",
    "section": "Is there a benefit to small class sizes?",
    "text": "Is there a benefit to small class sizes?\n\nMany people argue that smaller classes lead to better learning outcomes compared to large classes\nBut why can’t we just compare test scores of students in small classes and students in large classes?\nAngrist & Levy (1999) studied this by taking advantage of a rule in Israeli schools, where cohorts of &gt;40 students are split into two smaller classes"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#is-there-a-benefit-to-small-class-sizes-1",
    "href": "fall_sta235_2025/week_01/week_14.html#is-there-a-benefit-to-small-class-sizes-1",
    "title": "Data Science for Business Applications",
    "section": "Is there a benefit to small class sizes?",
    "text": "Is there a benefit to small class sizes?\nKey idea: Students in cohorts just below 40 students are essentially identical to students in cohorts just above 40,\nbut the ones in the latter group will get a smaller class."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#creating-the-rdd-model",
    "href": "fall_sta235_2025/week_01/week_14.html#creating-the-rdd-model",
    "title": "Data Science for Business Applications",
    "section": "Creating the RDD model",
    "text": "Creating the RDD model\n\nDefine a treatment variable:\n\\[\nT =\n\\begin{cases}\n  1, & \\text{if the cohort is split into two classes} \\\\\n  0, & \\text{if the cohort is kept intact in one class}\n\\end{cases}\n\\]\nRecenter the selection variable so the cutoff is at 0:\n\\[\nX = (\\text{Cohort size}) - 40\n\\]\nFit a model predicting reading scores from both (X) and (T):\n\\[\n\\hat{Y} = \\hat\\beta_0 + \\hat\\beta_1 X + \\hat\\beta_2 T\n\\]\n\nThe coefficient ( _2 ) is the causal effect we’re looking for!"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#rdd-model-in-r",
    "href": "fall_sta235_2025/week_01/week_14.html#rdd-model-in-r",
    "title": "Data Science for Business Applications",
    "section": "RDD model in R",
    "text": "RDD model in R\n\nschool &lt;- school %&gt;% \n  mutate(\n    treatment = ifelse(cohort.size &gt; 40, 1, 0),\n    selection = cohort.size - 40\n  )\n\nrdd1 &lt;- lm(read ~ selection + treatment, data = school)\nsummary(rdd1)\n\n\nCall:\nlm(formula = read ~ selection + treatment, data = school)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.195  -5.572   1.537   6.617  17.269 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  69.7556     1.2697  54.939 &lt;0.0000000000000002 ***\nselection    -0.1195     0.2020  -0.592              0.5545    \ntreatment     4.0031     2.1511   1.861              0.0638 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.135 on 294 degrees of freedom\nMultiple R-squared:  0.02237,   Adjusted R-squared:  0.01572 \nF-statistic: 3.363 on 2 and 294 DF,  p-value: 0.03596"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#but-wait",
    "href": "fall_sta235_2025/week_01/week_14.html#but-wait",
    "title": "Data Science for Business Applications",
    "section": "But wait!",
    "text": "But wait!\nOur first RDD model is forcing the two lines to have the same slope; that isn’t a great fit for the data:"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#but-wait-1",
    "href": "fall_sta235_2025/week_01/week_14.html#but-wait-1",
    "title": "Data Science for Business Applications",
    "section": "But wait!",
    "text": "But wait!\n\nTo allow the two slopes to differ, we can add an interaction term so that the slope of (\\(X\\)) is different for (\\(T=0\\)) (cohort kept intact) and (\\(T=1\\)) (cohort split into smaller classes):\n\n\\[\n\\hat{Y} = \\hat\\beta_0 + \\hat\\beta_1 X + \\hat\\beta_2 T + \\hat\\beta_3 (T X)\n\\]\n\nThe coefficient on (\\(T\\)) (\\(\\hat\\beta_2\\)) is our estimate of the causal effect of the treatment."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#regression-summary",
    "href": "fall_sta235_2025/week_01/week_14.html#regression-summary",
    "title": "Data Science for Business Applications",
    "section": "Regression Summary",
    "text": "Regression Summary\n\nsummary(lm(read ~ selection * treatment, data = school))\n\n\nCall:\nlm(formula = read ~ selection * treatment, data = school)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.618  -6.102   1.341   6.922  17.249 \n\nCoefficients:\n                    Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)          66.6294     1.8141  36.729 &lt;0.0000000000000002 ***\nselection            -0.8945     0.3806  -2.350              0.0194 *  \ntreatment             5.6641     2.2439   2.524              0.0121 *  \nselection:treatment   1.0720     0.4477   2.395              0.0173 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.063 on 293 degrees of freedom\nMultiple R-squared:  0.04113,   Adjusted R-squared:  0.03132 \nF-statistic:  4.19 on 3 and 293 DF,  p-value: 0.00634"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#a-better-rdd-model",
    "href": "fall_sta235_2025/week_01/week_14.html#a-better-rdd-model",
    "title": "Data Science for Business Applications",
    "section": "A better RDD model",
    "text": "A better RDD model"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#conclusion",
    "href": "fall_sta235_2025/week_01/week_14.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nFrom our data we can conclude that smaller class sizes cause reading scores to increase by about 5.7 points.\nRDD is usually great for internal validity, but there are many threats to external validity: e.g., would this generalize to different grade levels? Schools outside of Israel?"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_13.html",
    "href": "fall_sta235_2025/week_01/week_13.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "If we run a regression predicting \\(Y\\) from \\(X\\) and find that \\(X\\) is a significant predictor of \\(Y\\), we would like to conclude that \\(X\\) causes \\(Y\\). But it might be the case that:\n\n\\(Y\\) actually causes \\(X\\).\n\\(X\\) and \\(Y\\) are not actually related in the population; they happen to be correlated in the sample just by chance.\nA common variable \\(Z\\) (a confounder) causes both \\(X\\) and \\(Y\\).\nA common variable \\(W\\) (a collider) is caused by both \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html",
    "href": "fall_sta235_2025/week_01/week_14.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Although they are powerful for inferring causation, RCTs are hard to pull off:\n\nThey can be incredibly expensive (e.g., Phase 3 clinical trial)\nCompliance with the treatment protocol isn’t perfect (e.g., low-calorie diet, picking up the phone)\nIt can be hard to generalize beyond the participants involved in the study if they aren’t representative (e.g., psychology experiments conducted on college students)\nThey can be impractical (e.g., effect of education on later earnings) or even unethical (e.g., seatbelts, parachutes, medical trials)"
  },
  {
    "objectID": "fall_sta235_2025/week_01/week_14.html#pre-vs-post-comparison",
    "href": "fall_sta235_2025/week_01/week_14.html#pre-vs-post-comparison",
    "title": "Data Science for Business Applications",
    "section": "Pre vs post comparison",
    "text": "Pre vs post comparison\n\n\n\n\nBefore\nAfter\nDifference\n\n\n\n\nNew Jersey\n20.44\n21.03\n0.59\n\n\n\n\nEmployment went up by 0.59 employees per store in NJ. Can we interpret this as a causal effect?\n\n\nNo! We cannot distinguish the effect of the minimum wage increase from other things that changed in NJ at the same time."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html",
    "href": "sp_sta235_2026/week_01.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Use regression to build predictive models\nUnderstand the benefits and limitations of the models we build\nGiven a new business situation, select an appropriate model, build it, measure its effectiveness, and effectively communicate the results\nThis is a practical course!\n\n\n\n\n\nWhy bother learning this stuff when we can get ChatGPT to do data analysis for us?\nAI (and computing in general) is only useful when you have the expertise to be able to recognize the correctness (or not) of its output\nIn this class, you’ll develop that expertise!\n\n\n\n\n\nInstructor: Henrique Bolfarine, Ph.D.\n\nOffice hours: Tuesdays 11:00 AM - 12:00 PM (GSB 3.140 A)\nEmail: henrique.bolfarine@austin.utexas.edu\n\nCourse Assistants:\n\nOffice hours: Many TA/CA office hours every week (both in person and on Zoom) - This should be your first option!\nYou can ask any of the TAs/CAs about course content, but go to Ezgi for questions about logistics"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#course-goals",
    "href": "sp_sta235_2026/week_01.html#course-goals",
    "title": "Data Science for Business Applications",
    "section": "Course Goals",
    "text": "Course Goals\n\nUse regression to build predictive models\nUnderstand the benefits and limitations of the models we build\nGiven a new business situation, select an appropriate model, build it, measure its effectiveness, and effectively communicate the results\nThis is a practical course!"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#why-does-this-course-exist",
    "href": "sp_sta235_2026/week_01.html#why-does-this-course-exist",
    "title": "Data Science for Business Applications",
    "section": "Why Does This Course Exist?",
    "text": "Why Does This Course Exist?\n\nWhy bother learning this stuff when we can get ChatGPT to do data analysis for us?\nAI (and computing in general) is only useful when you have the expertise to be able to recognize the correctness (or not) of its output\nIn this class, you’ll develop that expertise!"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#about-the-course-staff",
    "href": "sp_sta235_2026/week_01.html#about-the-course-staff",
    "title": "Data Science for Business Applications",
    "section": "About the Course Staff",
    "text": "About the Course Staff\n\nInstructor: Henrique Bolfarine, Ph.D.\n\nOffice hours: Tuesdays 11:00 AM - 12:00 PM (GSB 3.140 A)\nEmail: henrique.bolfarine@austin.utexas.edu\n\nCourse Assistants:\n\nOffice hours: Many TA/CA office hours every week (both in person and on Zoom) - This should be your first option!\nYou can ask any of the TAs/CAs about course content, but go to Ezgi for questions about logistics"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#course-structure",
    "href": "sp_sta235_2026/week_01.html#course-structure",
    "title": "Data Science for Business Applications",
    "section": "Course Structure",
    "text": "Course Structure\n\nUnits\n\nUnit A: Fundamentals of regression modeling\nUnit B: Applications and extensions\n\nCanvas\n\nMake sure you can log in and are enrolled in STA 235 in Canvas"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#check-out-the-home-page-for-the-weekly-schedule-and-to-meet-the-course-staff",
    "href": "sp_sta235_2026/week_01.html#check-out-the-home-page-for-the-weekly-schedule-and-to-meet-the-course-staff",
    "title": "Data Science for Business Applications",
    "section": "Check out the home page for the weekly schedule and to meet the course staff",
    "text": "Check out the home page for the weekly schedule and to meet the course staff"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#statistical-computing",
    "href": "sp_sta235_2026/week_01.html#statistical-computing",
    "title": "Data Science for Business Applications",
    "section": "Statistical Computing",
    "text": "Statistical Computing\n\n\n\nWe will use R and RStudio for statistical analysis throughout the course\nMake sure both are installed on your laptop and bring it to every class\nIf you aren’t comfortable with R/RStudio from STA 301, don’t worry!"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#weekly-cadence-for-a-particular-topic",
    "href": "sp_sta235_2026/week_01.html#weekly-cadence-for-a-particular-topic",
    "title": "Data Science for Business Applications",
    "section": "Weekly Cadence for a Particular Topic",
    "text": "Weekly Cadence for a Particular Topic\n\nDue by the start of class on Monday/Tuesday: Perusall pre-class video/reading discussion covering the topic\nDuring class on Monday/Tuesday: Lecture, activities, practice topic\nDue by 11:59 PM the following Sunday/Monday: Homework covering the topic\nThe following Thursday at the beginning of class: Checkpoint Quiz on that topic"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#pre-class-work",
    "href": "sp_sta235_2026/week_01.html#pre-class-work",
    "title": "Data Science for Business Applications",
    "section": "Pre-Class Work",
    "text": "Pre-Class Work\n\nThis is a fast-paced course, so it’s essential that you think about the material before class.\nWe will use Perusall for pre-class video and reading assignments.\nUse Perusall to ask your classmates questions, and share your knowledge, thoughts, and opinions.\nThis helps you better understand the material and will help me gear class time to what topics you are having the most trouble with."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#pre-class-work-1",
    "href": "sp_sta235_2026/week_01.html#pre-class-work-1",
    "title": "Data Science for Business Applications",
    "section": "Pre-Class Work",
    "text": "Pre-Class Work\n\nPre-class assignments (typically videos) are due at the start of each class.\nAim to chime in with at least a few thoughtful questions, responses, or comments for each reading assignment.\nGrading is based on effort and thoughtfulness of your questions and comments and your engagement with classmates and the text.\nEach assignment is scored 0-3, but with a reasonable effort you will get a 3 on each one (so don’t worry about your grade)."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#homework",
    "href": "sp_sta235_2026/week_01.html#homework",
    "title": "Data Science for Business Applications",
    "section": "Homework",
    "text": "Homework\n\n\n\nWhy homework?\nHomework is due each week at 11:59 PM the night before class and submitted through Canvas.\nAutomatically graded; resubmit as many times as you want!\nOK to work together, but try the problems on your own first for maximum benefit."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#checkpoint-quizzes",
    "href": "sp_sta235_2026/week_01.html#checkpoint-quizzes",
    "title": "Data Science for Business Applications",
    "section": "Checkpoint Quizzes",
    "text": "Checkpoint Quizzes\n\nIt is critical in this course to stay on top of things and not fall behind.\nCheckpoint Quiz at the start of each class will help you ensure that you are really learning the material and give you an early heads-up if you aren’t.\nWe’ll drop your lowest quiz score from each unit (A and B).\nYou’ll have access to RStudio and a “cheat sheet” during quizzes (don’t spend time memorizing anything!)."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#mastery-exams",
    "href": "sp_sta235_2026/week_01.html#mastery-exams",
    "title": "Data Science for Business Applications",
    "section": "Mastery Exams",
    "text": "Mastery Exams\n\nEach unit concludes with a Mastery Exam:\n\nUnit A: March 9 or 10 at 7 PM\nUnit B: April 30\n\nYou’ll have access to RStudio using Posit and a “cheat sheet” during exams (don’t spend time memorizing anything!)."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#assessment-grading",
    "href": "sp_sta235_2026/week_01.html#assessment-grading",
    "title": "Data Science for Business Applications",
    "section": "Assessment Grading",
    "text": "Assessment Grading\n\nUnit A has 7 Checkpoint Quizzes and Unit B has 6.\nFor each unit, there will be a Quiz drop (lowest grade)."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#grading",
    "href": "sp_sta235_2026/week_01.html#grading",
    "title": "Data Science for Business Applications",
    "section": "Grading",
    "text": "Grading\n\n\n\n\nComponent\nPoints\n\n\n\n\nPre-class work (Perusall)\n44\n\n\nClass Participation\n86\n\n\nHomework (13)\n195\n\n\nCheckpoint Quizze (13)\n275\n\n\nExam A\n200\n\n\nExam B\n200\n\n\nTotal\n1,000"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#getting-help",
    "href": "sp_sta235_2026/week_01.html#getting-help",
    "title": "Data Science for Business Applications",
    "section": "Getting Help",
    "text": "Getting Help\n\nMy office hours: Schedule on Canvas.\nTA/CA office hours: Schedule on Canvas.\nPost questions in videos in Perusall (for questions about the course material).\nPost questions in group chats in Perusall (for general questions about the course, or homework questions).\nPerusall assignments are graded by the app not by the instructor."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#section",
    "href": "sp_sta235_2026/week_01.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "What personal characteristics about an instructor do you think are predictive of the scores they receive on student evaluations?"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#hamermesh-parker-2005-data-set",
    "href": "sp_sta235_2026/week_01.html#hamermesh-parker-2005-data-set",
    "title": "Data Science for Business Applications",
    "section": "Hamermesh & Parker (2005) Data Set",
    "text": "Hamermesh & Parker (2005) Data Set\n\nStudent evaluations of \\(N=463\\) instructors at UT Austin, 2000-2002\nFor each instructor:\n\\(\\texttt{eval}\\): average student evaluation of teacher\n\\(\\texttt{beauty}\\): average beauty score from a six-student panel\n\\(\\texttt{gender}\\): male or female\n\\(\\texttt{credits}\\): single- or multi-credit course\n\\(\\texttt{age}\\): age of instructor\n\n(and more…)"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#explore-the-data-texttteval",
    "href": "sp_sta235_2026/week_01.html#explore-the-data-texttteval",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: \\(\\texttt{eval}\\)",
    "text": "Explore the data: \\(\\texttt{eval}\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#explore-the-data-textttbeauty",
    "href": "sp_sta235_2026/week_01.html#explore-the-data-textttbeauty",
    "title": "Data Science for Business Applications",
    "section": "Explore the data: \\(\\texttt{beauty}\\)",
    "text": "Explore the data: \\(\\texttt{beauty}\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#section-1",
    "href": "sp_sta235_2026/week_01.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Do you think there is a positive or negative relationship between beauty and teaching ratings?"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#explore-the-data",
    "href": "sp_sta235_2026/week_01.html#explore-the-data",
    "title": "Data Science for Business Applications",
    "section": "Explore the data",
    "text": "Explore the data"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#section-2",
    "href": "sp_sta235_2026/week_01.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The correlation \\(r\\) between two variables \\(X\\) and \\(Y\\) measures the strength of the linear relationship between them. Correlation ranges from \\(-1\\) (perfect negative relationship) to \\(0\\) (no relationship) to \\(1\\) (perfect positive relationship)."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#correlation",
    "href": "sp_sta235_2026/week_01.html#correlation",
    "title": "Data Science for Business Applications",
    "section": "Correlation",
    "text": "Correlation\n\ncor(profs$eval, profs$beauty)\n\n[1] 0.189\n\n\n\nHow can we interpret this?\nThe $ sign accesses the variables in the data set profs.csv."
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#the-simple-regression-fit",
    "href": "sp_sta235_2026/week_01.html#the-simple-regression-fit",
    "title": "Data Science for Business Applications",
    "section": "The simple regression fit",
    "text": "The simple regression fit\n\nmodel_simple &lt;- lm(eval ~ beauty, data = profs)\nsummary(model_simple)\n\n\nCall:\nlm(formula = eval ~ beauty, data = profs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8002 -0.3630  0.0725  0.4021  1.1037 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   3.9983     0.0253  157.73 &lt; 0.0000000000000002 ***\nbeauty        0.1330     0.0322    4.13             0.000042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.545 on 461 degrees of freedom\nMultiple R-squared:  0.0357,    Adjusted R-squared:  0.0336 \nF-statistic: 17.1 on 1 and 461 DF,  p-value: 0.0000425\n\nbprof = coef(model_simple)"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#interpreting-the-coefficients",
    "href": "sp_sta235_2026/week_01.html#interpreting-the-coefficients",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nIntercept:\n\nWhen beauty = 0 (average), predicted eval = 3.998 points\nThe average evaluation for an average-looking instructor\nMeaningful this time!\n\nSlope:\n\nFor a one unit (standard deviation) increase in beauty, the predicted eval increases by 0.13 points\nComparing two instructors who differ by 1 SD on beauty, on average the more attractive instructor has an eval score 0.13 points higher"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#is-this-the-whole-story",
    "href": "sp_sta235_2026/week_01.html#is-this-the-whole-story",
    "title": "Data Science for Business Applications",
    "section": "Is this the whole story?",
    "text": "Is this the whole story?\nProbably not!\n\nLots of other predictors to consider\nCould the positive association be due to another variable that isn’t in the model yet (think bedrooms and living area)?\nAge might be important here – how? - On average, older instructors are less hot - As they age instructors might get better at teaching (experience) – or worse (stale or out of touch)"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#is-age-actually-correlated-with-beauty",
    "href": "sp_sta235_2026/week_01.html#is-age-actually-correlated-with-beauty",
    "title": "Data Science for Business Applications",
    "section": "Is age actually correlated with beauty?",
    "text": "Is age actually correlated with beauty?"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#multiple-regression-eval-on-beauty-and-age",
    "href": "sp_sta235_2026/week_01.html#multiple-regression-eval-on-beauty-and-age",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression: eval on beauty and age",
    "text": "Multiple regression: eval on beauty and age\n\nmodel_beauty_age &lt;- lm(eval ~ beauty + age, data = profs)\nsummary(model_beauty_age)\n\n\nCall:\nlm(formula = eval ~ beauty + age, data = profs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8024 -0.3651  0.0741  0.3991  1.1021 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 3.984401   0.133730   29.79 &lt; 0.0000000000000002 ***\nbeauty      0.134063   0.033744    3.97             0.000082 ***\nage         0.000287   0.002715    0.11                 0.92    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.546 on 460 degrees of freedom\nMultiple R-squared:  0.0358,    Adjusted R-squared:  0.0316 \nF-statistic: 8.53 on 2 and 460 DF,  p-value: 0.00023"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#interpreting-the-coefficients-1",
    "href": "sp_sta235_2026/week_01.html#interpreting-the-coefficients-1",
    "title": "Data Science for Business Applications",
    "section": "Interpreting the coefficients",
    "text": "Interpreting the coefficients\n\nIn this case, whether we compare instructors of the same age or not, we get the same answer: Hotter instructors get higher evaluations on average.\nGood(ish) news – we ruled out one alternative explanation for the association between beauty and eval\nAre there others?"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#summary",
    "href": "sp_sta235_2026/week_01.html#summary",
    "title": "Data Science for Business Applications",
    "section": "Summary",
    "text": "Summary\n\nAs we add or remove variables in regression models, the coefficients on other variables can go up, down, or stay about the same\nIt all depends on the relationships among the predictors and between the predictors and the outcome\nTo understand what’s happening – and which variables we should include to estimate the right effect – we need to understand the effect we’re estimating and whether it’s the one we want\nIn a few weeks we’ll learn how to build models that estimate the right effects, and how to build the best predictve models (these aren’t always the same!)"
  },
  {
    "objectID": "sp_sta235_2026/week_01.html#next-time-errors-and-uncertainty",
    "href": "sp_sta235_2026/week_01.html#next-time-errors-and-uncertainty",
    "title": "Data Science for Business Applications",
    "section": "Next time: Errors and uncertainty",
    "text": "Next time: Errors and uncertainty\nThe other missing part of the story:\n\nDoes the association between beauty and evals hold among ALL instructors, or just in this sample? - Could we be looking at a chance association that would disappear if we could get data on everyone?\nHow accurately can we predict the sale price of a house from its size and other factors? - How do we quantify prediction errors? Even with many variables our predictions will be off by some amount. How wrong should we expect to be?"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#predicting-price-from-area",
    "href": "sp_sta235_2026/week_02.html#predicting-price-from-area",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Predicting price from area",
    "text": "Predicting price from area"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#residuals-and-fitted-values",
    "href": "sp_sta235_2026/week_02.html#residuals-and-fitted-values",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Residuals and fitted values",
    "text": "Residuals and fitted values\n\nThe fitted value (“y hat”) for observation \\(i\\) is the predicted value from the regression line: \\[\n\\widehat{y}_i = \\widehat{\\beta_0} + \\widehat{\\beta_1} \\times \\text{area}_i\n\\]\nThe residual is the corresponding prediction error: \\[e_i = y_i - \\widehat{y}_i = \\text{observed price} - \\text{predicted price}\\]\nWhen a linear model fits well,\n\nThe fitted line tells us about the part of price predictable from area\nThe residuals tell us about the part of price not predictable from area"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#visualizing-residuals-and-fitted-values",
    "href": "sp_sta235_2026/week_02.html#visualizing-residuals-and-fitted-values",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Visualizing residuals and fitted values",
    "text": "Visualizing residuals and fitted values"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#what-does-a-residual-tell-us",
    "href": "sp_sta235_2026/week_02.html#what-does-a-residual-tell-us",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "What does a residual tell us?",
    "text": "What does a residual tell us?"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#what-is-a-typical-residual",
    "href": "sp_sta235_2026/week_02.html#what-is-a-typical-residual",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "What is a “typical” residual?",
    "text": "What is a “typical” residual?\n\nThe residual tells us the prediction error for a given point\nThe standard deviation of the residuals tells us the typical size of these prediction errors (in absolute value)\nIt’s a measure of how variable our prediction errors are\n\nLow variability: our predictions are usually close to actual values\nHigh variability: our predictions are often far off from actual values\n\nIn R, this is called the Residual Standard Error (RSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#section",
    "href": "sp_sta235_2026/week_02.html#section",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "",
    "text": "summary(fit)\n\n\nCall:\nlm(formula = price ~ area, data = houses)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-604840  -90641    2955   83298  519720 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  4296.668  21017.955  0.2044              0.8381    \narea          279.139     10.101 27.6337 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 155630 on 506 degrees of freedom\nMultiple R-squared:  0.60146,   Adjusted R-squared:  0.60067 \nF-statistic: 763.62 on 1 and 506 DF,  p-value: &lt; 0.000000000000000222\n\n\nFrom line 16: RSE = $155,630"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-r-squared",
    "href": "sp_sta235_2026/week_02.html#r2-r-squared",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) (R-squared)",
    "text": "\\(R^2\\) (R-squared)\n\\(R^2\\) is a measure of the strength of the linear relationship between the \\(Y\\) and all \\(X\\)’s in the linear regression\n\n\n\n\\(R^2\\) ranges from 0 to 1\n\n0 = no linear realtaionship\n1 = perfect linear relationship\n\nRelated to correlation\n\n\n\\(Cor(X,Y)\\) measures linear association between two variables\n\nBetween -1 and 1\nSign indicates direction of linear relationship\nMagnitude indicates strength of linear relationship (0=none, \\(\\pm 1\\)=perfect)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#correlation",
    "href": "sp_sta235_2026/week_02.html#correlation",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#defining-r2-r-squared",
    "href": "sp_sta235_2026/week_02.html#defining-r2-r-squared",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Defining \\(R^2\\) (R-squared)",
    "text": "Defining \\(R^2\\) (R-squared)\n\\(R^2\\) has two equivalent definitions:\n\nThe squared correlation between observed and fitted values from the regression model\nThe proportion of variance in the outcome \\(Y\\) “explained” by the regression model"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#finding-r2",
    "href": "sp_sta235_2026/week_02.html#finding-r2",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Finding \\(R^2\\)",
    "text": "Finding \\(R^2\\)\n\nsummary(fit)\n\n\nCall:\nlm(formula = price ~ area, data = houses)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-604840  -90641    2955   83298  519720 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  4296.668  21017.955  0.2044              0.8381    \narea          279.139     10.101 27.6337 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 155630 on 506 degrees of freedom\nMultiple R-squared:  0.60146,   Adjusted R-squared:  0.60067 \nF-statistic: 763.62 on 1 and 506 DF,  p-value: &lt; 0.000000000000000222\n\n\nLine 17 (Multiple R-squared): \\(R^2 = 0.601\\) (we’ll cover adjusted \\(R^2\\) later)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-cory-ŷ2",
    "href": "sp_sta235_2026/week_02.html#r2-cory-ŷ2",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2 = Cor(Y, Ŷ)^2\\)",
    "text": "\\(R^2 = Cor(Y, Ŷ)^2\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-generalized-correlation",
    "href": "sp_sta235_2026/week_02.html#r2-as-generalized-correlation",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as generalized correlation",
    "text": "\\(R^2\\) as generalized correlation\n\n\\(R^2\\) tells us how closely the predicted values track actual values\nIn a simple linear model, it’s the same as \\(Cor(X,Y)^2\\)\nSo you can think of \\(R^2\\) as a generalization of correlation to multiple X’s"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-generalized-correlation-1",
    "href": "sp_sta235_2026/week_02.html#r2-as-generalized-correlation-1",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as generalized correlation",
    "text": "\\(R^2\\) as generalized correlation\nIt inherits the same limitations as correlation:\n\nLow \\(R^2\\) does not imply\n\nNo “meaningful” linear relationship\nNo strong nonlinear relationship"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-generalized-correlation-2",
    "href": "sp_sta235_2026/week_02.html#r2-as-generalized-correlation-2",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as generalized correlation",
    "text": "\\(R^2\\) as generalized correlation\n\nHigh \\(R^2\\) does not imply\n\nA causal relationship\nA true linear relationship\nPrediction errors are small enough to be useful\nThe model will predict will for future data"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained",
    "href": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as proportion of variance “explained”",
    "text": "\\(R^2\\) as proportion of variance “explained”\n\nIf \\(R^2\\) is high then our predictions track the observed values closely\n\\(R^2\\) measures how much unpredictability (variance) in the outcome “goes away” when we ues X to predict"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained-1",
    "href": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained-1",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as proportion of variance “explained”",
    "text": "\\(R^2\\) as proportion of variance “explained”\n\\[\nR^2 = \\frac{\\overbrace{\\text{variance of Y} - \\text{variance of residuals}}^{\\text{variance ``explained''}}}{\\text{variance of Y}}\n\\]\n\nCommon phrasing: “\\(R^2\\) is the proportion of variance in \\(Y\\) explained by the model”"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained-2",
    "href": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained-2",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as proportion of variance “explained”",
    "text": "\\(R^2\\) as proportion of variance “explained”\n\nThe variance of \\(Y\\) is also the variance of the residuals under a simple regression model where we force \\(\\hat\\beta_1=0\\)\nSo we can think about \\(R^2\\) as the reduction in variance (average size) of our residuals when using X to predict Y compared to not using X at all\nIf we force \\(\\hat\\beta_1=0\\), all of our predictions will be \\(\\bar y\\) (the average \\(Y\\) value)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained-3",
    "href": "sp_sta235_2026/week_02.html#r2-as-proportion-of-variance-explained-3",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) as proportion of variance “explained”",
    "text": "\\(R^2\\) as proportion of variance “explained”\n\\[\nR^2 = \\frac{246,274^2 - 155,630^2}{246,274^2}\\approx 0.601\n\\]\n\nWe say \\(X\\) “explained” 60.1% (\\(R^2=0.601\\)) of the total variability in \\(Y\\)\nRemember: this is just a way of quantifying how closely the line tracks actual values\n\nYou can predict something without being able to explain why it happens!"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#multiple-or-adjusted-r2",
    "href": "sp_sta235_2026/week_02.html#multiple-or-adjusted-r2",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Multiple or Adjusted \\(R^2\\)?",
    "text": "Multiple or Adjusted \\(R^2\\)?\n\nThe only difference is how residual variance is estimated\nIn practice, the difference is usually small\n\nIf they’re very different then neither is very reliable\n\nOn a HW/Quiz/Exam you can use either\nLater we’ll see how to get more accurate measures of prediction error"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-or-rse",
    "href": "sp_sta235_2026/week_02.html#r2-or-rse",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) or RSE?",
    "text": "\\(R^2\\) or RSE?\n\n\\(R^2\\) measures the strength of the linear relationship between Y and the X’s\n\nSince it’s a relative measure, we don’t know how big our prediction errors are likely to be\n\n\\(RSE\\) measures the typical size of our prediction errors using X to predict Y\n\nSince it’s an absolute measure, we don’t know how strong the linear relationship is\n\n\nBoth can be useful, but in practice RSE is usually more relevant."
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-or-rse-1",
    "href": "sp_sta235_2026/week_02.html#r2-or-rse-1",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) or RSE?",
    "text": "\\(R^2\\) or RSE?\nPredicting the price of a house from its area:\n\nR^2 = 0.601\nRSE = $155,630\n\nPretty strong linear relationship, but typical prediction errors are still very large!"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#r2-doesnt-add-up",
    "href": "sp_sta235_2026/week_02.html#r2-doesnt-add-up",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "\\(R^2\\) doesn’t add up",
    "text": "\\(R^2\\) doesn’t add up\n\nWe can use \\(R^2\\) to measure “independent” contributions of multiple \\(X\\) variables\nCompare \\(R^2\\) from models with different sets of \\(X\\) variables\n\nArea: \\(R^2\\) = 0.601\nBeds: \\(R^2\\) = 0.137\nArea and Beds \\(R^2\\) = 0.615 \\(\\neq 0.601+0.137\\)\n\nAdding beds once you know area -&gt; only about 1.4% more variance explained"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#why",
    "href": "sp_sta235_2026/week_02.html#why",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Why?",
    "text": "Why?\n\n\nArea already contains a lot of information about beds\nThe new information we get from introducing beds is not that useful for predicting prices\nCompare this to starting with beds and adding area – we get a big boost in \\(R^2\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#prediction-isnt-everything",
    "href": "sp_sta235_2026/week_02.html#prediction-isnt-everything",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Prediction isn’t everything",
    "text": "Prediction isn’t everything\n\nNotice that the area and area+beds model have very similar \\(R^2\\) values\nThe RSE values are also similar\n\nArea only: 155627\nArea + Beds: 153047\n\nBoth models predict prices about equally well (or badly)\n\nBut they tell very different stories about what an additional sqft is worth!"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#accounting-for-error-and-uncertainty",
    "href": "sp_sta235_2026/week_02.html#accounting-for-error-and-uncertainty",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Accounting for error and uncertainty",
    "text": "Accounting for error and uncertainty\n\nSo far we’ve focused on estimated coefficients and predictions\nWe know our predictions will be wrong – how wrong are the likely to be?\nWe know our coefficients don’t match the true population values – how close are they?"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#the-simple-linear-regression-model",
    "href": "sp_sta235_2026/week_02.html#the-simple-linear-regression-model",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "The simple linear regression model",
    "text": "The simple linear regression model\nWe need a model for our data to answer these questions. Our model has four components:\n\nLinearity\nIndependence of prediction errors\nNormality of prediction errors\nEqual variance of prediction errors"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#linearity",
    "href": "sp_sta235_2026/week_02.html#linearity",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Linearity",
    "text": "Linearity\nThe relationship between \\(Y\\) and each \\(X\\) is linear:\n\\(Y = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_{2} + ... + \\beta_p X_{p} + \\epsilon\\)\n\n\\(\\beta_0, \\beta_1, ..., \\beta_p\\) are the true/population regression coefficients\n\\(\\epsilon\\) is the true error if we predict \\(Y\\) using the true regression coefficients\n\nOur other assumptions are about those true error terms \\(\\epsilon\\)…"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#independence",
    "href": "sp_sta235_2026/week_02.html#independence",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Independence",
    "text": "Independence\n\nEach observation has an independent true error term\nIf my neighbor’s house sells for more than expected, that doesn’t tell me anything about whether my house will sell for more or less than expected\nIf the last house sells for less than expected, that doesn’t tell us anything about whether the next house will sell for more or less than expected"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#normality",
    "href": "sp_sta235_2026/week_02.html#normality",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Normality",
    "text": "Normality\nThe true error terms \\(\\epsilon\\) are normally distributed and centered at zero\n\nThe average error over many predictions is zero\nOver-prediction and under-prediction are equally likely\nMost errors are small, but large errors are possible"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#equal-variance",
    "href": "sp_sta235_2026/week_02.html#equal-variance",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Equal Variance",
    "text": "Equal Variance\n\nThe true error terms \\(\\epsilon\\) all have the same standard deviation \\(\\sigma\\)\nThe size of the true errors does not depend on the values of the X’s (or anything else)\n\nFor example, errors are about the same size (on average) for small and large houses"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#the-linear-model",
    "href": "sp_sta235_2026/week_02.html#the-linear-model",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "The linear model",
    "text": "The linear model\nPutting the LINE assumptions together, we have the full linear regression model:\n\\[\nY = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_{2} + ... + \\beta_p X_{p} + \\epsilon\n\\]\n\\[\n\\epsilon\\sim N(0, \\sigma^2)\\text{ independently for each observation}\n\\]\nFor an individual observation with predictors \\(X\\), we have:\n\\[\n(Y\\mid X) \\sim N(\\beta_0 + \\beta_1 X_{1} + \\beta_2 X_{2} + ... + \\beta_p X_{p}, \\sigma^2)\n\\]"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#visualizing-the-linear-model",
    "href": "sp_sta235_2026/week_02.html#visualizing-the-linear-model",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#estimates-vs-true-values",
    "href": "sp_sta235_2026/week_02.html#estimates-vs-true-values",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Estimates vs True Values",
    "text": "Estimates vs True Values\n\nOur \\(\\hat \\beta_0, \\hat \\beta_1, ..., \\hat \\beta_p\\) are just estimates of the true \\(\\beta_0, \\beta_1, ..., \\beta_p\\)\nThe residuals estimate the true prediction errors\nThe RSE (standard deviation of residuals) is an estimate of the true \\(\\sigma\\) (error standard deviation)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#accounting-for-uncertainty-in-estimates",
    "href": "sp_sta235_2026/week_02.html#accounting-for-uncertainty-in-estimates",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Accounting for uncertainty in estimates",
    "text": "Accounting for uncertainty in estimates\n\nIf we want to know how uncertain our estimates are, we need to quantify their variability.\nWe can do this using the sampling distribution of our estimates (how much they change over repeated samples)\nMore specifically, we can compute confidence intervals"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#confidence-intervals",
    "href": "sp_sta235_2026/week_02.html#confidence-intervals",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nA confidence interval gives a range of plausible values for a population parameter (e.g., a regression coefficient)\nThese are possible values of the true coefficients that are consistent with our observed data at a given level of confidence (e.g., 95%)\nIf our LINE assumptions hold, these are easy to get:\n\n\nconfint(fit)\n\n                   2.5 %      97.5 %\n(Intercept) -36996.53678 45589.87298\narea           259.29353   298.98532"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#returning-to-course-evaluations",
    "href": "sp_sta235_2026/week_02.html#returning-to-course-evaluations",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Returning to course evaluations",
    "text": "Returning to course evaluations\n\n\n\nCall:\nlm(formula = eval ~ beauty, data = profs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8002 -0.3630  0.0725  0.4021  1.1037 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   3.9983     0.0253  157.73 &lt; 0.0000000000000002 ***\nbeauty        0.1330     0.0322    4.13             0.000042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.545 on 461 degrees of freedom\nMultiple R-squared:  0.0357,    Adjusted R-squared:  0.0336 \nF-statistic: 17.1 on 1 and 461 DF,  p-value: 0.0000425"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#is-there-evidence-for-a-linear-relationship",
    "href": "sp_sta235_2026/week_02.html#is-there-evidence-for-a-linear-relationship",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Is there evidence for a linear relationship?",
    "text": "Is there evidence for a linear relationship?\n\nconfint(model_simple)\n\n             2.5 % 97.5 %\n(Intercept) 3.9485  4.048\nbeauty      0.0698  0.196\n\n\n\nThe range of plausible values for the slope \\(\\beta_1\\) is (0.07, 0.20)\nNo linear relationship would mean \\(\\beta_1=0\\)\nAll the plausible values are positive, so we have statistically significant evidence of a positive linear relationship between beauty and evaluations"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#how-strong-is-the-linear-relationship",
    "href": "sp_sta235_2026/week_02.html#how-strong-is-the-linear-relationship",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "How strong is the linear relationship?",
    "text": "How strong is the linear relationship?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{Cor}(\\text{beauty, eval}) = 0.19\\)\n\\(R^2\\) = 0.036 – only 3.6% of the variance in eval is explained by beauty\nBut we have strong statistical evidence that this (weak) linear relationship holds among the population of all instructors\n\ni.e., it isn’t just a fluke in this particular sample"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#how-strong-is-the-linear-relationship-1",
    "href": "sp_sta235_2026/week_02.html#how-strong-is-the-linear-relationship-1",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "How strong is the linear relationship?",
    "text": "How strong is the linear relationship?\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember: \\(R^2\\) doesn’t tell us directly how confident we are that a linear relationship exists\nIn this case we have strong evidence of a weak linear relationship\nThis is because many factors unrelated to beauty affect eval scores"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#accounting-for-uncertainty-in-predictions",
    "href": "sp_sta235_2026/week_02.html#accounting-for-uncertainty-in-predictions",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Accounting for uncertainty in predictions",
    "text": "Accounting for uncertainty in predictions\nIf I’m selling a house I’d like to know:\n\nWhat’s the most likely sale price (best prediction)?\nWhat’s a range of likely sale prices (prediction interval)?"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#prediciton-intervals",
    "href": "sp_sta235_2026/week_02.html#prediciton-intervals",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Prediciton intervals",
    "text": "Prediciton intervals\n\n\n\n\n\n\n\n\n\n\n\n\nOur model: Actual prices for houses of a given size will:\n\nHave normal distributions\nCentered at the predicted value (location on the line)\nWith standard deviation \\(\\sigma\\)\n\n95% of the time, actual prices fall within \\(\\pm 2\\sigma\\) of the predicted price (if we knew the true line and \\(\\sigma\\))"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#prediction-intervals",
    "href": "sp_sta235_2026/week_02.html#prediction-intervals",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nUnder our LINE assumptions, we can get prediction intervals for new observations that account for both:\n\nUncertainty in our estimated regression line\nThe inherent variability in actual outcomes around that line\n\n\nnew_house = data.frame(area = 2000)\npredict(fit, newdata = new_house, \n  interval = \"prediction\")\n\n     fit    lwr    upr\n1 562576 256519 868632\n\n\n95% prediction interval: ($256,518, $868,632)"
  },
  {
    "objectID": "sp_sta235_2026/week_02.html#prediction-intervals-1",
    "href": "sp_sta235_2026/week_02.html#prediction-intervals-1",
    "title": "Error, Uncertainty and the Linear Model",
    "section": "Prediction intervals",
    "text": "Prediction intervals\n\nnew_house = data.frame(area = 2000)\npredict(fit, newdata = new_house, \n  interval = \"prediction\")\n\n     fit    lwr    upr\n1 562576 256519 868632\n\n\n\nThere’s a 95% chance that a new house will sell for a price in this range\n95% of all future houses with 2000 sq ft will sell for between $256,518 and $868,632"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-assumptions-and-potential-problems",
    "href": "sp_sta235_2026/week_03.html#regression-assumptions-and-potential-problems",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nLinear models are useful:\n\nPrediction - given a new observations\nExplanatory power- which variables affects the response\n\nBut issues in linear model are not uncommon:\n\nThey can affect the explanatory, and predictive power of our model\nThey can affect our confidence in our model\nWe will look at some of the most common problems in linear regression, and how we can fix them"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-assumptions-and-potential-problems-1",
    "href": "sp_sta235_2026/week_03.html#regression-assumptions-and-potential-problems-1",
    "title": "Data Science for Business Applications",
    "section": "Regression Assumptions, and Potential Problems",
    "text": "Regression Assumptions, and Potential Problems\nThese issues are related to:\n\nRegression model assumptions\nInfluential observations, and outliers"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#multiple-regression-assumptions",
    "href": "sp_sta235_2026/week_03.html#multiple-regression-assumptions",
    "title": "Data Science for Business Applications",
    "section": "Multiple regression assumptions",
    "text": "Multiple regression assumptions\nWe need four things to be true for regression to work properly:\n\nLinearity: \\(Y\\) is a linear function of the \\(X\\)’s (except for the prediction errors).\nIndependence: The prediction errors are independent.\nNormality: The prediction errors are normally distributed.\nEqual Variance: The variance of \\(Y\\) is the same for any value of \\(X\\) (“homoscedasticity”)."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#non-linearity",
    "href": "sp_sta235_2026/week_03.html#non-linearity",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity",
    "text": "Non-Linearity\n\nWhat we would expect to observe in a regression where there is a linear relation?\n\n\nlibrary(tidyverse)\nggplot(linear_data, aes(x=X, y=Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#residuals",
    "href": "sp_sta235_2026/week_03.html#residuals",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nLet’s plot the residuals \\(r_i\\), such that \\[r_i = y_i − \\widehat{y}_i\\] where \\(\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\) vs \\(x_i\\)\nResiduals are basically the distances between the model and the points\nHopefully identify non-linear relationships\nWe are looking for patterns or trends in the residuals"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#residuals-1",
    "href": "sp_sta235_2026/week_03.html#residuals-1",
    "title": "Data Science for Business Applications",
    "section": "Residuals",
    "text": "Residuals\n\nPlot of the residuals\nHow can these residuals be useful for us?"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-diagnostic-plots",
    "href": "sp_sta235_2026/week_03.html#regression-diagnostic-plots",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plots",
    "text": "Regression diagnostic plots\nWe’ll use regression diagnostic plots to help us evaluate some of the assumptions.\nThe residuals vs fitted graph plots:\n\nResiduals on the \\(Y\\)-axis\nFitted values (predicted \\(Y\\) values) on the \\(X\\)-axis\n\nThis graph effectively subtracts out the linear trend between \\(Y\\) and the \\(X\\)’s, so we want to see no trend left in this graph."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-diagnostic-plot",
    "href": "sp_sta235_2026/week_03.html#regression-diagnostic-plot",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check non-linearity we focus on the Residual vs. Fitted plot\n\n\nlibrary(ggfortify)\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-1",
    "href": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nFrom the Residual vs. Fitted plot, we can observe that since the residuals are evenly distributed around zero in relation to the fitted values, we have that the linear regression model is a good fit for this data.\nThis means that we are learning the linear representation contained in this data."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#non-linearity-example",
    "href": "sp_sta235_2026/week_03.html#non-linearity-example",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nWhat we would expect to observe if the relation is non linear?\n\n\nggplot(nonlinear_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#non-linearity-example-1",
    "href": "sp_sta235_2026/week_03.html#non-linearity-example-1",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nLet’s look at the residuals for this model\n\n\n\nLet’s check the residual plot"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#non-linearity-example-2",
    "href": "sp_sta235_2026/week_03.html#non-linearity-example-2",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nlm2 = lm(Y ~ X, data = nonlinear_data)\nautoplot(lm2)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#non-linearity-example-3",
    "href": "sp_sta235_2026/week_03.html#non-linearity-example-3",
    "title": "Data Science for Business Applications",
    "section": "Non-Linearity Example",
    "text": "Non-Linearity Example\n\nFrom the Residual vs. Fitted, we can observe that the residuals are not evenly distributed around zero.\nThis indicates that for lower and higher values of \\(x_i\\) our model is overpredicting and underpredicting in the mid values.\nWhat are the implications in this case?\nWorse predictions"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#independence",
    "href": "sp_sta235_2026/week_03.html#independence",
    "title": "Data Science for Business Applications",
    "section": "Independence",
    "text": "Independence\n\nIndependence means that knowing the prediction error for one observation doesn’t tell you anything about the error for another observation\nData collected over time are usually not independent\nWe can’t use regression diagnostics to decide the independence\nWe have to measure the autocorrelation of the residuals\nWe’ll get back to autocorrelation when we discuss Time Series models"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#normality-assumption",
    "href": "sp_sta235_2026/week_03.html#normality-assumption",
    "title": "Data Science for Business Applications",
    "section": "Normality assumption",
    "text": "Normality assumption\n\nWhen we’ve been interpreting residual standard error (RSE) , we’ve used the following interpretation:\n95% of our predictions will be accurate to within plus or minus \\(2\\times RSE\\).\nIn order for this to be true, the residuals have to be Normally distributed"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#normality-example",
    "href": "sp_sta235_2026/week_03.html#normality-example",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nWe can check the distribution of the residuals\n\n\nlinear_data = linear_data %&gt;% \n  mutate(resid = residuals(lm1))\n\nggplot(linear_data, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 0.2)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#normality-example-1",
    "href": "sp_sta235_2026/week_03.html#normality-example-1",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nBut how can we judge if the residuals follows a Normal distribution?\nThe key is to look at the Normal Q-Q plot, which compares the distribution of our residuals to a perfect Normal distribution.\nIf the dots line up along an (approximately) straight line, then the Normality assumption is satisfied."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-2",
    "href": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-2",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check for Normality we focus on the Normal Q-Q plot\n\n\nlm1 = lm(Y ~ X, data = linear_data)\nautoplot(lm1)\n\n\n\nIn this case the normality assumptions seem to be met"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#normality-example-2",
    "href": "sp_sta235_2026/week_03.html#normality-example-2",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nLet’s look at different data.\nIn this case the data has non Normal errors."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#normality-example-3",
    "href": "sp_sta235_2026/week_03.html#normality-example-3",
    "title": "Data Science for Business Applications",
    "section": "Normality example",
    "text": "Normality example\n\nHistogram of the residuals (right skewed)\n\n\nlm3 = lm(Y ~ X, data = non_normal)\n\nnon_normal = non_normal %&gt;% \n  mutate(resid = residuals(lm3))\n\nggplot(non_normal, aes(x = resid)) + \n  geom_histogram(color = \"grey\", binwidth = 1)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-3",
    "href": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-3",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nautoplot(lm3)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#interpretation-of-the-plot",
    "href": "sp_sta235_2026/week_03.html#interpretation-of-the-plot",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Normal Q-Q plot, we can observe that the residuals are not following the line that indicates the Normal quantiles\nThis means that our model results in non-normal residuals\nThis affects statistical tests, and confidence intervals"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#equal-variance",
    "href": "sp_sta235_2026/week_03.html#equal-variance",
    "title": "Data Science for Business Applications",
    "section": "Equal variance",
    "text": "Equal variance\n\nEqual variance is also known as “homoscedasticity”\nThe variance of \\(Y\\) should be about the same at any \\(X\\) value (or combination of values for the \\(X\\)’s).\nIn other words, the vertical spread of the points should be the same anywhere along the \\(X\\)-axis.\nIf there’s no equal variance then we might have heteroskedasticity.\nLower precision, estimates are further from the correct population value."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#equal-variance-example",
    "href": "sp_sta235_2026/week_03.html#equal-variance-example",
    "title": "Data Science for Business Applications",
    "section": "Equal variance example",
    "text": "Equal variance example\n\nThe vertical spread of the points is larger along the right side of the graph\n\n\nggplot(heter_data, aes(x = X, y = Y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-4",
    "href": "sp_sta235_2026/week_03.html#regression-diagnostic-plot-4",
    "title": "Data Science for Business Applications",
    "section": "Regression diagnostic plot",
    "text": "Regression diagnostic plot\n\nTo check for homoscidacity we focus on the Scale-Location plot\n\n\nlm4 = lm(Y ~ X, data = heter_data)\nautoplot(lm4)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#interpretation-of-the-plot-1",
    "href": "sp_sta235_2026/week_03.html#interpretation-of-the-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the plot",
    "text": "Interpretation of the plot\n\nFrom the Sacle-Location plot, we can observe that the residuals have a fan shape, indicating that there is heteroscedacity in the data.\nThis resulted in lower precision; thus, estimates are further from the correct population value."
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#example-austin-houses",
    "href": "sp_sta235_2026/week_03.html#example-austin-houses",
    "title": "Data Science for Business Applications",
    "section": "Example: Austin houses",
    "text": "Example: Austin houses\n\nmodel.houses = lm(price ~ area, data = houses)\nautoplot(model.houses)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#influential-observations",
    "href": "sp_sta235_2026/week_03.html#influential-observations",
    "title": "Data Science for Business Applications",
    "section": "Influential observations",
    "text": "Influential observations\n\nAdding a new observation with \\(X\\) near the mean of \\(X\\) doesn’t matter much even if it’s out of line with the rest of the data:\n\n\n\nThis point has high residual but low leverage. RSE = 0.5504"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#diagnostics-plot",
    "href": "sp_sta235_2026/week_03.html#diagnostics-plot",
    "title": "Data Science for Business Applications",
    "section": "Diagnostics Plot",
    "text": "Diagnostics Plot\n\nWe can observe the point with high residual on the Residual vs. Leverage plot\n\n\nlm5 = lm(Y ~ X, data = outlier_residual)\nautoplot(lm5)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#high-leverage",
    "href": "sp_sta235_2026/week_03.html#high-leverage",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can also have points with high leverage - when a point in \\(X\\) is distant from the average on \\(X\\)\n\n\n\nThis point has low residual but high leverage. RSE = 0.2956"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#high-leverage-1",
    "href": "sp_sta235_2026/week_03.html#high-leverage-1",
    "title": "Data Science for Business Applications",
    "section": "High leverage",
    "text": "High leverage\n\nWe can observe the point with high leverage on the Residual vs. Leverage plot\n\n\nlm6 = lm(Y ~ X, data = outlier_leverage)\nautoplot(lm6)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#points-with-high-influence",
    "href": "sp_sta235_2026/week_03.html#points-with-high-influence",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nPoints with high leverage and high residuals are known as influential points\n\n\n\nThis point has high residual but high leverage. RSE = 0.8281"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#points-with-high-influence-1",
    "href": "sp_sta235_2026/week_03.html#points-with-high-influence-1",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWe can observe the point with high influence on the Residual vs. Leverage plot\n\n\nlm7 = lm(Y ~ X, data = outlier_influence)\nautoplot(lm7)"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#points-with-high-influence-2",
    "href": "sp_sta235_2026/week_03.html#points-with-high-influence-2",
    "title": "Data Science for Business Applications",
    "section": "Points with high influence",
    "text": "Points with high influence\n\nWhen a case has a very unusual \\(X\\) value, it has leverage — the potential to have a big impact on the regression line\nIf the case is in line with the overall trend of the regression line, it won’t be a problem\nBut when that case also has a \\(Y\\) (high residual) value that is out of line\nWe need both a large residual and high leverage for an observation to be influential\nWe should be worried about these points\nThey affect the coefficents and predictions"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#download-data",
    "href": "sp_sta235_2026/week_03.html#download-data",
    "title": "Data Science for Business Applications",
    "section": "Download data",
    "text": "Download data\n\nlinear_data = read.csv(\"week_03_docs/linear_data.csv\")\nnonlinear_data = read.csv(\"week_03_docs/nonlinear_data.csv\")\nnon_normal = read.csv(\"week_03_docs/nonnormal_data.csv\")\nheter_data = read.csv(\"week_03_docs/heter_data.csv\")\noutlier_residual = read.csv(\"week_03_docs/outlier_data.csv\")"
  },
  {
    "objectID": "sp_sta235_2026/week_03.html#important-take-aways",
    "href": "sp_sta235_2026/week_03.html#important-take-aways",
    "title": "Data Science for Business Applications",
    "section": "Important take aways",
    "text": "Important take aways\n\nViolations of linearity are the most serious: Everything is wrong\n\nIf only linearity holds, we can make good predictions\nBut not prediction intervals, coefficient estimates, or confidence intervals\n\nViolations of normality are the least serious:\n\nPrediction intervals are wrong, but everything else is OK if sample size is large enough-\nViolations of normality/equal variance can be addressed with bootstrap techniques (beyond the scope of this class)\n\nViolations of independence are best addressed by modeling the dependence\n\nIf we don’t, our intervals will tend to be too small\n\nImportant: Often fixing a linearity problem will fix other issues as well, so we usually check this first."
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#whats-the-impact-of-gender-on-student-evaluations",
    "href": "sp_sta235_2026/week_04.html#whats-the-impact-of-gender-on-student-evaluations",
    "title": "Data Science for Business Applications",
    "section": "What’s the impact of gender on student evaluations?",
    "text": "What’s the impact of gender on student evaluations?"
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#incorporating-gender-into-the-multiple-regression-model",
    "href": "sp_sta235_2026/week_04.html#incorporating-gender-into-the-multiple-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Incorporating gender into the multiple regression model",
    "text": "Incorporating gender into the multiple regression model\n\nThe model is given by: \\[\n\\texttt{eval} = \\beta_0 + \\beta_1 \\cdot \\texttt{beauty} + \\beta_2 \\cdot \\texttt{gender} + \\epsilon\n\\] Where we have that:\n\n\\(\\texttt{eval}\\): is the response variable - (numerical)\n\\(\\texttt{beauty}\\): is a predictor - (numerical)\n\\(\\texttt{gender}\\): is a predictor - (categorical) - two groups:\n\nfemale\nmale"
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#incorporating-gender-into-the-multiple-regression-model-1",
    "href": "sp_sta235_2026/week_04.html#incorporating-gender-into-the-multiple-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Incorporating gender into the multiple regression model",
    "text": "Incorporating gender into the multiple regression model\n\n\nGender is a categorical variable (male or female in this data set) so we can’t use it as-is as a predictor.\nIdea: Recode gender into the quantitative variable 1 = male, 0 = female.\nR does this for us!\nThe ordering is totally arbitrary! If you put a categorical variable into a model, R will arbitrarily pick one category or the other as the “1” category, and make the other the “0” category."
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#run-the-regression-model",
    "href": "sp_sta235_2026/week_04.html#run-the-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Run the Regression Model",
    "text": "Run the Regression Model\n\n\noptions(scipen = 999)\nmodel1 &lt;- lm(eval ~ beauty + gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty + gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  3.88377    0.03866  100.47 &lt; 0.0000000000000002 ***\nbeauty       0.14859    0.03195    4.65           0.00000434 ***\ngendermale   0.19781    0.05098    3.88              0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 0.0000001407"
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#regression-a-model-using-gender",
    "href": "sp_sta235_2026/week_04.html#regression-a-model-using-gender",
    "title": "Data Science for Business Applications",
    "section": "Regression a model using gender",
    "text": "Regression a model using gender\n\nA multiple regression predicting evaluation score from beauty and gender effectively fits two parallel regression lines:"
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#is-there-a-generation-gap",
    "href": "sp_sta235_2026/week_04.html#is-there-a-generation-gap",
    "title": "Data Science for Business Applications",
    "section": "Is there a generation gap?",
    "text": "Is there a generation gap?\n\n\nThe variable generation is either silent (born before 1945), boomer (born 1945-1964), or genx (born after 1965).\nIs generation a significant predictor of evaluations above and beyond gender and beauty?\nTo answer this, we need a model that includes as predictors all of gender, beauty, and generation.\nBut we can’t just create a variable that is 0 for the silent generation, 1 for baby boomers, and 2 for gen X—why not?\nSolution is to pick a “reference category” and create dummy variables for the other categories."
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#ok-boomer",
    "href": "sp_sta235_2026/week_04.html#ok-boomer",
    "title": "Data Science for Business Applications",
    "section": "OK boomer",
    "text": "OK boomer\n\nLet’s arbitrarily pick boomers as a reference category:\n\n\n\nCategory\ngenx\nsilent\n\n\n\n\nBoomers\n0\n0\n\n\nGen Xers\n1\n0\n\n\nSilent Gens\n0\n1\n\n\n\n\nR will do this automatically when you add a categorical variable with 3+ categories to a regression (it will arbitrarily pick a reference category)!"
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#run-the-regression-model-1",
    "href": "sp_sta235_2026/week_04.html#run-the-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Run the Regression Model",
    "text": "Run the Regression Model\n\n\nmodel2 &lt;- lm(eval ~ beauty + gender + generation, data=profs)\nsummary(model2)\n\n\nCall:\nlm(formula = eval ~ beauty + gender + generation, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.91613 -0.36042  0.03609  0.42282  1.04398 \n\nCoefficients:\n                 Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       3.89727    0.04403  88.521 &lt; 0.0000000000000002 ***\nbeauty            0.14021    0.03304   4.243            0.0000267 ***\ngendermale        0.22230    0.05345   4.159            0.0000382 ***\ngenerationgenx   -0.02831    0.06149  -0.460               0.6454    \ngenerationsilent -0.16292    0.07992  -2.039               0.0421 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.536 on 458 degrees of freedom\nMultiple R-squared:  0.07477,   Adjusted R-squared:  0.06669 \nF-statistic: 9.253 on 4 and 458 DF,  p-value: 0.0000003386"
  },
  {
    "objectID": "sp_sta235_2026/week_04.html#analysis",
    "href": "sp_sta235_2026/week_04.html#analysis",
    "title": "Data Science for Business Applications",
    "section": "Analysis",
    "text": "Analysis\n\nAll else equal (i.e., among professors of the same gender and beauty):\n\nGen X professors are predicted to get scores that are 0.03 points below those of boomers.\nSilent gen professors are predicted to get scores that are 0.16 points below those of boomers.\nOnly the boomer/silent generation difference is statistically significant; Gen X professors are not significantly different than boomers.\nIn other words: age only seems to matter if you are really old."
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#section",
    "href": "sp_sta235_2026/week_05.html#section",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Last week, we predicted evaluation score from beauty and gender, and the model forced the lines to be parallel:\n\n\n\nWhat if we could build a more flexible model without forcing the lines to be parallel?"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#what-is-an-interaction",
    "href": "sp_sta235_2026/week_05.html#what-is-an-interaction",
    "title": "Data Science for Business Applications",
    "section": "What is an interaction?",
    "text": "What is an interaction?\n\nAn interaction is an additional term in a regression model that allows the slope of one variable to depend on the value of another."
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#does-beauty-matter-more-for-men-or-for-women",
    "href": "sp_sta235_2026/week_05.html#does-beauty-matter-more-for-men-or-for-women",
    "title": "Data Science for Business Applications",
    "section": "Does beauty matter more for men, or for women?",
    "text": "Does beauty matter more for men, or for women?\n\nA categorical variable and a quantitative variable\nWe found that for the same level of attractiveness, male professors tend to get higher evaluation scores than female professors\nBut what if the effect of beauty depends on gender (is different for men vs women)?"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interactions-beauty-and-gender",
    "href": "sp_sta235_2026/week_05.html#interactions-beauty-and-gender",
    "title": "Data Science for Business Applications",
    "section": "Interactions: Beauty and Gender",
    "text": "Interactions: Beauty and Gender\n\nThe idea is to add a new variable that is itself the product of the two variables:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_1(\\text{gender}) + \\hat\\beta_2(\\text{beauty}) + \\hat\\beta_3(\\text{gender})(\\text{beauty})\n\\]\n\nFor female professors, male = 0, so the \\(\\beta_1\\) and \\(\\beta_3\\) terms cancel out:\n\n\\[\n\\hat Y = \\hat\\beta_0 + \\hat\\beta_2(\\text{beauty})\n\\]\n\nFor male professors, male = 1, so we get both a different intercept and a different slope for beauty:\n\n\\[\n\\hat Y = (\\hat\\beta_0 + \\hat\\beta_1) + (\\hat\\beta_2+\\hat\\beta_3)(\\text{beauty})\n\\]"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interactions-beauty-and-gender-1",
    "href": "sp_sta235_2026/week_05.html#interactions-beauty-and-gender-1",
    "title": "Data Science for Business Applications",
    "section": "Interactions: Beauty and Gender",
    "text": "Interactions: Beauty and Gender\n\n\nmodel1 &lt;- lm(eval ~ beauty*gender, data=profs)\nsummary(model1)\n\n\nCall:\nlm(formula = eval ~ beauty * gender, data = profs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83820 -0.37387  0.04551  0.39876  1.06764 \n\nCoefficients:\n                  Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)        3.89085    0.03878 100.337 &lt; 0.0000000000000002 ***\nbeauty             0.08762    0.04706   1.862             0.063294 .  \ngendermale         0.19510    0.05089   3.834             0.000144 ***\nbeauty:gendermale  0.11266    0.06398   1.761             0.078910 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5361 on 459 degrees of freedom\nMultiple R-squared:  0.07256,   Adjusted R-squared:  0.0665 \nF-statistic: 11.97 on 3 and 459 DF,  p-value: 0.000000147"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#section-1",
    "href": "sp_sta235_2026/week_05.html#section-1",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Beauty seems to matter more for men than for women!\nThe gender gap is largest for good-looking professors"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#main-effects-and-interaction-effects",
    "href": "sp_sta235_2026/week_05.html#main-effects-and-interaction-effects",
    "title": "Data Science for Business Applications",
    "section": "Main effects and interaction effects",
    "text": "Main effects and interaction effects\n\nIn a model with an interaction term \\(X_1X_2\\), you must also keep the main effects: the variables that are being interacted together.\nThe main effect of \\(X_1\\) represents the predicted increase in \\(Y\\) for a 1-unit change in \\(X_1\\), holding \\(X_2\\) constant at zero.\n\nThe main effect gendermale (0.20) represents the predicted advantage, but only for an average-looking professor (beauty = 0).\n\nThe main effect beauty (0.09) represents the predicted improvement in evaluation scores for each additional beauty point, but only among women (gendermale = 0).\n\nYou can also include other variables in the model that are not being interacted!"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interactions-1",
    "href": "sp_sta235_2026/week_05.html#interactions-1",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nWe observed that there was a significant difference between the price of luxury and non-luxury cars.\nIs there a difference in the price of the car depending on what type of badge it holds?\nIn other words, does the effect of one variable (i.e., its slope coefficient) depend on the value of another?\nFor this we will include a interaction."
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interactions-2",
    "href": "sp_sta235_2026/week_05.html#interactions-2",
    "title": "Data Science for Business Applications",
    "section": "Interactions",
    "text": "Interactions\n\nThe idea is to add a term that is the product of the two variables:\n\n\\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + \\beta_2\\text{luxury} + \\beta_3 (\\text{luxury} \\times \\text{mileage}) + e\n\\]\n\nIf we have a non-luxury car, then luxury = \\(\\text{\"no\"} = 0\\), so the \\(\\beta_2\\) and \\(\\beta_3\\) terms cancel out: \\[\n\\text{price} = \\beta_0 + \\beta_1\\text{mileage} + e\n\\]\nIf we have a luxury car, then luxury = \\(\\text{\"yes\"}  = 1\\), so we get both a different intercept and a different slope for mileage: \\[\n\\text{price} = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\text{mileage} + e\n\\]"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#regression-model",
    "href": "sp_sta235_2026/week_05.html#regression-model",
    "title": "Data Science for Business Applications",
    "section": "Regression Model",
    "text": "Regression Model\n\nLet’s run the regression model\n\n\nlm3 = lm(price ~ mileage*luxury, data = cars_luxury)\nsummary(lm3)\n\n\nCall:\nlm(formula = price ~ mileage * luxury, data = cars_luxury)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-25662  -6055  -2066   3563  83626 \n\nCoefficients:\n                      Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       23893.601384   545.040269  43.838 &lt; 0.0000000000000002 ***\nmileage              -0.154697     0.009595 -16.122 &lt; 0.0000000000000002 ***\nluxuryyes         19772.433662  1092.529243  18.098 &lt; 0.0000000000000002 ***\nmileage:luxuryyes    -0.155457     0.021457  -7.245    0.000000000000606 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10880 on 2084 degrees of freedom\nMultiple R-squared:   0.36, Adjusted R-squared:  0.3591 \nF-statistic: 390.8 on 3 and 2084 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interpretation-of-the-model",
    "href": "sp_sta235_2026/week_05.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nHow do we interpret this model?\nintercept (baseline), luxury = \\(\"no\"\\) = 0: For a non-luxury car with zero mileage, the average selling price is equal to US$ 23,894.\nNow we have two cases:\nluxury = \\(\"no\"\\) = 0:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.15 in the price of non-luxury cars.\nluxury = \\(\"yes\"\\) = 1:\nmileage: For each extra increase in mileage (in miles), there will be a decrease of US$ 0.16 in the price of luxury cars on top of the decrease of US$ 0.15 of non-luxury cars."
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interpretation-of-the-model-1",
    "href": "sp_sta235_2026/week_05.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nWe also have the following interpretation:\nluxury = \\(\\text{\"yes\"}\\) = 0 \\[\n\\begin{align}\n\\text{price} &= 23,894 - 0.15\\times \\text{mileage} + 19,772 (0) - 0.16\\times \\text{mileage} (0) \\\\\n             &=  23,894 - 0.15\\times \\text{mileage}\n\\end{align}\n\\]\nluxury = \\(\\texttt{\"yes\"}\\) = 1 \\[\n\\begin{align}\n\\texttt{price} &= 23,894 - 0.15\\times \\texttt{mileage} + 19,772 (1) - 0.16\\times \\texttt{mileage} (1) \\\\\n             &=  (23,894 + 19,772) - (0.15 + 0.16) \\times \\texttt{mileage} \\\\\n             &=  43,666 - 0.31 \\times \\texttt{mileage} \\\\\n\\end{align}\n\\]\nWe have that not only the intercept change but also the slope."
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#section-2",
    "href": "sp_sta235_2026/week_05.html#section-2",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "The lines are not parallel in this case which indicates a change in the slope due to the intercation term.\n\n\n    ggplot(cars_luxury, aes(x = mileage, y = price, col = luxury)) +\n      geom_point() + \n      geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#when-should-you-use-interactions-in-a-model",
    "href": "sp_sta235_2026/week_05.html#when-should-you-use-interactions-in-a-model",
    "title": "Data Science for Business Applications",
    "section": "When should you use interactions in a model?",
    "text": "When should you use interactions in a model?\n\nInteractions make a model more complex to analyze and explain, so it’s only worth doing so when you get a substantial bump in \\(R^2\\) by including the interaction.\nChoose interactions by thinking about what you are trying to model: if you suspect that the impact of one variable depends on the value of another, try an interaction term between them!"
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#interactions-neq-correlations-between-predictor-variables",
    "href": "sp_sta235_2026/week_05.html#interactions-neq-correlations-between-predictor-variables",
    "title": "Data Science for Business Applications",
    "section": "Interactions \\(\\neq\\) Correlations between predictor variables",
    "text": "Interactions \\(\\neq\\) Correlations between predictor variables\n\nInstead, interactions let us model a situation where the relationship of one predictor variable and \\(Y\\) is different depending on the value of another \\(X\\) variable:\n\nHow much attractiveness matters for student evaluation scores depends on gender.\nHow much tenure matters for student evaluation scores depends on gender.\nHow much your 3-point shooting volume matters depends on how accurate your 3-point shooting is."
  },
  {
    "objectID": "sp_sta235_2026/week_05.html#youll-experiment-with-this-in-the-lab",
    "href": "sp_sta235_2026/week_05.html#youll-experiment-with-this-in-the-lab",
    "title": "Data Science for Business Applications",
    "section": "You’ll experiment with this in the lab!",
    "text": "You’ll experiment with this in the lab!\n\nTwo categorical variables\nTwo numerical variables"
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "“The intercept, , is the expected number of customers on a  when the food truck is located ___ mile(s) from the downtown area.”\n\n\n\n\nStrengths: Tests fundamental understanding of intercept interpretation in a regression model with interaction terms. Correctly identifies that the intercept represents the reference category (weekday) at baseline (distance = 0).\nPedagogical value: Forces students to think about what “zero” means for each predictor variable.\nPotential issue: None - this is a well-designed question.\n\n\n\n\nCurrent: “The intercept, , is the expected number of customers on a  when the food truck is located ___ mile(s) from the downtown area.”\nSuggested revision: “The intercept, , represents the expected number of customers on a  when the food truck is ___ mile(s) from downtown.”\nChanges made: - “represents” is more precise than “is” for statistical parameters - Removed “the” before “downtown area” or shortened to “downtown” for conciseness - Changed “located” to just the distance (simpler)\nAlternative clearer version: “The intercept value of ___ is the predicted number of customers on a ___ when distance equals ___ mile(s).”\n\n\n\n\n\n\n\n“Use Model 1 to predict the expected number of customers visiting the food truck on a weekend when the truck is parked 5.7 miles away from downtown (the answer can be in decimal).”\n\n\n\n\nStrengths:\n\nTests ability to apply the model to make predictions\nRequires understanding of how to plug in values for both the categorical variable (weekend) and continuous variable (distance)\nRequires understanding of interaction terms in calculation\nThe specific distance (5.7) is good - not a simple round number\n\nPedagogical value: High - this is practical application that shows students how to use regression models\nPotential improvements: None needed\n\n\n\n\nCurrent: “Use Model 1 to predict the expected number of customers visiting the food truck on a weekend when the truck is parked 5.7 miles away from downtown (the answer can be in decimal).”\nSuggested revision: “Use Model 1 to predict the expected number of customers visiting the food truck on a weekend when the truck is parked 5.7 miles from downtown. (Provide your answer as a decimal.)”\nChanges made: - Removed “away” (redundant with “from”) - Moved the decimal instruction to a separate sentence with active voice - “Provide your answer as a decimal” is clearer than “the answer can be in decimal”\n\n\n\n\n\n\n\n“What does the coefficient of distance tell us on its own?”\n\n\n\nThe phrase “on its own” creates significant statistical and pedagogical issues.\nMajor Concerns:\n\nContradicts fundamental principle of interaction models: In a model with interactions, main effect coefficients should NOT be interpreted “on their own” - they are inherently conditional on the other variable being at its reference level.\nThe phrase “on its own” is ambiguous and misleading:\n\nDoes it mean “ignoring the interaction term”? (pedagogically wrong)\nDoes it mean “when the other variable is at zero/reference”? (technically correct but poorly worded)\nDoes it mean “the isolated effect of distance”? (doesn’t exist in interaction models)\n\nTeaches bad statistical practice: The wording could encourage students to ignore interactions when interpreting coefficients, which is a common and serious error in applied regression analysis.\n\nWhat the distance coefficient (-3.00) actually represents: - The effect of distance specifically when day = weekday (the reference category) - It is NOT the effect of distance “on its own” or “in general” or “independently”\nAnalysis of answer choices:\n\n✓ CORRECT (but question wording undermines this): “There is strong evidence that, on weekdays, customer turnout decreases by three for every one mile increase in distance from downtown.”\n\nThis is statistically accurate - it correctly specifies “on weekdays”\nHowever, this answer contradicts the question’s phrase “on its own” since it requires specifying the day type\nPedagogical confusion: Students who correctly understand interactions might second-guess this answer because the question asks what it tells us “on its own” but the answer requires conditional interpretation\n\n✗ INCORRECT: “There is strong evidence that, on weekends, customer turnout decreases by two for each extra mile in distance from downtown.”\n\nThis is about the net effect (distance + interaction), not the distance coefficient alone\nThe slope on weekends is -3 + 2 = -1, not -2 (this answer is also factually wrong)\n\n⚠️ COULD BE MISINTERPRETED AS CORRECT given the poor wording: “There is strong evidence that customer turnout decreases as the distance from downtown increases.”\n\nThis is too general and statistically incorrect for an interaction model\nBUT: students might select this because the question asks for “on its own” interpretation\nThis is the dangerous answer - it reinforces the exact misconception that the question should be teaching against\n\n✗ CLEARLY INCORRECT: “There is strong evidence that moving a food truck further away from downtown causes it to gain customers.”\n\nThis is obviously wrong given the negative coefficient\nNote: Also conflates correlation with causation (“causes”)\n\n\nCritical Issue: The wording “on its own” may lead well-informed students to choose answer 3 (the general statement) thinking that’s what “on its own” means, while the intended correct answer (1) requires conditional interpretation - the exact opposite of “on its own.”\n\n\n\nCurrent: “What does the coefficient of distance tell us on its own?”\nProblems with current wording: - “on its own” is vague and problematic - Doesn’t acknowledge the interaction structure - Could encourage ignoring interactions\nStrongly Recommended Revision:\nOption 1 (Most Clear): “In this interaction model, what does the coefficient for distance (-3.00) specifically tell us?”\nOption 2 (Explicit about reference category): “What does the coefficient for distance (-3.00) tell us about the reference category (weekdays)?”\nOption 3 (Educational framing): “The distance coefficient is -3.00. What does this value represent in an interaction model?”\nOption 4 (Most pedagogically sound): “In a model with an interaction term, the coefficient for distance (-3.00) tells us:”\n\n\n\nThe current wording with “on its own” is pedagogically harmful. It either: 1. Contradicts the correct answer (which requires conditional interpretation), OR 2. Encourages students to ignore the interaction (statistical malpractice)\nBetter approach: Rephrase to explicitly acknowledge that the interpretation is conditional on the reference category. This teaches the correct principle: in interaction models, main effects are always conditional.\n\n\n\n\n\n\n\n“What can we conclude by examining the coefficient of the interaction term?”\n\n\n\n\nStrengths:\n\nTests understanding of interaction term interpretation\nAll distractors are plausible misinterpretations\nThe correct answer demonstrates understanding that interactions indicate differential effects\n\n\nAnalysis of answer choices:\n\n✗ “There is strong evidence that moving the truck further away from downtown has a negative impact on customer turnout independently of the day of the week.”\n\nGood distractor - tests if students understand that the interaction means the effect is NOT independent\n\n✗ “There is strong evidence that customer turnout is higher on weekdays than on weekends.”\n\nGood distractor - tests if students confuse interaction with main effect\nActually, the main effect of dayweekend is -1.00, but this isn’t directly what the interaction tells us\n\n✗ “There is strong evidence that customer turnout is higher on weekends than on weekdays, regardless of the distance from downtown.”\n\nGood distractor - the word “regardless” is key here; interactions mean effects DEPEND on other variables\n\n✓ CORRECT: “There is strong evidence that the effect of distance on customer numbers is significantly different on weekends compared to weekdays.”\n\nPerfect interpretation of an interaction term\n\n\n\n\n\nCurrent: “What can we conclude by examining the coefficient of the interaction term?”\nSuggested revision: “What can we conclude from the interaction term coefficient (distance:dayweekend = 2.00)?”\nChanges made: - “from” is slightly more natural than “by examining” - Added the specific coefficient for reference - Included the term name from the output\n\n\n\n\n\n\n\n“Which plot above corresponds to the predictions from Model 1?”\n\n\n\n\nStrengths:\n\nTests visual interpretation of regression with interactions\nRequires students to understand:\n\nBoth lines should be negative (distance coefficient is negative)\nWeekday line should be steeper (slope = -3)\nWeekend line should be flatter (slope = -1)\nLines should have different intercepts\nLines should cross over (interaction effect)\n\nThis is high-level synthesis\n\n\nAnalysis based on what Plot A should show: - Weekday: steeper negative slope (red line, slope = -3) - Weekend: flatter negative slope (blue line, slope = -1) - At distance = 0: weekday ≈ 21, weekend ≈ 20 - Lines cross, with weekend better at higher distances\n\n\n\nCurrent: “Which plot above corresponds to the predictions from Model 1?”\nSuggested revisions:\nOption 1: “Which plot best represents the predictions from Model 1?”\nOption 2: “Which plot correctly shows the predicted relationship from Model 1?”\nChanges made: - “best represents” or “correctly shows” is more precise than “corresponds to” - Removed “above” (the plots are clearly visible, so this is redundant)\nAlternative with more guidance: “Based on Model 1, which plot correctly shows how the predicted number of customers changes with distance for weekdays versus weekends?”\n\n\n\n\n\n\n\n\nProgressive difficulty: Questions build from interpretation to application to synthesis\nTests multiple competencies: Parameter interpretation, calculation, conceptual understanding, visual interpretation\nFocus on interactions: Excellent pedagogical focus on a challenging topic\nRealistic context: Food truck scenario is relatable and practical\n\n\n\n\n\nQuestion 3 - CRITICAL ISSUE: The phrase “on its own” is pedagogically harmful and statistically misleading:\n\nCreates direct contradiction between question wording and correct answer\nThe phrase “on its own” suggests ignoring the interaction, which is bad statistical practice\nMay cause well-informed students to choose the wrong answer\nSTRONG RECOMMENDATION: Revise to explicitly acknowledge the conditional interpretation (e.g., “In this interaction model, what does the coefficient for distance specifically tell us?”)\n\nConsistency in wording: Some questions say “strong evidence” while others don’t. Consider standardizing language around statistical significance.\nExplicit connection to output: Questions could reference specific coefficient values to help students connect their interpretations to the actual output.\n\n\n\n\n\nQuestion 1: Awkward phrasing with multiple blanks - consider restructuring\nQuestion 2: “answer can be in decimal” → “provide answer as a decimal”\nAll questions: Consider adding actual coefficient values in parentheses for reference\n\n\n\n\n\nQuestion 1: Minor rephrasing for clarity\nQuestion 2: Clarify decimal instruction\nQuestion 3: CRITICAL - REQUIRES REVISION - Remove “on its own” phrasing, which contradicts proper interpretation of interactions and may confuse students who understand the material correctly\nQuestion 4: Minor wording improvement\nQuestion 5: Minor wording improvement\n\nOverall, this quiz has strong pedagogical goals and tests important concepts about interaction terms. However, Question 3 needs immediate revision because the phrase “on its own” directly contradicts the fundamental principle that main effects in interaction models should be interpreted conditionally, not in isolation. This could teach students incorrect statistical practice or penalize those who correctly understand interactions."
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-1-fill-in-the-blank-4-points",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-1-fill-in-the-blank-4-points",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "“The intercept, , is the expected number of customers on a  when the food truck is located ___ mile(s) from the downtown area.”\n\n\n\n\nStrengths: Tests fundamental understanding of intercept interpretation in a regression model with interaction terms. Correctly identifies that the intercept represents the reference category (weekday) at baseline (distance = 0).\nPedagogical value: Forces students to think about what “zero” means for each predictor variable.\nPotential issue: None - this is a well-designed question.\n\n\n\n\nCurrent: “The intercept, , is the expected number of customers on a  when the food truck is located ___ mile(s) from the downtown area.”\nSuggested revision: “The intercept, , represents the expected number of customers on a  when the food truck is ___ mile(s) from downtown.”\nChanges made: - “represents” is more precise than “is” for statistical parameters - Removed “the” before “downtown area” or shortened to “downtown” for conciseness - Changed “located” to just the distance (simpler)\nAlternative clearer version: “The intercept value of ___ is the predicted number of customers on a ___ when distance equals ___ mile(s).”"
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-2-formula-4-points",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-2-formula-4-points",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "“Use Model 1 to predict the expected number of customers visiting the food truck on a weekend when the truck is parked 5.7 miles away from downtown (the answer can be in decimal).”\n\n\n\n\nStrengths:\n\nTests ability to apply the model to make predictions\nRequires understanding of how to plug in values for both the categorical variable (weekend) and continuous variable (distance)\nRequires understanding of interaction terms in calculation\nThe specific distance (5.7) is good - not a simple round number\n\nPedagogical value: High - this is practical application that shows students how to use regression models\nPotential improvements: None needed\n\n\n\n\nCurrent: “Use Model 1 to predict the expected number of customers visiting the food truck on a weekend when the truck is parked 5.7 miles away from downtown (the answer can be in decimal).”\nSuggested revision: “Use Model 1 to predict the expected number of customers visiting the food truck on a weekend when the truck is parked 5.7 miles from downtown. (Provide your answer as a decimal.)”\nChanges made: - Removed “away” (redundant with “from”) - Moved the decimal instruction to a separate sentence with active voice - “Provide your answer as a decimal” is clearer than “the answer can be in decimal”"
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-3-multiple-choice-3-points",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-3-multiple-choice-3-points",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "“What does the coefficient of distance tell us on its own?”\n\n\n\nThe phrase “on its own” creates significant statistical and pedagogical issues.\nMajor Concerns:\n\nContradicts fundamental principle of interaction models: In a model with interactions, main effect coefficients should NOT be interpreted “on their own” - they are inherently conditional on the other variable being at its reference level.\nThe phrase “on its own” is ambiguous and misleading:\n\nDoes it mean “ignoring the interaction term”? (pedagogically wrong)\nDoes it mean “when the other variable is at zero/reference”? (technically correct but poorly worded)\nDoes it mean “the isolated effect of distance”? (doesn’t exist in interaction models)\n\nTeaches bad statistical practice: The wording could encourage students to ignore interactions when interpreting coefficients, which is a common and serious error in applied regression analysis.\n\nWhat the distance coefficient (-3.00) actually represents: - The effect of distance specifically when day = weekday (the reference category) - It is NOT the effect of distance “on its own” or “in general” or “independently”\nAnalysis of answer choices:\n\n✓ CORRECT (but question wording undermines this): “There is strong evidence that, on weekdays, customer turnout decreases by three for every one mile increase in distance from downtown.”\n\nThis is statistically accurate - it correctly specifies “on weekdays”\nHowever, this answer contradicts the question’s phrase “on its own” since it requires specifying the day type\nPedagogical confusion: Students who correctly understand interactions might second-guess this answer because the question asks what it tells us “on its own” but the answer requires conditional interpretation\n\n✗ INCORRECT: “There is strong evidence that, on weekends, customer turnout decreases by two for each extra mile in distance from downtown.”\n\nThis is about the net effect (distance + interaction), not the distance coefficient alone\nThe slope on weekends is -3 + 2 = -1, not -2 (this answer is also factually wrong)\n\n⚠️ COULD BE MISINTERPRETED AS CORRECT given the poor wording: “There is strong evidence that customer turnout decreases as the distance from downtown increases.”\n\nThis is too general and statistically incorrect for an interaction model\nBUT: students might select this because the question asks for “on its own” interpretation\nThis is the dangerous answer - it reinforces the exact misconception that the question should be teaching against\n\n✗ CLEARLY INCORRECT: “There is strong evidence that moving a food truck further away from downtown causes it to gain customers.”\n\nThis is obviously wrong given the negative coefficient\nNote: Also conflates correlation with causation (“causes”)\n\n\nCritical Issue: The wording “on its own” may lead well-informed students to choose answer 3 (the general statement) thinking that’s what “on its own” means, while the intended correct answer (1) requires conditional interpretation - the exact opposite of “on its own.”\n\n\n\nCurrent: “What does the coefficient of distance tell us on its own?”\nProblems with current wording: - “on its own” is vague and problematic - Doesn’t acknowledge the interaction structure - Could encourage ignoring interactions\nStrongly Recommended Revision:\nOption 1 (Most Clear): “In this interaction model, what does the coefficient for distance (-3.00) specifically tell us?”\nOption 2 (Explicit about reference category): “What does the coefficient for distance (-3.00) tell us about the reference category (weekdays)?”\nOption 3 (Educational framing): “The distance coefficient is -3.00. What does this value represent in an interaction model?”\nOption 4 (Most pedagogically sound): “In a model with an interaction term, the coefficient for distance (-3.00) tells us:”\n\n\n\nThe current wording with “on its own” is pedagogically harmful. It either: 1. Contradicts the correct answer (which requires conditional interpretation), OR 2. Encourages students to ignore the interaction (statistical malpractice)\nBetter approach: Rephrase to explicitly acknowledge that the interpretation is conditional on the reference category. This teaches the correct principle: in interaction models, main effects are always conditional."
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-4-multiple-choice-3-points",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-4-multiple-choice-3-points",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "“What can we conclude by examining the coefficient of the interaction term?”\n\n\n\n\nStrengths:\n\nTests understanding of interaction term interpretation\nAll distractors are plausible misinterpretations\nThe correct answer demonstrates understanding that interactions indicate differential effects\n\n\nAnalysis of answer choices:\n\n✗ “There is strong evidence that moving the truck further away from downtown has a negative impact on customer turnout independently of the day of the week.”\n\nGood distractor - tests if students understand that the interaction means the effect is NOT independent\n\n✗ “There is strong evidence that customer turnout is higher on weekdays than on weekends.”\n\nGood distractor - tests if students confuse interaction with main effect\nActually, the main effect of dayweekend is -1.00, but this isn’t directly what the interaction tells us\n\n✗ “There is strong evidence that customer turnout is higher on weekends than on weekdays, regardless of the distance from downtown.”\n\nGood distractor - the word “regardless” is key here; interactions mean effects DEPEND on other variables\n\n✓ CORRECT: “There is strong evidence that the effect of distance on customer numbers is significantly different on weekends compared to weekdays.”\n\nPerfect interpretation of an interaction term\n\n\n\n\n\nCurrent: “What can we conclude by examining the coefficient of the interaction term?”\nSuggested revision: “What can we conclude from the interaction term coefficient (distance:dayweekend = 2.00)?”\nChanges made: - “from” is slightly more natural than “by examining” - Added the specific coefficient for reference - Included the term name from the output"
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-5-multiple-choice-4-points",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#question-5-multiple-choice-4-points",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "“Which plot above corresponds to the predictions from Model 1?”\n\n\n\n\nStrengths:\n\nTests visual interpretation of regression with interactions\nRequires students to understand:\n\nBoth lines should be negative (distance coefficient is negative)\nWeekday line should be steeper (slope = -3)\nWeekend line should be flatter (slope = -1)\nLines should have different intercepts\nLines should cross over (interaction effect)\n\nThis is high-level synthesis\n\n\nAnalysis based on what Plot A should show: - Weekday: steeper negative slope (red line, slope = -3) - Weekend: flatter negative slope (blue line, slope = -1) - At distance = 0: weekday ≈ 21, weekend ≈ 20 - Lines cross, with weekend better at higher distances\n\n\n\nCurrent: “Which plot above corresponds to the predictions from Model 1?”\nSuggested revisions:\nOption 1: “Which plot best represents the predictions from Model 1?”\nOption 2: “Which plot correctly shows the predicted relationship from Model 1?”\nChanges made: - “best represents” or “correctly shows” is more precise than “corresponds to” - Removed “above” (the plots are clearly visible, so this is redundant)\nAlternative with more guidance: “Based on Model 1, which plot correctly shows how the predicted number of customers changes with distance for weekdays versus weekends?”"
  },
  {
    "objectID": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#overall-assessment",
    "href": "sp_sta235_2026/quizzes_suggestions_2026/quiz_review_quiz_05.html#overall-assessment",
    "title": "Quiz 5: Statistical and Grammatical Review",
    "section": "",
    "text": "Progressive difficulty: Questions build from interpretation to application to synthesis\nTests multiple competencies: Parameter interpretation, calculation, conceptual understanding, visual interpretation\nFocus on interactions: Excellent pedagogical focus on a challenging topic\nRealistic context: Food truck scenario is relatable and practical\n\n\n\n\n\nQuestion 3 - CRITICAL ISSUE: The phrase “on its own” is pedagogically harmful and statistically misleading:\n\nCreates direct contradiction between question wording and correct answer\nThe phrase “on its own” suggests ignoring the interaction, which is bad statistical practice\nMay cause well-informed students to choose the wrong answer\nSTRONG RECOMMENDATION: Revise to explicitly acknowledge the conditional interpretation (e.g., “In this interaction model, what does the coefficient for distance specifically tell us?”)\n\nConsistency in wording: Some questions say “strong evidence” while others don’t. Consider standardizing language around statistical significance.\nExplicit connection to output: Questions could reference specific coefficient values to help students connect their interpretations to the actual output.\n\n\n\n\n\nQuestion 1: Awkward phrasing with multiple blanks - consider restructuring\nQuestion 2: “answer can be in decimal” → “provide answer as a decimal”\nAll questions: Consider adding actual coefficient values in parentheses for reference\n\n\n\n\n\nQuestion 1: Minor rephrasing for clarity\nQuestion 2: Clarify decimal instruction\nQuestion 3: CRITICAL - REQUIRES REVISION - Remove “on its own” phrasing, which contradicts proper interpretation of interactions and may confuse students who understand the material correctly\nQuestion 4: Minor wording improvement\nQuestion 5: Minor wording improvement\n\nOverall, this quiz has strong pedagogical goals and tests important concepts about interaction terms. However, Question 3 needs immediate revision because the phrase “on its own” directly contradicts the fundamental principle that main effects in interaction models should be interpreted conditionally, not in isolation. This could teach students incorrect statistical practice or penalize those who correctly understand interactions."
  },
  {
    "objectID": "quiz_suggestions.html",
    "href": "quiz_suggestions.html",
    "title": "Quiz suggestions",
    "section": "",
    "text": "Spring 2026\n\nSTA 235 - Data Science for Business Applications\n\nclass 05 - Interactions"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models",
    "href": "sp_sta235_2026/week_06.html#polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nThe data set utilities contains information on the utility bills for a house in Minnesota. We’ll focus on two variables:\n\ndailyspend is the average amount of money spent on utilities (e.g. heating) for each day during the month\ntemp is the average temperature outside for that month"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-1",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-1",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe’ll use polynomial regression to fix problems\nIf a polynomial curve (e.g., quadratic, cubic, etc) would be a better fit for the data than a line, we can fit a curve to the data.\nThe way we do this is by adding \\(X^2\\) to the model as a second predictor variable.\nThis can “fix” the linearity problem because now \\(Y\\) is a linear function of \\(X\\) and \\(X^2\\), resulting in: \\[\nY = \\beta_0 + \\beta_1\\cdot X + \\beta\\cdot X^2 + e\n\\]"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-2",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-2",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWe add the term I(temp^2) in the regression equation:\n\n\nlm2 &lt;- lm(dailyspend ~ temp + I(temp^2), data=utilities) \nsummary(lm2)\n\n\nCall:\nlm(formula = dailyspend ~ temp + I(temp^2), data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.87250 -0.28048 -0.03929  0.26391  2.19117 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  9.4722885  0.3907892  24.239 &lt; 0.0000000000000002 ***\ntemp        -0.2115553  0.0191046 -11.074 &lt; 0.0000000000000002 ***\nI(temp^2)    0.0012476  0.0002037   6.124         0.0000000133 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7547 on 114 degrees of freedom\nMultiple R-squared:  0.8803,    Adjusted R-squared:  0.8782 \nF-statistic: 419.3 on 2 and 114 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nWe have that the new term is evaluated as an extra variable."
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-3",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-3",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nWriting out the equation: \\[\n\\widehat{\\texttt{dailyspend}} = 9.4723 −0.2116\\cdot \\texttt{temp} + 0.0012\\cdot \\texttt{temp}^2\n\\] The effect of the extra variable is statistically significant:\n\nconfint(lm2)\n\n                    2.5 %       97.5 %\n(Intercept)  8.6981381712 10.246438869\ntemp        -0.2494014032 -0.173709160\nI(temp^2)    0.0008440041  0.001651114\n\n\n\nThe residual standard error of the polynomial model is \\(\\texttt{0.75}\\).\nThe residual standard error of the linear model is \\(\\texttt{0.87}\\)."
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-4",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-4",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nAdding an \\(X^2\\) term fits a parabola to the data (orange line)\n\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm1)), col = \"blue\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-5",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-5",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\nIt solves the linearity problem\n\nautoplot(lm2)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-6",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-6",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWhat about a higher-order polynomial?\nWe could fit a cubic curve by adding an \\(X^3\\) term\nMaking the polynomial higher order will decrease the RSE\nWhy not go nuts and fit a 7th degree polynomial?\n\n\n\n\nDegree\nname\nRSE\n\n\n\n\n1\nlinear\n0.866\n\n\n2\nquadratic\n0.754\n\n\n3\ncubic\n0.755\n\n\n4\nquartic\n0.755\n\n\n5\nquintic\n0.758\n\n\n6\n\n0.761\n\n\n7\n\n0.761"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-7",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-7",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ poly(temp,7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-8",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-8",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ poly(temp,7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-9",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-9",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nWhat about a higher-order polynomial?\nWe could fit a cubic curve by adding an \\(X^3\\) term\nMaking the polynomial higher order will decrease the RSE\nWhy not go nuts and fit a 7th degree polynomial?\n\n\n\n\nDegree\nname\nRSE\n\n\n\n\n1\nlinear\n0.866\n\n\n2\nquadratic\n0.754\n\n\n3\ncubic\n0.755\n\n\n4\nquartic\n0.755\n\n\n5\nquintic\n0.758\n\n\n6\n\n0.761\n\n\n7\n\n0.761"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#polynomial-models-10",
    "href": "sp_sta235_2026/week_06.html#polynomial-models-10",
    "title": "Data Science for Business Applications",
    "section": "Polynomial models",
    "text": "Polynomial models\n\nlm7 &lt;- lm(dailyspend ~ poly(temp,7), data=utilities) \nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() + \n  geom_line(aes(x = temp, y = predict(lm7)), col = \"red\") + \n  geom_line(aes(x = temp, y = predict(lm2)), col = \"orange\")\n\n\n\nToo high a degree creates dangers with extrapolation"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#building-polynomial-models",
    "href": "sp_sta235_2026/week_06.html#building-polynomial-models",
    "title": "Data Science for Business Applications",
    "section": "Building polynomial models",
    "text": "Building polynomial models\nStart simple: only add higher-degree terms to the extent it gives you a substantial decrease in the RSE, or satisfies an assumption hold that wasn’t satisfied before\n\nYou must include lower-order terms: e.g., if you add \\(X^3\\), you must also include \\(X\\) and \\(X^2\\)\nBe careful about overfitting when adding higher-order terms!\nBe particularly careful about extrapolating beyond the range of the data!\nMind-bender: We can think about an \\(X^2\\) term as an interaction of \\(X\\) with itself: in a parabola, the slope depends on the value of \\(X\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#the-log-transformation",
    "href": "sp_sta235_2026/week_06.html#the-log-transformation",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nWe saw that we can use transformations to fix problems\nSometimes, a violation of regression assumptions can be fixed by transforming one or the other of the variables (or both).\nWhen we transform a variable, we have to also transform our interpretation of the equation."
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#the-log-transformation-1",
    "href": "sp_sta235_2026/week_06.html#the-log-transformation-1",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\nThe log transformation is frequently useful in regression, because many nonlinear relationships are naturally exponential.\n\n\\(\\log_b x=y\\) when \\(b^y=x\\)\nFor example, \\(\\log_{10} 1000 = 3\\), \\(\\log_{10}100 = 2\\), and \\(\\log_{10}10 = 1\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#the-log-transformation-2",
    "href": "sp_sta235_2026/week_06.html#the-log-transformation-2",
    "title": "Data Science for Business Applications",
    "section": "The log transformation",
    "text": "The log transformation\n\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!)\nSkewed data is also a good candidate for log"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#moores-law",
    "href": "sp_sta235_2026/week_06.html#moores-law",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nMoore’s Law was a prediction made by Gordon Moore in 1965 (!) that the number of transistors on computer chips would double every 2 years\nThis implies exponential growth, so a linear model won’t fit well (and neither will any polynomial)\n\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#moores-law-1",
    "href": "sp_sta235_2026/week_06.html#moores-law-1",
    "title": "Data Science for Business Applications",
    "section": "Moore’s Law",
    "text": "Moore’s Law\n\nlm_moore = lm(Transistor.count ~ Date.of.introduction, data = moores)\nautoplot(lm_moore)\n\n\n\nA linear model is a spectacular fail"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#modeling-exponential-growth",
    "href": "sp_sta235_2026/week_06.html#modeling-exponential-growth",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\nIf \\(Y = ae^{bX}\\), then\n\\[\\log(Y) = \\log(a)+ bX\\]\n\nIn other words, \\(\\log(Y)\\) is a linear function of \\(X\\) when \\(Y\\) is an exponential function of \\(X\\)\nSo if we think \\(Y\\) is an exponential function of \\(X\\), predict \\(\\log(Y)\\) as a linear function of \\(X\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#modeling-exponential-growth-1",
    "href": "sp_sta235_2026/week_06.html#modeling-exponential-growth-1",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nTransistors does NOT have a linear relationship with year\n\\(\\log(\\texttt{Transistors})\\) does have a linear relationship with year\n\n\nggplot(moores, aes(x = Date.of.introduction, y = log(Transistor.count))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#log-linear-model",
    "href": "sp_sta235_2026/week_06.html#log-linear-model",
    "title": "Data Science for Business Applications",
    "section": "Log-linear Model",
    "text": "Log-linear Model\nLet’s run the regression model\n\noptions(scipen = 999)\nlm_moore = lm(log(Transistor.count) ~ Date.of.introduction, data = moores)\nsummary(lm_moore)\n\n\nCall:\nlm(formula = log(Transistor.count) ~ Date.of.introduction, data = moores)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1299 -0.3338  0.1767  0.5230  2.0626 \n\nCoefficients:\n                        Estimate  Std. Error t value            Pr(&gt;|t|)    \n(Intercept)          -681.212056   15.958165  -42.69 &lt;0.0000000000000002 ***\nDate.of.introduction    0.349154    0.007981   43.75 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.054 on 99 degrees of freedom\nMultiple R-squared:  0.9508,    Adjusted R-squared:  0.9503 \nF-statistic:  1914 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#modeling-exponential-growth-2",
    "href": "sp_sta235_2026/week_06.html#modeling-exponential-growth-2",
    "title": "Data Science for Business Applications",
    "section": "Modeling exponential growth",
    "text": "Modeling exponential growth\n\nautoplot(lm_moore)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#interpretation-of-the-model",
    "href": "sp_sta235_2026/week_06.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\nOur model is \\[\\widehat{\\log(\\texttt{Transistors})} = −681.21 + 0.35 \\cdot \\texttt{Year}\\]\nTwo interpretations of the slope coefficient:\n\nEvery year, the predicted log of transistors goes up by 0.35\nMore useful: Every year, the predicted number of transistors goes up by \\((\\exp(0.35)-1)\\times 100\\) = 41.9%\nA constant percentage increase every year is exponential growth!"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#interpretation-of-the-model-1",
    "href": "sp_sta235_2026/week_06.html#interpretation-of-the-model-1",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nMaking predictions using the log-linear model\nWhen making predictions, we have to remember that our equation gives us predictions for \\(\\log(\\texttt{Transistors})\\), not Transistors!\n\nExample: To make a prediction for the number of transistors in 2022: \\[\n\\log(\\texttt{Transistors}) = −681.21 + 0.35(2022) = 26.49\n\\] But our prediction is not 26.49:\n\\(e^{\\log(\\texttt{Transistors})} = e^{26.49} = 319,492,616,196\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#interpretation-of-the-model-2",
    "href": "sp_sta235_2026/week_06.html#interpretation-of-the-model-2",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nggplot(moores, aes(x = Date.of.introduction, y = Transistor.count)) +\n  geom_point() +\n  geom_line(aes(x = Date.of.introduction, y = exp(predict(lm_moore))), col = \"orange\")"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#interpretation-of-the-model-3",
    "href": "sp_sta235_2026/week_06.html#interpretation-of-the-model-3",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\n\n\n\n\n\n\n\nModel\nEquation\nInterpretation\n\n\n\n\nLinear\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies \\(\\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-linear\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X\\)\n1 unit increase in \\(X\\) implies \\((\\exp(\\widehat{\\beta}_1)-1)\\times 100\\%\\) in \\(\\widehat{Y}\\)\n\n\nLinear-log\n\\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(0.01 \\cdot \\widehat{\\beta}_1\\) unit increase in \\(\\widehat{Y}\\)\n\n\nLog-log\n\\(\\log(\\widehat{Y}) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\log(X)\\)\n1% increase in \\(X\\) implies ≈ \\(\\widehat{\\beta}_1 \\%\\) increase in \\(\\widehat{Y}\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#conclusion",
    "href": "sp_sta235_2026/week_06.html#conclusion",
    "title": "Data Science for Business Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhen is the log transformation useful?\nYou can transform \\(X \\rightarrow \\log(X)\\), \\(Y \\rightarrow \\log(Y)\\), or both\nAnytime you need to ”squash” one of the variables (logs make huge numbers not so big!), try transforming it with a log\nIn this case, Transistors is skewed right so it is a good candidate for log\nYou may need to do a little bit of trial and error to see what works best\nOther transformations are possible!"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#simple-linear-regression-model",
    "href": "sp_sta235_2026/week_06.html#simple-linear-regression-model",
    "title": "Data Science for Business Applications",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\nlm1 &lt;- lm(dailyspend ~ temp, data=utilities) \nsummary(lm1)\n\n\nCall:\nlm(formula = dailyspend ~ temp, data = utilities)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84674 -0.50361 -0.02397  0.51540  2.44843 \n\nCoefficients:\n             Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  7.347617   0.206446   35.59 &lt;0.0000000000000002 ***\ntemp        -0.096432   0.003911  -24.66 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8663 on 115 degrees of freedom\nMultiple R-squared:  0.841, Adjusted R-squared:  0.8396 \nF-statistic: 608.1 on 1 and 115 DF,  p-value: &lt; 0.00000000000000022\n\n\n\nLet’s interpret this relation\nFor one unit increase in temperature (Fahrenheit), there will be a 7-cent decrease in spending"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#simple-linear-regression-model-1",
    "href": "sp_sta235_2026/week_06.html#simple-linear-regression-model-1",
    "title": "Data Science for Business Applications",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nWhat problems do you see here?\n\nlibrary(tidyverse)\nggplot(utilities, aes(x = temp, y = dailyspend)) +\n  geom_point() +\n  geom_line(aes(x = temp, y = predict(lm1)), col = \"blue\")"
  },
  {
    "objectID": "sp_sta235_2026/week_06.html#diagnostics-plot",
    "href": "sp_sta235_2026/week_06.html#diagnostics-plot",
    "title": "Data Science for Business Applications",
    "section": "Diagnostics Plot",
    "text": "Diagnostics Plot\n\nlibrary(ggfortify)\nautoplot(lm1)\n\n\n\nLinearity and homoscedasticity are violated"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html",
    "href": "sp_sta235_2026/week_07.html",
    "title": "Data Science for Business Applications",
    "section": "",
    "text": "Apple quarterly revenue (Billions of dollars)\nGoal: What is the pattern here, and how can we forecast future earnings?\n\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#basic-time-series-concepts",
    "href": "sp_sta235_2026/week_07.html#basic-time-series-concepts",
    "title": "Data Science for Business Applications",
    "section": "Basic time series concepts",
    "text": "Basic time series concepts\n\nApple quarterly revenue (Billions of dollars)\nGoal: What is the pattern here, and how can we forecast future earnings?\n\n\nlibrary(tidyverse)\nlibrary(ggfortify)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#what-are-time-series",
    "href": "sp_sta235_2026/week_07.html#what-are-time-series",
    "title": "Data Science for Business Applications",
    "section": "What are time series?",
    "text": "What are time series?\n\nData where the cases represent time: data collected every day, month, year, etc.\nTime series are important for both explaining how variables change over time and forecasting the future\nExamples of time series data:\nGoogle’s closing daily stock price every day in 2020\nInventory levels of each item at a retail store at the end of every week in 2020\nNumber of new COVID cases in the US each day since the start of the pandemic\nApple’s quarterly revenue since 2009"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#anatomy-of-a-time-series",
    "href": "sp_sta235_2026/week_07.html#anatomy-of-a-time-series",
    "title": "Data Science for Business Applications",
    "section": "Anatomy of a time series",
    "text": "Anatomy of a time series\nSome notation:\n\n\\(t = 1,2,3,...\\), time index\n\\(Y_t\\), is the value: of the variable of interest at time \\(t\\)\n\\(Y_t\\) may be composed of one or more components:\nTrend\nSeasonal\nCyclical\nRandom"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#trend-component",
    "href": "sp_sta235_2026/week_07.html#trend-component",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nA trend is persistent upwards or downwards movement in the data (not necessarily linear)."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#trend-component-1",
    "href": "sp_sta235_2026/week_07.html#trend-component-1",
    "title": "Data Science for Business Applications",
    "section": "Trend component",
    "text": "Trend component\n\nExample: Moore’s Law (accelerating increase of transistor count)\nExample: US population over time\nA time series with no trend is called stationary."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#seasonal-component",
    "href": "sp_sta235_2026/week_07.html#seasonal-component",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nSeasonal fluctuation occurs when predictable up or down movements occur over a regular interval."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#seasonal-component-1",
    "href": "sp_sta235_2026/week_07.html#seasonal-component-1",
    "title": "Data Science for Business Applications",
    "section": "Seasonal component",
    "text": "Seasonal component\n\nThe ups and downs must occur over a regular interval (e.g., every month, or every year)\nExample: Highway traffic volume is highest during rush hour every day\nExample: Supermarket sales may be highest every month right after common paydays like the 15th and 30th"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#cyclic-component",
    "href": "sp_sta235_2026/week_07.html#cyclic-component",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nCyclic fluctuations occur at unpredictable intervals, e.g. due to changing business or economic conditions."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#cyclic-component-1",
    "href": "sp_sta235_2026/week_07.html#cyclic-component-1",
    "title": "Data Science for Business Applications",
    "section": "Cyclic component",
    "text": "Cyclic component\n\nIn contrast to seasonal fluctuations, cyclic fluctuations do not occur at regular, predictable intervals\nIt may be possible to predict cyclic components based on some other (non-time) variable\nExample: Restaurant sales dropped dramatically in 2020 due to COVID, as people ate out less\nExample: Sales of bell bottoms rose in the 60s and 70s, declined by the 80s, and then had a resurgence in the 90s"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#remaindererror-component",
    "href": "sp_sta235_2026/week_07.html#remaindererror-component",
    "title": "Data Science for Business Applications",
    "section": "Remainder/Error component",
    "text": "Remainder/Error component\n\nAny real time series will always have random noise as well, which can’t be predicted or forecast."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#time-series-components",
    "href": "sp_sta235_2026/week_07.html#time-series-components",
    "title": "Data Science for Business Applications",
    "section": "Time Series Components",
    "text": "Time Series Components\n\nWhich component(s) you see in each of these time series?"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#putting-these-together",
    "href": "sp_sta235_2026/week_07.html#putting-these-together",
    "title": "Data Science for Business Applications",
    "section": "Putting these together",
    "text": "Putting these together\nReal time series will usually include a combination of these four components. We will model the time series \\(Y_t\\) either additively:\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\] Or multiplicatively: \\[\nY_t = \\text{Trend}\\cdot\\text{Seasonal}\\cdot\\text{Random}= T_t \\cdot S_t \\cdot E_t\n\\] * (\\(E_t\\) consists of both the cyclic and error components, as both are unpredictable.) This model can be rewritten as a log model: \\[\n\\log{Y_t} = \\log(T_t) + \\log(S_t) + \\log(E_t)\n\\]"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#additive-models",
    "href": "sp_sta235_2026/week_07.html#additive-models",
    "title": "Data Science for Business Applications",
    "section": "Additive models",
    "text": "Additive models\n\\[\nY_t = \\text{Trend} + \\text{Seasonal} + \\text{Random} = T_t +S_t +E_t\n\\]\n\nMost appropriate when seasonal fluctuations are consistent (do not increase or decrease over time)\nThe trend component \\(T_t\\) is a function of t (e.g., linear or quadratic)\nThe seasonal component \\(S_t\\) is a set of dummy variable representing “seasons”\nSo we can estimate additive models using regular regression"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#additive-decomposition",
    "href": "sp_sta235_2026/week_07.html#additive-decomposition",
    "title": "Data Science for Business Applications",
    "section": "Additive decomposition",
    "text": "Additive decomposition\n\nRun a regression predicting \\(Y\\) as a function of:\n\n\n\\(t\\), \\(t^2\\), \\(\\log(t)\\) etc (the trend component \\(T_t\\))\nDummy variables for the seasons (the seasonal component \\(S_t\\))\n\n\nTo make a prediction for \\(Y\\), plug into the model!\nThe residuals of this model correspond to the error component \\(E_t\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-quarterly-revenue",
    "href": "sp_sta235_2026/week_07.html#apple-quarterly-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple quarterly revenue",
    "text": "Apple quarterly revenue\n\nWhat components do you see here?\n\n\nlibrary(tidyverse)\nggplot(apple, aes(x=Time, y=Revenue)) + \n  geom_line()"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#fitting-additive-model",
    "href": "sp_sta235_2026/week_07.html#fitting-additive-model",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nlm_additive = lm(Revenue ~ Period + Quarter, data=apple) \nsummary(lm_additive)\n\n\nCall:\nlm(formula = Revenue ~ Period + Quarter, data = apple)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.3671  -4.6673   0.4707   4.8452  18.3182 \n\nCoefficients:\n             Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  34.88613    2.51292  13.883 &lt; 0.0000000000000002 ***\nPeriod        1.38697    0.04943  28.062 &lt; 0.0000000000000002 ***\nQuarterQ2   -21.20822    2.70397  -7.843  0.00000000008182889 ***\nQuarterQ3   -28.19154    2.66346 -10.585  0.00000000000000194 ***\nQuarterQ4   -24.43405    2.66392  -9.172  0.00000000000043655 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.647 on 61 degrees of freedom\nMultiple R-squared:  0.9375,    Adjusted R-squared:  0.9334 \nF-statistic: 228.7 on 4 and 61 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#interpretation-of-the-model",
    "href": "sp_sta235_2026/week_07.html#interpretation-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Interpretation of the model",
    "text": "Interpretation of the model\n\nThe trend that we can infer from the variable Period indicates a positive growth in revenue of US$ 1.4 billion for each increase in the periods.\nThe seasonal from the Quarter component indicates:\n\n\nQ2’s are expected to be $21.2 worse than Q1’s\nQ3’s are expected to be $28.2 worse than Q1’s\nQ4’s are expected to be $24.4 worse than Q1’s\nQ3’s are significantly worse than Q1’s\n\n\nThese effects are statistically significant (confint(lm_additive))\nThe RSE from this model is US$ 7.647 billions of dollars.\nHow can we interpret these results?"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#fitting-additive-model-1",
    "href": "sp_sta235_2026/week_07.html#fitting-additive-model-1",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line() +\n  geom_line(aes(x = Time, y = predict(lm_additive)), col = \"orange\")"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#fitting-additive-model-2",
    "href": "sp_sta235_2026/week_07.html#fitting-additive-model-2",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nWhat does the final model predict from the Quarter component indicates: for Apple in 2026 Q1?\n\n\npredict(lm_additive, list(Period = 67, Quarter = \"Q1\"), interval = \"prediction\")\n\n       fit     lwr      upr\n1 127.8133 111.698 143.9287\n\n\n\nThe actual revenue was US$ 143.8 billions\nWhat does the final model predict from the Quarter component indicates: for Apple in 2030 Q1? (Should we trust that prediction?)"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#fitting-additive-model-3",
    "href": "sp_sta235_2026/week_07.html#fitting-additive-model-3",
    "title": "Data Science for Business Applications",
    "section": "Fitting additive model",
    "text": "Fitting additive model\n\nThe residuals from this model show the “detrended and deasonalized” data (but there’s still some trend left!):\nWe hadn’t yet dealt with the time dependence\n\n\nggplot(apple, aes(x = Time, y = Revenue)) + \n  geom_line(aes(x = Time, y = residuals(lm_additive)))"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#autorgression-model",
    "href": "sp_sta235_2026/week_07.html#autorgression-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression model",
    "text": "Autorgression model\n\nHow we deal with the time dependence ? Key idea: Instead of predicting \\(Y_t\\) as a function of \\(t\\) (or other variables), predict \\(Y_t\\) as a function of \\(Y_{t-1}\\): \\[\nY_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\n\\]\n\\(Y_{t-1}\\) is called the “1st lag” of \\(Y\\)\nThis is called autoregressive (AR) because it predicts the values of a time series based on previous values\nThe model above is an AR(1) model\nWe can have AR(\\(p\\)) models, with lag \\(p\\)"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#autocorrelation",
    "href": "sp_sta235_2026/week_07.html#autocorrelation",
    "title": "Data Science for Business Applications",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation, is the correlation of \\(Y_t\\) with each of its lags \\(Y_t, Y_{t−1},\\dots\\) \\[\nCor(Y_t, Y_{t−1}), Cor(Y_t, Y_{t−2}),\\dots\n\\]\nWe also have the autocorrelation of the residuals, \\(r_t\\)’s, which indicates that there’s a strong indication that the independence assumption is violated \\[\nCor(r_t, r_{t−1}), Cor(r_t, r_{t−2}),\\dots\n\\]"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#ozone-example",
    "href": "sp_sta235_2026/week_07.html#ozone-example",
    "title": "Data Science for Business Applications",
    "section": "Ozone example",
    "text": "Ozone example\n\nCreating an AR(1) model: Daily ozone levels in Houston\n\n\nggplot(ozone, aes(x = day, y = ozone)) + \n  geom_line()"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#acf-plot",
    "href": "sp_sta235_2026/week_07.html#acf-plot",
    "title": "Data Science for Business Applications",
    "section": "ACF plot",
    "text": "ACF plot\n\nVisualizing the autocorrelation function (ACF)\n\n\nacf(ozone$ozone)\n\n\n\nAutocorrelations outside of the dashed blue lines are statistically significant."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#autorgression-of-the-model",
    "href": "sp_sta235_2026/week_07.html#autorgression-of-the-model",
    "title": "Data Science for Business Applications",
    "section": "Autorgression of the model",
    "text": "Autorgression of the model\n\nWe use the lag function to create the lagged observations\n\n\nozone &lt;- ozone %&gt;% \n  mutate(lag1=lag(ozone)) \nozone.model = lm(ozone ~ lag1, data=ozone) \nsummary(ozone.model)\n\n\nCall:\nlm(formula = ozone ~ lag1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.192  -3.464  -1.108   2.679  16.679 \n\nCoefficients:\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept)  6.87446    1.06976   6.426 0.00000000276 ***\nlag1         0.40419    0.08381   4.823 0.00000419740 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.999 on 120 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1624,    Adjusted R-squared:  0.1554 \nF-statistic: 23.26 on 1 and 120 DF,  p-value: 0.000004197"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#assumptions-of-an-ar1-model",
    "href": "sp_sta235_2026/week_07.html#assumptions-of-an-ar1-model",
    "title": "Data Science for Business Applications",
    "section": "Assumptions of an AR(1) model",
    "text": "Assumptions of an AR(1) model\n\nLinearity, Normality, Equal Variance: Check using residual plot (linearity + homoscedasticity), Q-Q plot (normality), scale/location (homoscedasticity) like any other regression model\nIndependence: Since this is a time series, we can actually check this by looking at the autocorrelation of the residuals (we want no significant autocorrelation)"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#autoplot",
    "href": "sp_sta235_2026/week_07.html#autoplot",
    "title": "Data Science for Business Applications",
    "section": "Autoplot",
    "text": "Autoplot\n\nLinearity, Normality, Equal Variance\n\n\nautoplot(ozone.model)"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#acf-of-the-residuals",
    "href": "sp_sta235_2026/week_07.html#acf-of-the-residuals",
    "title": "Data Science for Business Applications",
    "section": "ACF of the residuals",
    "text": "ACF of the residuals\n\nacf(ozone.model$residuals)\n\n\n\nWe expect 5% of autocorrelations to be significant just by chance, so having just 1 out of the 20 lags flagged as significant indicates we are OK on independence!"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#making-predictions-in-time-series",
    "href": "sp_sta235_2026/week_07.html#making-predictions-in-time-series",
    "title": "Data Science for Business Applications",
    "section": "Making predictions in time series",
    "text": "Making predictions in time series\n\n\n\n\n\n\n\n\nType\nModel\nPredicted \\(Y_t\\)\n\n\n\n\nWhite noise\n\\(Y_t = e_t\\)\n\\(0\\)\n\n\nRandom sample\n\\(Y_t = \\beta_0 + e_t\\)\n\\(\\widehat{\\beta}_0\\) (or average \\(Y\\))\n\n\nRandom walk\n\\(Y_t = \\beta_0 + Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + Y_{t-1}\\)\n\n\nGeneral AR(1)\n\\(Y_t = \\beta_0 + \\beta_1 Y_{t-1} + e_t\\)\n\\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 Y_{t-1}\\)\n\n\n\n\nUnit root occurs when \\(\\beta_1 = 1\\). This means:\nThe series is a random walk.\nThere’s no mean reversion, and any shocks will have a permanent effect.\nWhen \\(\\beta_1 = 1\\), the model is non-stationary, meaning the series tends to “drift” without stabilizing around a fixed mean.\nIf \\(|\\beta_1| &lt; 1\\), the series is mean-reverting, and shocks are temporary."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#statistical-analysis",
    "href": "sp_sta235_2026/week_07.html#statistical-analysis",
    "title": "Data Science for Business Applications",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nconfint(ozone.model)\n\n                2.5 %    97.5 %\n(Intercept) 4.7564110 8.9925161\nlag1        0.2382561 0.5701286\n\n\n\nThe coefficient \\(\\widehat{\\beta}_1\\) is associated with the variable lag1.\nIn this case, for the larger population, with 95% confidence, \\(\\widehat{\\beta}_1\\) lies between 0.24 and 0.57.\nThis means that \\(|\\beta_1| &lt; 1\\), indicating that the series is mean-reverting."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-revenue-acf-plot",
    "href": "sp_sta235_2026/week_07.html#apple-revenue-acf-plot",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the additive model."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-revenue",
    "href": "sp_sta235_2026/week_07.html#apple-revenue",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nCombining decomposition and autoregression in a multiplicative model\n\n\\[\n\\log(\\texttt{Revenue}_t) = \\log(\\texttt{Period}_t) + \\texttt{Quarter}_t + \\log(\\texttt{Revenue}_{t-1})\n\\]\n\nWe need to create the lag variable.\nIt will have only one lag, and thus is an AR(1) model.\n\n\napple = apple %&gt;% \n  mutate(lag1 = lag(Revenue))"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-revenue-1",
    "href": "sp_sta235_2026/week_07.html#apple-revenue-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue",
    "text": "Apple Revenue\n\nlog_apple = lm(log(Revenue) ~ log(Period) + Quarter + log(lag1), data = apple)\nsummary(log_apple)\n\n\nCall:\nlm(formula = log(Revenue) ~ log(Period) + Quarter + log(lag1), \n    data = apple)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19347 -0.06209  0.01017  0.06018  0.18761 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  1.11675    0.17179   6.501   0.0000000188344198 ***\nlog(Period)  0.20925    0.06715   3.116              0.00283 ** \nQuarterQ2   -0.53111    0.04650 -11.423 &lt; 0.0000000000000002 ***\nQuarterQ3   -0.45559    0.03154 -14.446 &lt; 0.0000000000000002 ***\nQuarterQ4   -0.30792    0.03112  -9.895   0.0000000000000381 ***\nlog(lag1)    0.63758    0.09762   6.531   0.0000000167218029 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08801 on 59 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.9766,    Adjusted R-squared:  0.9746 \nF-statistic: 492.7 on 5 and 59 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-revenue-predictions",
    "href": "sp_sta235_2026/week_07.html#apple-revenue-predictions",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nPredictions of multiplicative model"
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-revenue-predictions-1",
    "href": "sp_sta235_2026/week_07.html#apple-revenue-predictions-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue Predictions",
    "text": "Apple Revenue Predictions\n\nConfidence interval of the multiplicative model\n\n\nconfint(log_apple)\n\n                  2.5 %     97.5 %\n(Intercept)  0.77299475  1.4605006\nlog(Period)  0.07488371  0.3436182\nQuarterQ2   -0.62414433 -0.4380662\nQuarterQ3   -0.51869720 -0.3924831\nQuarterQ4   -0.37018901 -0.2456545\nlog(lag1)    0.44224667  0.8329156\n\n\n\nThe slope associated with lag is statistically significant, and its value is between minus and plus one; we have that this is a mean-reverting time series.\nWe also have a better fit (here we feed lag1 with prediction from the previous period, US$ 102.47 billions):\n\n\n exp(predict(log_apple, list(Period = 67, Quarter = \"Q1\", lag1 = 102.47)))\n\n       1 \n140.9366 \n\n\n\nThe confidence interval for the forecast is narrower, and the difference between what we observe and predict is smaller."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#apple-revenue-acf-plot-1",
    "href": "sp_sta235_2026/week_07.html#apple-revenue-acf-plot-1",
    "title": "Data Science for Business Applications",
    "section": "Apple Revenue ACF plot",
    "text": "Apple Revenue ACF plot\n\nACF plot of the residuals of the multiplicative model.\n\n\n\nThe independent assumptions look better, but it might be necessary to add more lags."
  },
  {
    "objectID": "sp_sta235_2026/week_07.html#time-series-strategy",
    "href": "sp_sta235_2026/week_07.html#time-series-strategy",
    "title": "Data Science for Business Applications",
    "section": "Time Series Strategy",
    "text": "Time Series Strategy\nTo building a time series model:\n\nStart with a an additive or multiplicative model with trend and seasonal components. (Plot your data! If the seasonal variation increases or decreases over time you’ll want a multiplicative model.)\nExamine the usual diagnostic plots, and plot your residuals as a function of time. Do you need a (different) nonlinear time trend? A transformation of \\(Y\\)?\nCheck your residuals for autocorrelation. If it’s present, add appropriate lag terms to your model."
  }
]