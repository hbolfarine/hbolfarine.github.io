---
title: "Data Science for Business Applications"
author: "Class 03 - Regression Assumptions, and Potential Problems"
title-slide-attributes:
    data-background-image: week_03_docs/background_sta235h.png
format: 
  revealjs:
    chalkboard: true
    logo: week_03_docs/texas_logo2.png
    toc: true
    toc-title: "Presentation Outline"
    toc-depth: 1
editor: visual
code-block-height: 500px
include-in-header:
  - text: |
      <style type="text/css">
      ul li ul li {
        font-size: 0.8em;
      }
      </style>
---

```{r init_setup, include=FALSE, cache=FALSE}
# Include setup
library(tidyverse)
library(ggfortify)
library(knitr)

knitr::opts_chunk$set(
  echo = FALSE, fig.align = "center"
)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

```{r, include=FALSE, cache=FALSE}
linear_data = read.csv("week_03_docs/linear_data.csv")
nonlinear_data = read.csv("week_03_docs/nonlinear_data.csv")
non_normal = read.csv("week_03_docs/nonnormal_data.csv")
heter_data = read.csv("week_03_docs/heter_data.csv")
outlier_residual = read.csv("week_03_docs/outlier_data.csv")
# outlier_leverage = read.csv("week_04_docs/")
# outlier_influence = read.csv("week_04_docs/")
options(scipen = 999)
```

## Regression Assumptions, and Potential Problems {.smaller}

Linear models are useful:

-   **Prediction** - given a new observations

-   **Explanatory power**- which variables affects the response

But issues in linear model are not uncommon:

-   They can affect the **explanatory**, and **predictive** power of our model

-   They can affect our **confidence** in our model

-   We will look at some of the most common **problems** in linear regression, and how we can **fix** them

## Regression Assumptions, and Potential Problems

These issues are related to:

-   Regression **model assumptions**
-   **Influential observations**, and **outliers**

## Multiple regression assumptions

We need four things to be true for regression to work properly:

-   **Linearity**: $Y$ is a linear function of the $X$’s (except for the prediction errors).

-   **Independence**: The prediction errors are independent.

-   **Normality**: The prediction errors are normally distributed.

-   **Equal Variance**: The variance of $Y$ is the same for any value of $X$ (“homoscedasticity”).

## Non-Linearity {.smaller}

-   What we would expect to observe in a regression where there is a [linear relation]{style="color:darkorange;"}?

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
library(tidyverse)
ggplot(linear_data, aes(x=X, y=Y)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE)
```

## Residuals

-   Let's plot the residuals $r_i$, such that $$r_i = y_i − \widehat{y}_i$$ where $\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i$ vs $x_i$
-   Residuals are basically the distances between the model and the points
-   Hopefully identify non-linear relationships
-   We are looking for patterns or trends in the residuals

## Residuals {.smaller}

-   Plot of the residuals
-   How can these residuals be useful for us?

```{r, fig.align = 'center', echo = FALSE}
set.seed(2)
x=rnorm(300) 
y=x+rnorm(300, 0, 0.3)
model <- lm(y ~ x)
fitted = predict(model)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) +
  geom_segment(aes(xend=x, yend=fitted, col=ifelse(fitted-y>0, "green", "red")), show.legend=F) +
  geom_point() +
  geom_smooth(method="lm", se=F)
```

## Regression diagnostic plots

We’ll use regression diagnostic plots to help us evaluate some of the assumptions.

The **residuals vs fitted** graph plots:

-   [Residuals]{style="color:darkorange;"} on the $Y$-axis
-   [Fitted values]{style="color:darkorange;"} (predicted $Y$ values) on the $X$-axis

This graph effectively subtracts out the linear trend between $Y$ and the $X$’s, so we want to [see no trend left]{style="color:darkorange;"} in this graph.

## Regression diagnostic plot {.smaller}

-   To check non-linearity we focus on the [Residual vs. Fitted]{style="color:darkorange;"} plot

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
library(ggfortify)

lm1 = lm(Y ~ X, data = linear_data)
autoplot(lm1)
```

## Regression diagnostic plot

-   From the [Residual vs. Fitted]{style="color:darkorange;"} plot, we can observe that since the residuals **are evenly distributed** around zero in relation to the fitted values, we have that the [linear regression model is a good fit]{style="color:darkorange;"} for this data.

-   This means that we are **learning the linear representation** contained in this data.

## Non-Linearity Example {.smaller}

-   What we would expect to observe if the relation is [non linear]{style="color:darkorange;"}?

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
ggplot(nonlinear_data, aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE)
```

## Non-Linearity Example {.smaller}

-   Let's look at the residuals for this model

```{r, fig.align = 'center', echo = FALSE}
set.seed(2)
x=rnorm(300) 
y=x^2+rnorm(300, 0, 0.2)
model <- lm(y ~ x)
fitted = predict(model)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) +
  geom_segment(aes(xend=x, yend=fitted, col=ifelse(fitted-y>0, "green", "red")), show.legend=F) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE)
```

-   Let's check the **residual plot**

## Non-Linearity Example {.smaller}

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
lm2 = lm(Y ~ X, data = nonlinear_data)
autoplot(lm2)
```

## Non-Linearity Example

-   From the [Residual vs. Fitted]{style="color:darkorange;"}, we can observe that the residuals are not evenly distributed around zero.

-   This indicates that for lower and higher values of $x_i$ our model is **overpredicting** and **underpredicting** in the mid values.

-   What are the implications in this case?

-   [Worse predictions]{style="color:darkorange;"}

## Independence

-   [Independence]{style="color:darkorange;"} means that knowing the prediction error for one observation doesn’t tell you anything about the error for another observation
-   Data collected over time are usually not independent
-   We [can’t use regression diagnostics]{style="color:darkorange;"} to decide the independence
-   We have to measure the [autocorrelation]{style="color:darkorange;"} of the residuals
-   We'll get back to [autocorrelation]{style="color:darkorange;"} when we discuss Time Series models

## Normality assumption

-   When we’ve been interpreting **residual standard error (RSE)** , we’ve used the following interpretation:
-   95% of our [predictions]{style="color:darkorange;"} will be accurate to within plus or minus $2\times RSE$.
-   In order for this to be true, the residuals have to be [Normally distributed]{style="color:darkorange;"}

## Normality example {.smaller}

-   We can check the distribution of the residuals

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true

linear_data = linear_data %>% 
  mutate(resid = residuals(lm1))

ggplot(linear_data, aes(x = resid)) + 
  geom_histogram(color = "grey", binwidth = 0.2) 

```

## Normality example

-   But how can we judge if the residuals follows a Normal distribution?
-   The key is to look at the **Normal Q-Q** plot, which compares the distribution of our residuals to a perfect Normal distribution.
-   If the dots line up along an [(approximately) straight line]{style="color:darkorange;"}, then the Normality assumption is satisfied.

## Regression diagnostic plot {.smaller}

-   To check for Normality we focus on the **Normal Q-Q** plot

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true

lm1 = lm(Y ~ X, data = linear_data)
autoplot(lm1)
```

-   In this case the normality assumptions seem to be met

## Normality example {.smaller}

-   Let's look at different data.
-   In this case the data has non Normal errors.

```{r}
ggplot(non_normal, aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method="lm", se=F)
```

## Normality example {.smaller}

::: nonincremental
-   Histogram of the residuals (right skewed)
:::

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
lm3 = lm(Y ~ X, data = non_normal)

non_normal = non_normal %>% 
  mutate(resid = residuals(lm3))

ggplot(non_normal, aes(x = resid)) + 
  geom_histogram(color = "grey", binwidth = 1) 
```

## Regression diagnostic plot {.smaller}

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true

autoplot(lm3)
```

## Interpretation of the plot

-   From the [Normal Q-Q]{style="color:darkorange;"} plot, we can observe that the residuals [are not]{style="color:darkorange;"} following the line that indicates the Normal quantiles

-   This means that our model results in [non-normal residuals]{style="color:darkorange;"}

-   This affects [statistical tests]{style="color:darkorange;"}, and [confidence intervals]{style="color:darkorange;"}

## Equal variance {.smaller}

-   Equal variance is also known as [“homoscedasticity”]{style="color:darkorange;"}
-   The variance of $Y$ should be about the same at any $X$ value (or combination of values for the $X$’s).
-   In other words, the vertical spread of the points should be the same anywhere along the $X$-axis.
-   If there's no equal variance then we might have heteroskedasticity.
-   Lower precision, estimates are further from the correct population value.

```{r}
heter_data = read.csv("week_03_docs/heter_data.csv")
```

## Equal variance example {.smaller}

-   The vertical spread of the points is larger along the right side of the graph

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
ggplot(heter_data, aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method="lm", se = FALSE)
```

## Regression diagnostic plot {.smaller}

::: nonincremental
-   To check for [homoscidacity]{style="color:darkorange;"} we focus on the [Scale-Location]{style="color:darkorange;"} plot
:::

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true

lm4 = lm(Y ~ X, data = heter_data)
autoplot(lm4)
```

## Interpretation of the plot

-   From the [Sacle-Location]{style="color:darkorange;"} plot, we can observe that the residuals have a [fan shape]{style="color:darkorange;"}, indicating that there is heteroscedacity in the data.

-   This resulted in lower precision; thus, estimates are further from the correct population value.

## Example: Austin houses {.smaller}

```{r}
houses = read.csv("week_03_docs/houses.csv")
```

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true

model.houses = lm(price ~ area, data = houses)
autoplot(model.houses)
```

```{r}
outlier_residual = read.csv("week_03_docs/outlier_residual.csv")
```

## Influential observations {.smaller}

-   Adding a new observation with $X$ near the mean of $X$ doesn’t matter much even if it’s out of line with the rest of the data:

```{r, echo = FALSE, fig.align = 'center'}
hx <- 0
hy <- -8
ggplot(outlier_residual, aes(x=X, y=Y)) +
  geom_point(col = ifelse(outlier_residual$X == hx & outlier_residual$Y == hy, "red", "black")) +
  geom_smooth(method = "lm", se=F)
```

-   This point has high residual but low leverage. RSE = 0.5504

## Diagnostics Plot {.smaller}

-   We can observe the point with high residual on the Residual vs. Leverage plot

```{r}
#| eval: true
#| echo: true
#| output: true
lm5 = lm(Y ~ X, data = outlier_residual)
autoplot(lm5)
```

```{r}
outlier_leverage = read.csv("week_03_docs/outlier_leverage.csv")
```

## High leverage {.smaller}

-   We can also have points with high leverage - when a point in $X$ is distant from the average on $X$

```{r, echo = FALSE, fig.align = 'center'}
hx <- 5
hy <- 5
ggplot(outlier_leverage, aes(x=X, y=Y)) +
  geom_point(col = ifelse(outlier_leverage$X == hx & outlier_leverage$Y == hy, "red", "black")) +
  geom_smooth(method = "lm", se=F)
```

-   This point has low residual but high leverage. RSE = 0.2956

## High leverage {.smaller}

-   We can observe the point with high leverage on the Residual vs. Leverage plot

```{r}
#| eval: true
#| echo: true
#| output: true
lm6 = lm(Y ~ X, data = outlier_leverage)
autoplot(lm6)
```

```{r}
outlier_influence = read.csv("week_03_docs/outlier_influence.csv")
```

## Points with high influence {.smaller}

-   Points with high leverage and high residuals are known as influential points

```{r, echo = FALSE, fig.align = 'center'}
hx <- 7.5
hy <- -7
ggplot(outlier_influence, aes(x=X, y=Y)) +
  geom_point(col = ifelse(outlier_influence$X == hx & outlier_influence$Y == hy, "red", "black")) +
  geom_smooth(method = "lm", se=F)
```

-   This point has high residual but high leverage. RSE = 0.8281

## Points with high influence {.smaller}

-   We can observe the point with high influence on the Residual vs. Leverage plot

```{r}
#| eval: true
#| echo: true
#| output: true
lm7 = lm(Y ~ X, data = outlier_influence)
autoplot(lm7)
```

## Points with high influence {.smaller}

-   When a case has a very unusual $X$ value, it has leverage — the potential to have a big impact on the regression line
-   If the case is in line with the overall trend of the regression line, it won’t be a problem
-   But when that case also has a $Y$ (high residual) value that is out of line
-   We need both a large residual and high leverage for an observation to be influential
-   We should be worried about these points
-   They affect the coefficents and predictions

## Important take aways {.smaller}

-   Violations of linearity are the most serious: Everything is wrong
    -   If only linearity holds, we can make good predictions
    -   But not prediction intervals, coefficient estimates, or confidence intervals
-   Violations of normality are the least serious:
    -   Prediction intervals are wrong, but everything else is OK if sample size is large enough-
    -   Violations of normality/equal variance can be addressed with bootstrap techniques (beyond the scope of this class)
-   Violations of independence are best addressed by modeling the dependence
    -   If we don't, our intervals will tend to be **too small**

- **Important**: Often fixing a linearity problem will fix other issues as well, so we usually check this first.
