---
title: "Data Science for Business Applications"
author: "Class 07 - Model building for prediction"
title-slide-attributes:
    data-background-image: week_07_docs/background_sta235h.png
format: 
  revealjs:
    chalkboard: true
    logo: week_07_docs/texas_logo2.png
    toc: true
    toc-title: "Presentation Outline"
    toc-depth: 1
editor: visual
code-block-height: 500px
include-in-header:
  - text: |
      <style type="text/css">
      ul li ul li {
        font-size: 0.8em;
      }
      </style>
---

```{r init_setup, include=FALSE, cache=FALSE}
# Include setup
library(tidyverse)
library(ggfortify)
library(knitr)

knitr::opts_chunk$set(
  echo = FALSE, fig.align = "center"
)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines) == 1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

```{r}
# library(ggfortify)
n <- 3
hues = seq(15, 375, length = n + 1)
clrs <- hcl(h = hues, l = 65, c = 100)[1:n]
par(family='Alegreya', mar=c(4.1, 4.1, 1.1, 1.1))
utilities <- read.csv("week_07_docs/utilities.csv")
apple <- read.csv("week_07_docs/apple_2025.csv")
library(tidymodels)
# options(digits=4)
options(scipen = 999)
```

# Mastery Exam A {.smaller}

::: incremental
-   Mastery Exam A is 7-9 PM on Wednesday, October 22 or Thursday, October 23 (your choice---sign up for a time using the survey link sent out).
-   The exam covers Unit A (weeks 1-7, including today).
-   The exam will be on Canvas using Respondus.
-   You can bring 1 full-sized page of notes (whatever you want, as long as you make it yourself).
-   Two types of questions:
    -   Conceptual and computational questions like the homework and quizzes
    -   Open-ended questions that ask you to apply the concepts to a new business situation and write up your analysis in paragraph form
-   Practice material for the exam is posted in Canvas.
:::

# Two reasons for building regression models {.smaller}

-   **Prediction**: Predict the value of $Y$ based on the values of the $X$ variables (today)
-   **Understanding**: Understand the relationships between $Y$ and the $X$ variables (later in the semester)

How we evaluate a model depends on what our use for it is.

# Evaluating forecasts

## How should we evaluate a forecast?

```{r echo=FALSE}
ggplot(apple, aes(x=Time, y=Revenue)) + 
  geom_line() +
  ggtitle("Apple Quarterly Revenue") +
  xlab("Year") + ylab("Revenue ($B)")
```

##  {.smaller}

```{r echo=T}
apple <- apple %>% mutate(
  COVID = ifelse(Period >= 38 & Period <= 45, 1, 0),
  lag1 = lag(Revenue)
)
model4 <- lm(log(Revenue) ~ Period + Quarter 
                  + COVID + log(lag1), data=apple)
summary(model4)
```

##  {.smaller}

::: center
If you were an analyst, how impressed would your boss be if you claimed to be able to predict the past? (Not very --- they want you to be able to predict the future!)
:::

## How should we evaluate a forecast?

-   $R^2$ and the residual standard error are *optimistic* measures of predictive performance.
-   They measure the quality of predictions using the same data used to fit the model.
-   But we're interested in how the model predicts into the future, not how well it fits the past!

##  {.smaller}

We can mimic predicting on future data by holding back the most recent observations.

Fit the model to the "training" data only (blue) and test the model by forecast the most recent year of data (red).

```{r echo=FALSE, warning=FALSE, fig.height=2.8}
# Create a column to identify the last 4 data points
apple$highlight <- ifelse(apple$Time >= 2024.25, "Last 4", "Previous")
ggplot(apple, aes(x = Time, y = Revenue)) + 
  # Add line for the entire data series
  geom_line(color = "blue") +
  # Overlay line for last 4 data points in dark gray (testing data)
  geom_line(data = subset(apple, highlight == "Last 4"), color = "red", size = 1) +
  # Add vertical line at the 4th to last data point 
  geom_vline(xintercept = 2024.25, linetype = "dashed", color = "black") +
  # Labels
  xlab("Year") + 
  ylab("Revenue ($B)") +
  # Add text labels for training and testing data
  annotate("text", x = mean(subset(apple, highlight == "Previous")$Time), 
          y = min(apple$Revenue), label = "Training Data", vjust = 0.5) +
  # Rotate the testing data label to be vertical and position it to the right of the dashed line
  annotate("text", x = 2023, y = mean(apple$Revenue), 
          label = "Testing Data")
```

## Fitting the model to the training set {.smaller}

We want to split our data into a test set (last 4 quarters) and a training set (everything else).

```{r echo=TRUE}
training <- apple[1:51,]
test <- apple[52:55,]
```

Then we want to fit the model to the training data and test it on the test data.

```{r echo=TRUE}
model4 <- lm(log(Revenue) ~ Period + Quarter 
                  + COVID + log(lag1), data=training)
```

## Testing the model on the test set {.smaller}

Now we can see how well the model predicts the test set.

```{r echo=TRUE}
predict(model4, test)
```

. . .

These are predictions for log revenue, so we need to convert them back to revenue:

```{r echo=TRUE}
exp(predict(model4, test))
```

. . .

How do we compare this to the actual revenues of the last 4 quarters?

```{r echo=TRUE}
test.Rev = test$Revenue
```

## Measuring prediction error {.smaller}

Mean squared prediction error (MSE or MSPE) lets us compare all 4 predictions to the actual revenues at once:

$$
  \text{MSPE} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat Y_i)^2
$$

($n$ is the size of the test set; $n=4$ this case)

Take the square root to get an interpretable number (in the units of $Y$):

$$
  \text{RMSPE} = \sqrt{\text{MSPE}}
$$

Interpret RMSPE just like residual standard error (smaller is better!)!

## Calculating RMSPE {.smaller}

| $i$ | $Y_i$ | $\hat Y_i$ | $Y_i - \hat Y_i$ | $(Y_i - \hat Y_i)^2$ |
|-------------|-------------|-------------|-----------------|-----------------|
| $1$ | $`r test.Rev[1]`$ | $`r round(exp(predict(model4, test))[1], 2)`$ | $`r round(test.Rev[1] - exp(predict(model4, test))[1], 2)`$ | $`r round((test.Rev[1] - exp(predict(model4, test))[1])^2, 2)`$ | | $2$   | $`r test.Rev[2]`$ | $`r round(exp(predict(model4, test))[2], 2)`$ | $`r round(test.Rev[2] - exp(predict(model4, test))[2], 2)`$ | $`r round((test.Rev[2] - exp(predict(model4, test))[2])^2, 2)`$ |
| $3$ | $`r test.Rev[3]`$ | $`r round(exp(predict(model4, test))[3], 2)`$ | $`r round(test.Rev[3] - exp(predict(model4, test))[3], 2)`$ | $`r round((test.Rev[3] - exp(predict(model4, test))[3])^2, 2)`$ | | $4$   | $`r test.Rev[4]`$ | $`r round(exp(predict(model4, test))[4], 2)`$ | $`r round(test.Rev[4] - exp(predict(model4, test))[4], 2)`$ | $`r round((test.Rev[4] - exp(predict(model4, test))[4])^2, 2)`$ |

. . .

$$
  \text{RMSPE} = \sqrt{\frac{1}{4} \sum_{i=1}^4 (Y_i - \hat Y_i)^2} = `r round(sqrt(mean((test$Revenue - exp(predict(model4, test)))^2)), 2)`
$$

. . .

In other words, we expect our model to be off by about 
```{r echo=TRUE}
round(sqrt(mean((test$Revenue - exp(predict(model4, test)))^2)), 2)
``` 
billion dollars on average *when forecasting the future*.

## A shortcut for calculating RMSPE {.smaller}

```{r echo=TRUE}
# exp here only because we're predicting log(Revenue) instead of Revenue
predictions <- exp(predict(model4, test))
actuals <- test$Revenue
sqrt(mean((predictions - actuals)^2))
```

# Training/test splits in general

## Train/test splits to measure performance {.smaller}

:::::: columns
::: {.column width="60%"}
![](week_07_docs/train-test-split-diagram.png)
:::

:::: {.column width="40%"}
::: incremental
-   Randomly allocating observations makes training/testing sets representative of the full data set
-   80/20% training/testing splits are common
-   More testing data $\to$ More accurate estimates of prediction error
-   More training data $\to$ Better representation of full-data fit
:::
::::
::::::

## Predicting utility expenditures {.smaller}

```{r echo=FALSE}
ggplot(aes(x=temp, y=dailyspend), data=utilities) + 
  geom_point()
```

## Creating a train/test split {.smaller}

Let's do the same thing for the utilities data, but by *randomly* splitting the data into a training and test set.

```{r echo=TRUE}
library(tidyverse)
set.seed(9529)

# Ask R to shuffle all 117 observations
randomized <- slice_sample(utilities, n = 117)

# 24 is about 20% of 117, so we'll use the first 24 rows as our test set
utilities_test <- randomized[1:24,]

# The rest are our training set
utilities_train <- randomized[25:117,]
```

The `set.seed(9529)` command ensures reproducible "random" splits. (I chose this number because it's my EID number; use your EID number!)

## Creating a train/test split {.smaller}

Black = training data, Red = testing data

Random splitting is important! (What would happen if we trained on fall-spring and tested on summer?)

```{r fig.width=13}
ggplot(aes(x=temp, y=dailyspend), data=utilities) + 
  geom_point(color="red") + 
  geom_point(data=utilities_train)
```

## Using training/test splits to pick between competing models {.smaller}

When comparing models, we want to pick the one that minimizes prediction error **on the test set** --- we want to avoid overfitting to the training data!

When we fit to the entire data set, we might overfit to the training data and get an overly optimistic estimate of prediction error on new data.

## A linear (1st-degree) polynomial model {.smaller}

```{r}
options(scipen=9)
```

::::: columns
::: {.column width="60%"}
```{r echo=1}
model1 <- lm(dailyspend ~ temp, data=utilities)
summary(model1)
```
:::

::: {.column width="40%"}
```{r echo=FALSE, fig.height=10}
ggplot(utilities, aes(x=temp, y=dailyspend)) +
  geom_point() +
  stat_smooth(method="lm", se=T, fill=NA,
              formula=y ~ poly(x, 1, raw=TRUE), color="red")
```
:::
:::::

## A quadratic (2nd-degree) polynomial model {.smaller}

::::: columns
::: {.column width="60%"}
```{r echo=1}
model2 <- lm(dailyspend ~ temp + I(temp^2), data=utilities)
summary(model2)
```
:::

::: {.column width="40%"}
```{r echo=FALSE, fig.height=10}
ggplot(utilities, aes(x=temp, y=dailyspend)) +
  geom_point() +
  stat_smooth(method="lm", se=T, fill=NA,
              formula=y ~ poly(x, 2, raw=TRUE), color="red")
```
:::
:::::

## A 9th-degree polynomial model {.smaller}

::::: columns
::: {.column width="60%"}
```{r echo=1}
model9 <- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) 
              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),
              data=utilities)
summary(model9)
```
:::

::: {.column width="40%"}
```{r echo=FALSE, fig.height=10}
ggplot(utilities, aes(x=temp, y=dailyspend)) +
  geom_point() +
  stat_smooth(method="lm", se=T, fill=NA,
              formula=y ~ poly(x, 9, raw=TRUE), color="red")
```
:::
:::::

## Building the models {.smaller}

We can avoid overfitting to the training data by fitting the models to the training data and then testing them on the test data.

Fit the models **using the training set**:

```{r echo=TRUE}
model1 <- lm(dailyspend ~ temp, data=utilities_train)
model2 <- lm(dailyspend ~ temp + I(temp^2), data=utilities_train)
model9 <- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) 
              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),
              data=utilities_train)
```

Get their predictions **on the test set**:

```{r echo=TRUE}
actuals <- utilities_test$dailyspend
predictions1 <- predict(model1, utilities_test)
predictions2 <- predict(model2, utilities_test)
predictions9 <- predict(model9, utilities_test)
```

## Measuring performance {.smaller}

Now let's calculate RMSPE on the testing data:

```{r echo=TRUE}
sqrt(mean((actuals - predictions1)^2))
sqrt(mean((actuals - predictions2)^2))
sqrt(mean((actuals - predictions9)^2))
```

. . .

| Model | RSE (all data) | RMSPE (training/test split) |
|--------------------------|-------------------|--------------------------|
| Linear | $`r round(sigma(model1), 2)`$ | $`r round(sqrt(mean((actuals - predictions1)^2)), 2)`$ |
| Quadratic | $`r round(sigma(model2), 2)`$ | $`r round(sqrt(mean((actuals - predictions2)^2)), 2)`$ |
| 9th Degree | $`r round(sigma(model9), 2)`$ | $`r round(sqrt(mean((actuals - predictions9)^2)), 2)`$ |

. . .

-   These are all worse than the predictions of the models fit to the entire data set!
-   It shows that the quadratic model is the best of the three for making predictions on new data

## The problem with this approach {.smaller}

::: center
We might get "unlucky," and our train/test split might not give us a representative estimate of prediction error!
:::

# Cross-validation

## $k$-fold Cross-Validation {.smaller}

::: center
![](week_07_docs/k-fold-cv-diagram.png)
:::

::: incremental
-   Averages results over $k$ random train/test splits, to avoid getting bad results from possible bad luck of a single train/test split.
-   $k=n$ is called *leave-one-out* cross validation
:::

## Let's work through an example with $k=3$ {.smaller}

First we need to split the data into 3 random groups of roughly the same size ($39\times 3=117$):

```{r echo=TRUE}
fold1 <- randomized[1:39,]
fold2 <- randomized[40:78,]
fold3 <- randomized[79:117,]
```

. . .

Now we need to build the model 3 times:

-   Model A: test on `fold1`, train on `fold2` and `fold3`
-   Model B: test on `fold2`, train on `fold1` and `fold3`
-   Model C: test on `fold3`, train on `fold1` and `fold2`

## Building the models {.smaller}

```{r echo=TRUE}
modelA <- lm(dailyspend ~ temp, data=rbind(fold2, fold3))
modelB <- lm(dailyspend ~ temp, data=rbind(fold1, fold3))
modelC <- lm(dailyspend ~ temp, data=rbind(fold1, fold2))
```

## Getting the predictions {.smaller}

```{r echo=TRUE}
predictionsA <- predict(modelA, fold1)
predictionsB <- predict(modelB, fold2)
predictionsC <- predict(modelC, fold3)
```

## Calculating RMSPE {.smaller}

We now get 3 different RMSPE estimates:

```{r echo=TRUE}
actualsA <- fold1$dailyspend
sqrt(mean((actualsA - predictionsA)^2))

actualsB <- fold2$dailyspend
sqrt(mean((actualsB - predictionsB)^2))

actualsC <- fold3$dailyspend
sqrt(mean((actualsC - predictionsC)^2))
```

Our final estimate of RMSPE is the average of these 3 estimates:

$$
  \text{RMSPE} = \frac{ `r round(sqrt(mean((fold1$dailyspend - predictionsA)^2)), 2)` + `r round(sqrt(mean((fold2$dailyspend - predictionsB)^2)), 2)` + `r round(sqrt(mean((fold3$dailyspend - predictionsC)^2)), 2)` }{3} = `r round((sqrt(mean((fold1$dailyspend - predictionsA)^2)) + sqrt(mean((fold2$dailyspend - predictionsB)^2)) + sqrt(mean((fold3$dailyspend - predictionsC)^2))) / 3, 2)`
$$

## Re-doing our model comparisons {.smaller}

```{r}
model1A <- lm(dailyspend ~ temp, data=rbind(fold2, fold3))
model1B <- lm(dailyspend ~ temp, data=rbind(fold1, fold3))
model1C <- lm(dailyspend ~ temp, data=rbind(fold1, fold2))
predictions1A <- predict(model1A, fold1)
predictions1B <- predict(model1B, fold2)
predictions1C <- predict(model1C, fold3)
actualsA <- fold1$dailyspend
actualsB <- fold2$dailyspend
actualsC <- fold3$dailyspend
rmse1 <- (sqrt(mean((actualsA - predictions1A)^2)) + sqrt(mean((actualsB - predictions1B)^2)) + sqrt(mean((actualsC - predictions1C)^2)))/3

model2A <- lm(dailyspend ~ temp + I(temp^2), data=rbind(fold2, fold3))
model2B <- lm(dailyspend ~ temp + I(temp^2), data=rbind(fold1, fold3))
model2C <- lm(dailyspend ~ temp + I(temp^2), data=rbind(fold1, fold2))
predictions2A <- predict(model2A, fold1)
predictions2B <- predict(model2B, fold2)
predictions2C <- predict(model2C, fold3)
actualsA <- fold1$dailyspend
actualsB <- fold2$dailyspend
actualsC <- fold3$dailyspend
rmse2 <- (sqrt(mean((actualsA - predictions2A)^2)) + sqrt(mean((actualsB - predictions2B)^2)) + sqrt(mean((actualsC - predictions2C)^2)))/3

model9A <- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) 
              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),
              data=rbind(fold2, fold3))
model9B <- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) 
              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),
              data=rbind(fold1, fold3))
model9C <- lm(dailyspend ~ temp + I(temp^2) + I(temp^3) + I(temp^4) 
              + I(temp^5) + I(temp^6) + I(temp^7) + I(temp^8) + I(temp^9),
              data=rbind(fold1, fold2))
predictions9A <- predict(model9A, fold1)
predictions9B <- predict(model9B, fold2)
predictions9C <- predict(model9C, fold3)
actualsA <- fold1$dailyspend
actualsB <- fold2$dailyspend
actualsC <- fold3$dailyspend
rmse9 <- (sqrt(mean((actualsA - predictions9A)^2)) + sqrt(mean((actualsB - predictions9B)^2)) + sqrt(mean((actualsC - predictions9C)^2)))/3
```

| Model | RSE (all data) | RMSPE (3 Fold CV) | RMSPE (Single Train/Test Split) |
|--------------|-----------------------|--------------|----------------------|
| Linear | $`r round(sigma(model1), 2)`$ | $`r round(rmse1, 2)`$ | $`r round(sqrt(mean((actuals - predictions1)^2)), 2)`$ |
| Quadratic | $`r round(sigma(model2), 2)`$ | $`r round(rmse2, 2)`$ | $`r round(sqrt(mean((actuals - predictions2)^2)), 2)`$ |
| 9th Degree | $`r round(sigma(model9), 2)`$ | $`r round(rmse9, 2)`$ | $`r round(sqrt(mean((actuals - predictions9)^2)), 2)`$ |

::: incremental
-   Comparing RSE and CV:
    -   RSE misses overfitting
    -   For every model, RSE is too optimistic about how well the model will perform on new data
-   Our initial train/test split was unlucky! It over-estimated the RMSE
:::

## Isn't there a faster way? {.smaller}

In practice, you don't have to do this all manually! The `tidymodels` package makes it easy to do cross-validation even for larger $k$.

```{r echo=TRUE}
library(tidymodels)

# Randomly split the data into 5 folds
folds <- vfold_cv(utilities, v=5)

# Build a tidymodels "workflow" for a linear model
# with the formula dailyspend ~ temp, using RMSE as the target metric
workflow() %>% 
  add_model(linear_reg()) %>%
  add_formula(dailyspend ~ temp) %>%
  fit_resamples(folds, metrics=metric_set(rmse)) %>%
  collect_metrics()
```
