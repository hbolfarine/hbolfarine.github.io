---
title: "Data Science for Business Applications"
author: "Class 06 - Model Selection"
title-slide-attributes:
    data-background-image: image/background_sta235h.png
format:
  revealjs: 
    theme: "my-styles.css"
    logo: image/texas_logo2.png
    incremental: true 
    self-contained-math: true
    slide-number: true
    show-slide-number: print
    
---

## Introduction to prediction {.smaller}

* So far, we have been focusing mostly on trying to [explain]{style="color:darkorange;"} the effects from the predictors $X$ through the coefficients.
* Until now, our focus was on the [soundness of our model]{style="color:darkorange;"} in relation to [statistical significance]{style="color:darkorange;"} and how well our model was fitting the data (Regression Assumptions).
* Today, we will focus on making models that return [Estimate/predict]{style="color:darkorchid;"} outcomes with high accuracy [without extrapolating]{style="color:darkorchid;"}, with [previously unseen data]{style="color:darkorchid;"}.

## Inference and Prediction 

* Inference $\rightarrow$ focus on the [predictor]{style="color:darkorange;"}
* [*Interpretability of model*]{style="color:darkorchid;"}

* Prediction $\rightarrow$ focus on [outcome variable]{style="color:darkorange;"}
* [*Accuracy of model*]{style="color:darkorchid;"} 

## Bias vs. Variance {.smaller}

* Bias vs Variance trade-off
* [**Variance**]{style="color:darkorange;"}: *The amount by which the function $f$ would change if we estimated it using a different training dataset*
* [**Bias**]{style="color:darkorange;"}: *Error introduced by approximating a real-life problem with a model*

* [More flexible]{style="color:darkorange;"} models have a [higher variance]{style="color:darkorange;"} and a lower bias 
* [Less flexible]{style="color:darkorange;"} models have a [lower variance]{style="color:darkorange;"} but a higher bias

* Validation set approach: [**Training and testing data**]{style="color:darkorange;"}
* Balance between flexibility and accuracy

## Bias vs. Variance

* When explaining, [bias]{style="color:darkorange;"} is usually greater than [variance]{style="color:darkorange;"}
* In [prediction]{style="color:darkorange;"}, we care about [both]{style="color:darkorange;"}
* Measures of [accuracy]{style="color:darkorange;"} will have both [bias]{style="color:darkorange;"} and [variance]{style="color:darkorange;"}
 
## Measures of accuracy {.smaller}

* How do we [measure accuracy]{style="color:darkorange;"}?
* [Mean Squared Error]{style="color:darkorange;"} (MSE): Can be decomposed into variance and bias terms
$$
\text{MSE} = \text{Var} + \text{Bias}^2 + \text{Irreducible Error} 
$$
where MSE is equal to 
$$
MSE = \frac{1}{n} \sum_{i = 1}^n(y_i-\widehat{y}_i)^2
$$

* [Root Mean Squared Error]{style="color:darkorange;"} (RMSE): Measured in the same units as the outcome
$$
\text{RMSE} = \sqrt{\text{MSE}}
$$

* Other measures: Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC)

## Is flexibility always better? {.smaller}

![](image/RMSE_train_pred.png){fig-align="center"width=100%}

## Measures of accuracy

* Models with [increasing flexibility]{style="color:darkorange;"} (linear, cubic, spline).
* Think of a spline as a polynomial model with a high degree.
* [RMSE decreases]{style="color:darkorange;"} with flexibility in the [training data]{style="color:darkorange;"}. 
* The spline [overfits]{style="color:darkorange;"} the training data since the RMSE of the testing data is large.

## What is churn? {.smaller}

:::: {.columns}

::: {.column width="55%"}
* Churn: Measure of how many customers [stop]{style="color:darkorange;"} using your product (e.g. cancel a subscription).
* Less costly to keep a customer than bring a new one
* Goal: [Prevent churn]{style="color:darkorange;"}
* Identify customer that are likely to cancel/quit/fail to renew

:::

::: {.column width="45%"}
![](image/Model_selec_churn.png){fig-align="center"}
:::

::::

```{r}
hbo = read.csv("script_class_06/hbomax.csv")
```

## Predicting "pre-churn"

* We will predict "pre-churn".
* At a good measure for someone at risk of unsubscribing ("pre-churn") is the times they've [logged in the past week]{style="color:darkorange;"}.
* We are interested in the number of log ins in the variable `logins`.
* We will predict `logins` from the other variable in the data.
* We two candidates: [*Simple*]{style="color:darkorange;"} vs [*Complex*]{style="color:darkorange;"}

## Predicting "pre-churn" {.smaller}

* *Simple* Model:
$$
logins = \beta_0 + \beta_1 \cdot Succession + \beta_2 \cdot city + \epsilon
$$

* *Complex* Model:
$$
logins = \beta_0 + \beta_1 \cdot Succession + \beta_2 \cdot age + \beta_3 \cdot age^2 + \beta_4 \cdot city + \beta_5 \cdot female + \epsilon
$$

* Can we build more complex methods? Yes!
* First we will just analyse these two.

## Create Validation Sets {.smaller}

* Create [Training]{style="color:darkorange;"} and [Testing]{style="color:darkorange;"} sets
* We will use 75% of the data to train the data
* The remaining part of the data, 25%, we reserve for testing 
* This split is done randomly
* To do so we use the libraries `modelr`, and `rsample`

```{r}
#| eval: true
#| echo: true
#| output: true
library(modelr) # for common model performance metrics
library(rsample)  # for creating train/test splits

set.seed(100) #Always set seed for replication
hbo_split =  initial_split(hbo, prop=0.75)
hbo_train = training(hbo_split)
hbo_test  = testing(hbo_split)
```

## RMSE in training and testing data {.smaller}

```{r}
#| eval: true
#| echo: true
#| output: true

# Simple Model
lm_simple = lm(logins ~ succession + city, data = hbo_train)

# Complex Model
lm_complex = lm(logins ~ female + city + age + I(age^2) + succession, data = hbo_train)

# Testing error for the simple model:
rmse(lm_simple, hbo_test)

# Testing error for the complex model:
rmse(lm_complex, hbo_test)
```

* Which model we should choose?
* The model with the [smallest out of sample error]{style="color:darkorange;"}
* Out of sample means evaluation in the testing data

## Cross-Validation {.smaller}

* To avoid using only one training and testing dataset, we can iterate over [k-fold]{style="color:darkorange;"} division of our data:

![](image/cross_validation_S2024.png){fig-align="center"width=85%}

* [Grey]{style="color:darkorange;"}: all of the data

* [Pink]{style="color:darkorange;"}: Testing data

* [Yellow]{style="color:darkorange;"}: Training data

## Cross-Validation {.smaller}

Procedure for k-fold cross-validation:

1. Divide your data in k-folds (usually, $K = 5$ or $K = 10$).

2. Use as $k = 1$ the testing data and $k = 2,3,\dots, K$ as the training data.

3. Calculate the accuracy measure on the testing data, $RMSE_k$.

4. Repeat for each $k$.

5. Average $RMSE_k$ for all $k$.

**Main advantage**: Use the entire dataset for training AND testing.

## Apple quarterly revenue {.smaller}

* Install the library `caret`

```{r}
#| eval: true
#| echo: true
#| output: true
library(caret)
set.seed(100)
train.control = trainControl(method = "cv", number = 10)

lm_simple = train(logins ~ succession + city, data = hbo, method= "lm", trControl = train.control)

lm_simple
```


## Stepwise selection {.smaller}

* We have seen how to choose between some given models. But what if we want to test all possible models?

* [Stepwise selection]{style="color:darkorange;"}: Computationally-efficient algorithm to select a model based on the data we have (subset selection).

* Algorithm for forward stepwise selection:

1. Start with the null model, (no predictors)

2. For : [(a)]{style="color:darkorange;"} Consider all models that augment with one additional predictor. [(b)]{style="color:darkorange;"} Choose the best among these models and call it .

3. Select the single best model from using CV.

* Backwards stepwise follows the same procedure, but starts with the full model.

## Stepwise selection and CV {.smaller}

```{r}
#| eval: true
#| echo: true
#| output: false
set.seed(100)
# Linear Regression with Forward Selection
# Remove unsubscribe
train.control = trainControl(method = "cv", number = 10) #set up a 10-fold cv
lm.fwd = train(logins ~.- unsubscribe, data = hbo, method = "leapForward", 
               tuneGrid = data.frame(nvmax = 1:5), trControl = train.control)
```

```{r}
#| eval: true
#| echo: true
#| output: true
lm.fwd$results
```


* Which one would you choose out of the 5 models? Why?
* The model with the [smallest RMSE]{style="color:darkorange;"}, which is model 5.
* Can we see this model?

## Stepwise selection and CV {.smaller}

* And how does that model looks like:

```{r}
#| eval: true
#| echo: true
#| output: true
summary(lm.fwd$finalModel)
```

* The selected model has the following variables:

* `female`, `city`, `age`, `succession`, `id` 

## Conclusion

* In prediction, everything is going to be about:

* [**Bias vs Variance**]{style="color:darkorange;"}

* [**Importance of validation sets**]{style="color:darkorange;"}

* [**We now have methods to select models**]{style="color:darkorange;"}
 