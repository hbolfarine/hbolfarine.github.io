---
title: "Data Science for Business Applications"
author: "Class 01 - Linear Regression Review"
title-slide-attributes:
    data-background-image: image/background_sta235h.png
format:
  revealjs: 
    theme: "my-styles.css"
    logo: image/texas_logo2.png
    incremental: true 
    self-contained-math: true
    
---

## What is Data Science?

- In Data Science we use the available [data]{style="color:darkorange;"} to obtain:

![](image/image_01.png){fig-align="center"width=50%}

## Data Science tasks

*   [**Description**]{style="color:darkslateblue;"}: *Can we classify our customers into different segments?* ([simple task]{style="color:darkred;"})
*   [**Prediction**]{style="color:darkorchid;"}: *What is the probability of a shopper coming back to our website?* ([kind of a simple task]{style="color:darkred;"})
*   [**Causal Inference**]{style="color:orange;"}: *What is the effect of increasing our advertising budget on our total revenue?* ([difficult task]{style="color:darkred;"})

## Simple and Multiple Regression

- [Linear Regression]{style="color:darkorange;"} will be the most important tool for solving these Data Science tasks.
- Basically, in the [linear regression]{style="color:darkorange;"} model, we are [explaining]{style="color:darkorange;"} the relation between different variables by a line ([that's where the linear comes from.]{style="color:darkred;"})
- Many fancy methods are [generalizations or extensions]{style="color:darkorange;"} of Linear Regression!
- In this class, we will do a quick review on linear regression.

## Cookie Example {.smaller}

:::: {.columns}

::: {.column width="60%"}
- Suppose we have some data, and we want to understand how happiness [changes]{style="color:darkorange;"} in relation to the number of cookies eaten.
- To do so, we [summarize]{style="color:darkorange;"} the relation between these two variables through a [line]{style="color:darkorange;"}.
- This line is the [linear regression]{style="color:darkorange;"} line.
- Let's see how this works!
:::

::: {.column width="40%"}

cookies| happiness
:-----:|:-----:
1|0.1
2|2
3|1
4|2.5
5|3
6|1.3
7|1.9
8|2.4
9|1.8
10|3
:::

::::

## Cookie Example {.smaller}

:::: {.columns}

::: {.column width="50%"}

- The [regression line]{style="color:darkorange;"} is defined by two values, the [intercept]{style="color:darkorange;"} and the [slope]{style="color:darkorange;"}. 
- The [intercept]{style="color:darkorange;"} is where the line intercepts the happiness axis.
- The [slope]{style="color:darkorange;"} relates to the inclination of the line.
- If the [slope is positive]{style="color:darkorange;"} the line has a upward direction.
- If the [slope is negative]{style="color:darkorange;"} the line has a downward direction.
- The [regression line]{style="color:darkorange;"} is a [model]{style="color:darkorange;"} of the relation between happiness and the number of cookies eaten.
<!-- - We also refer to these parameters to the Greek letter $\beta$. -->

:::

::: {.column width="50%"}
![](image/cookie_image_03.png){fig-align="center"width=100%}
:::

::::


## Some questions on regression {.smaller}

- From the [regression line]{style="color:darkorchid;"}, what is the relationship between happiness and the number of cookies eaten?
- Are there [other factors]{style="color:darkorchid;"} other than the number of cookies that might affect the happiness level?
- From this model, can we conclude that eating cookies alone [causes]{style="color:darkorchid;"} happiness?
- Why not look only at the [correlation]{style="color:darkorchid;"} between happiness and the number of cookies?
- How do we [obtain]{style="color:darkorchid;"} the intercept and the slope?
- Does this result apply to the [entire population]{style="color:darkorchid;"}?

## Regressions Details

- The [Linear Regression model]{style="color:darkorange;"} is represented by the formula:
$$
Y = \beta_0 + \beta_1\cdot X_1 + \beta_2\cdot X_2 + e 
$$
- [Multiple Regression]{style="color:darkorange;"} means we have two or more $X$'s.
- Let's break down this model into its essential parts.

## Essential Parts of a Regression

- $Y$ - Outcome Variable, Response Variable, Dependent Variable ([Thing you want to explain or predict]{style="color:darkorchid;"})
- $X$ - Explanatory Variable, Predictor Variable, Independent Variable, Covariate ([Thing you use to explain or predict]{style="color:darkorchid;"} $Y$)
- $\beta$'s - Coefficients, Parameters ([How]{style="color:darkorchid;"} $Y$ [changes numerically in relation  to]{style="color:darkorchid;"} $X$)
- $e$ - Residual, Noise ([Things we didn't account in our model]{style="color:darkorchid;"})

## Two Purposes of Regression

![](image/image_02.png){fig-align="center"}

## Back to the Cookie example {.smaller}

- By writing the cookie example as a model where the variable happiness is the [response variable]{style="color:darkorange;"} and the number of cookies is the [predictor]{style="color:darkorange;"}, we have
$$
\texttt{happiness} = \beta_0 + \beta_1\cdot\texttt{cookies} + e
$$
- `happiness` is the [response variable]{style="color:darkorange;"} ($Y$).
- `cookies` is the [predictor variable]{style="color:darkorange;"} ($X$).
- $\beta_0$ is the [intercept]{style="color:darkorange;"}.
- $\beta_1$ is the [slope]{style="color:darkorange;"} associate to the `cookies` variable. 
- $e$ are the [unknown factors]{style="color:darkorange;"} that might explain the relation between cookies and happiness.
- The challenge now is to [estimate]{style="color:darkorange;"} the parameters $\beta_0$, and $\beta_1$.

## How do we estimate the coefficients in a regression?

* A very useful strategy is use what is called the [Ordinary Least Squares]{style="color:darkorange;"} (OLS).
* The method is called ordinary least squares because the algorithm selects the coefficients ($\beta$'s) that
minimize the sum of the squares of the errors in our sample.
* So the data in this case is fundamental.
* We use [**R**]{style="color:blue;"} to learn $\beta$.
* The estimated coefficients are referred to as $\widehat{\beta}$.

## Let's get into some data  {.smaller}

- **Example**: [Movie data Set]{style="color:darkorange;"} (`movie_1990_data.csv`)
- We will create a model that [explains]{style="color:darkorange;"} and [predicts]{style="color:darkorange;"} the movie revenue in terms of the budget. 
- There are 1,368 different movies in the data, with 22 different attributes.
- This means that the data contains 1,368 lines and 22 columns.
- We are interested in two attributes, the [movie budget]{style="color:darkorange;"}, and [movie revenue]{style="color:darkorange;"}.
- [Movie budget]{style="color:darkorange;"} is in the predictor variable - `Adj_Budget`.
- [Movie revenue]{style="color:darkorange;"} is in the response variable - `Adj_Revenue`.
- The units in this case are important (both are in millions of dollars).
- Let's visualize the relation between these two variables.

## {.smaller}
- First we load the library `tidyverse`, then we use the `ggplot` function to make 
plot between `Adj_Budget` and `Adj_Revenue`
```{r}
movie_1990_data = read.csv("rscript_class_01/movie_1990_data.csv")
```

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
#| 
# Load library
library(tidyverse)

# Create plot
ggplot(movie_1990_data) +
  geom_point(aes(x = Adj_Budget, y = Adj_Revenue))
```

<!-- :::: {.columns} -->

<!-- ::: {.column width="45%"} -->
<!-- - Example: [Movie data Set]{style="color:darkorange;"} -->
<!-- - Three criteria: -->
<!-- 1. At least two named women -->
<!-- 2. Who talk to each other -->
<!-- 3. About something besides a man -->
<!-- ::: -->

<!-- ::: {.column width="54%"} -->
<!-- ![](image/bechdel_test_02.png){width=90%} -->
<!-- ::: -->

<!-- :::: -->

## {.smaller}

We encode the model below in [**R**]{style="color:blue;"}.

$$
\texttt{Adj_Revenue} = \beta_0 + \beta_1\cdot\texttt{Adj_Budget} + e
$$
```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true


# The model
# Revenue = intercept + slope*Budget + e
lm1 <- lm(Adj_Revenue ~ Adj_Budget, data = movie_1990_data)
summary(lm1)
```


## Let's interpret the output {.smaller}

- From the coefficients, $\widehat\beta_0 = 22.7$, $\widehat\beta_1 = 1.11$  we have the updated model:
$$
\widehat{\texttt{Adj_Revenue}} = 22.7 + 1.11\cdot\texttt{Adj_Budget}
$$
- When there's a hat, it means that the values were generated from the data.
- $e$ (noise) vanishes because we eliminated the unknown factors and concentrated the effect on what we observe.
- Now we have the [residuals]{style="color:darkorange;"}, that is the distance between the points in the data and the regression model.
- The question now is: how can we [interpret]{style="color:darkorange;"} this result?


## Explanation {.small}

- [**Intercept**]{style="color:darkorange;"}: By setting the movie budget to zero, we have that the average revenue is equal to [$22.7 million dollars]{style="color:darkorange;"}.
- [**Slope**]{style="color:darkorange;"}: For one unit change in the movie budget, that is, millions, there will be an increase of [$1.11 million dollars]{style="color:darkorange;"} in the movie's revenue.

## {.smaller}

Let's visualize this model.

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
#| 
# Create plot with regression line
ggplot(movie_1990_data) +
  geom_point(aes(x = Adj_Budget, y = Adj_Revenue)) +
  geom_smooth(aes(x = Adj_Budget, y = Adj_Revenue), method = "lm", se = FALSE)
```

## Statistical significance of the model {.small}

- Are these coefficients [statistically significant]{style="color:darkorange;"}?
- Can we extrapolate the results to the larger population?
- We can answer these questions by looking at the [confidence interval]{style="color:darkorange;"} (CI).

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
#| 
# We use the confint() function to get the confidence interval
confint(lm1)
```

## Statistical significance of the model {.smaller}

From the confidence interval we have that:

:::: {.columns}

::: {.column width="50%"}
- The value of the intercept is statistically different from zero since zero is not between the lower and upper values of the interval.
- The value of the slope is statistically different from zero since zero is not between the lower and upper values of the interval.

:::

::: {.column width="50%"}

- With 95% confidence, the value of the intercept at the population level is between 16.5 and 28.8 million dollars.
- With 95% confidence, the value of the slope at the population level is between 1.04 and 28.8 million dollars.

:::

::::


## Predictions {.smaller}

- Suppose we want to predict the revenue of a movie, knowing that the revenue was $25 million.
- We can use our estimated model to make the prediction.
- Input the value into the adjusted budget, resulting in:
$$
\widehat{\texttt{Adj_Revenue}} = 22.7 + 1.11\cdot 25 = 50.45
$$
- The average predicted revenue of a \$25 million dollar budget movie is \$50 million dollars.
- We can also do this using the `predict` function in [**R**]{style="color:blue;"}
```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
#| 
# We use the predict() function to get predictions
predict(lm1, list(Adj_Budget = 25))
```
## Predictions {.smaller}

- How good are these predictions?
- We can use the residual standard error (RSE), which is a result of the `summary` function.
- $\texttt{Residual standard error: 79.78}$,
- These mean that our predictions will be off by approximately 79.78.
- With 95% confidence, the prediction of the revenue will be 50.42 plus and minus $2\times 79.78 = 159.56$.
- Quite a big variation.

## Adding more variables {.smaller}

- We can add more variables in our model.
- We will add the variable `imdbRating` which encodes the different IMDB ratings in the data (1-10).
- The resulting model now is 
$$
\texttt{Adj_Revenue} = \beta_0 + \beta_1\cdot\texttt{Adj_Budget} + \beta_2\cdot\texttt{imdbRating} + e
$$
- You can observe that we have an extra slope in the equation.
- This will have an impact in how we interpret the model.
- Next we encode this model in [**R**]{style="color:blue;"}


## Adding more variables {.smaller}

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true


# The model
# Revenue = intercept + slope*Budget + slope*Rating + e
lm2 <- lm(Adj_Revenue ~ Adj_Budget + imdbRating, data = movie_1990_data)
summary(lm2)
```


## Visualizing the model {.smaller}

![](image/3d_plot_movie.png){fig-align="center"}

## Explanation {.smaller}
::: {.nonincremental}
- Let's interpret the model:
:::
$$
\texttt{Adj_Revenue} = -136.507 + 1.091 \cdot\texttt{Adj_Budget} + 24.099\cdot\texttt{imdbRating}
$$

- [**Intercept**]{style="color:darkorange;"}: By setting the movie budget and the rating to zero, we have that the average revenue is equal to [$-137 million dollars]{style="color:darkorange;"}.
- [**Slope Budget**]{style="color:darkorange;"}: For movies with the same fixed rating, for one unit change in the movie budget, there will be an increase of [$1.091 million dollars]{style="color:darkorange;"} in the movie's revenue.
- [**Slope Rating**]{style="color:darkorange;"}: For movies with the same fixed budget, for one unit change in the movie rating, there will be an increase of [$24.1 million dollars]{style="color:darkorange;"} in the movie's revenue.

## Statistical significance of the model {.small}

- Let's analyse the confidence interval

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
#| 
# We use the confint() function to get the confidence interval
confint(lm2)
```

- All coefficients are statistically significant.

## What about the predictions? {.smaller}

- We can observe from the summary that the `residual standard error` when down to $\texttt{76.43}$.
- Which will result in more accurate predictions.
- Suppose we have a movie 25 million dollar budget and the movie has a IMDb rating of 5.4 what is the prediction for this movie?
- What about the 95% prediction interval for this prediction?
```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true
#| 
# We use the confint() function to get the confidence interval
predict(lm2, list(Adj_Budget = 25, imdbRating = 5.4))
```
- What about the 95% prediction interval for this prediction?
- upper bound: $20.90542 + 2\times 76.43 = 173.7654$
- lower bound: $20.90542 - 2\times 76.43 = -131.9546$



