---
title: "Data Science for Business Applications"
author: "Class 07 - Logistic regression"
title-slide-attributes:
    data-background-image: image/background_sta235h.png
format:
  revealjs: 
    theme: "my-styles.css"
    logo: image/texas_logo2.png
    incremental: true 
    self-contained-math: true
    slide-number: true
    show-slide-number: print
    
---

## Quick review of our Class

This is what we covered in previous classes:

* [Simple and Multiple Regression]{style="color:darkorange;"}

* [Categorical Variables and Interactions]{style="color:darkorange;"} 

* [Residual Analysis]{style="color:darkorange;"}

* [Time Series]{style="color:darkorange;"} 

* [Model Selection]{style="color:darkorange;"} 

Today we will introduce a [*new model*]{style="color:darkorchid;"}.

## The OkCupid data set {.smaller}
:::: {.columns}

::: {.column width="50%"}
* The `OkCupid` data set contains information about 59826 profiles from users of the **OkCupid** online dating service.
* We have data on user [*age*]{style="color:darkorange;"}, [*height*]{style="color:darkorange;"}, [*sex*]{style="color:darkorange;"}, [*income*]{style="color:darkorange;"} , [*sexual orientation*]{style="color:darkorange;"}, *education level*]{style="color:darkorange;"}, [*body type*]{style="color:darkorange;"} , [*ethnicity*]{style="color:darkorange;"}, and more.
* Let’s see if we can predict the `sex` of the user based on their `height`. (In this data set, everyone is classified as [male]{style="color:darkorange;"} or [female]{style="color:darkorange;"}.)

:::

::: {.column width="50%"}
![](image/okcupid_pic.png){fig-align="center"}
:::

::::

```{r}
okcupid <- read.csv("script_class_07/okcupid.csv")
```


## Let's build the model

* What’s wrong with this regression?

$$
\widehat{\text{sex}} = \widehat{\beta}_{0} + \widehat{\beta}_{0} \cdot \text{height}
$$

* The $Y$ variable here is [*categorical*]{style="color:darkorchid;"} ([two levels]{style="color:darkorange;"}—everyone in this data set is either labeled `male` or `female`), so regular linear regression won’t work here.
* But what if we just [do it]{style="color:darkorange;"} anyway?

## Binary Variable

* Let’s first create a dummy variable `male` to convert sex to a quantitative dummy variable:
```{r}
#| eval: true
#| echo: true
#| output: true

library(tidyverse)
okcupid = okcupid %>% 
  mutate(male = ifelse(okcupid$sex == "m", 1, 0))
```

* We could do this with 1 representing either [male]{style="color:darkorange;"} or [female]{style="color:darkorange;"} (it wouldn’t matter).

## Regular Linear Regression {.smaller}

```{r, fig.align = 'center'}
#| eval: true
#| echo: true
#| output: true

ggplot(okcupid, aes(x=height, y = male)) + 
  geom_point() + 
  geom_smooth(method="lm", se = F)
```

* A line is not a great fit to this data—it’s not even close to linear. And what does it mean to predict that `male` = 0.7 (or 1.2)?

## Logistic Regression 

* Instead of predicting whether someone is [*male*]{style="color:darkorange;"}, let’s predict the [*probability*]{style="color:darkorange;"} that they are [*male*]{style="color:darkorange;"}

* In [*logistic regression*]{style="color:darkorange;"}, one level of $Y$ is always called [“success”]{style="color:darkorchid;"} and the other called [“failure.”]{style="color:darkorchid;"} Since $Y = 1$ for males, in our setup we have designated males as [“success.”]{style="color:darkorchid;"} (You could also set $Y = 1$ for females and call females “success.”)

* Let’s [fit a curve]{style="color:darkorange;"} that is always between 0 and 1.

## Odds and Probabilty {.smaller}

* To fit the Logistic regression model we need to know the difference between [**odds**]{style="color:darkorchid;"} and [**probability**]{style="color:darkorchid;"} and how they relate.
* When something has [“even (1/1) odds,”]{style="color:darkorange;"} the probability of success is 1/2.
* When something has [“2/1 odds,”]{style="color:darkorange;"} the probability of success is 2/3.
* When something has [“3/2 odds,”]{style="color:darkorange;"} the probability of success is 3/5.
* In general, the [**odds**]{style="color:darkorchid;"} of something happening are $p/(1 − p)$.
* Where $p$ is the [**probability**]{style="color:darkorchid;"} defined bewteen zero and one.
* You can transform odds to probability:
$$
\text{Odds} = \frac{3}{2} = \frac{3/(3+2)}{2/(3+2)} = \frac{3/5}{2/5}  = \frac{p}{1-p}
$$ 

* If the odds are between zero and one they are not in your favor, $(1-p)>p$
* Let's the explore this relation! 

## Probability vs odds vs log odds {.smaller}

| Probability $p$ | Odds $p/(1 − p)$ | Log odds $\log(p/(1 − p))$|
|:----------------:|:----------------:|:-------------------------:|
| 0               | 0                | $-\infty$                 |
| 0.25            | 0.33             | −1.10                     |
| 0.5             | 1                | 0                         |
| 0.75            | 3                | 1.10                      |
| 0.8             | 4                | 1.39                      |
| 0.9             | 9                | 2.20                      |
| 0.95            | 19               | 2.94                      |
| 1               | $\infty$         | $\infty$                  |

* Probability is between [zero and one]{style="color:darkorange;"}.
* Odds are [strictly positive]{style="color:darkorange;"} (greater than zero).
* Log odds ranges the [whole real line]{style="color:darkorange;"}.

## The logistic regression model

* [Logistic regression]{style="color:darkorange;"} models the [**log odds**]{style="color:darkorchid;"} of success $p$ as a linear function of $X$:
$$
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 \cdot X + e
$$  

* This fits an "S-shaped" curve to the data 
* We’ll see what it looks like later
* By making this choice, we have a series of benefits.
* Let’s try it!

## The logistic regression model {.smaller}

* We need a different function - `glm()` (generalized linear models)
```{r}
#| eval: true
#| echo: true
#| output: true
model <- glm(male ~ height, data = okcupid, family = binomial)
summary(model)
```

* How ca we interpret this model?

## The logistic regression model {.smaller}

* The logistic regression output tells us that our [prediction]{style="color:darkorange;"} is
$$
\log(\text{odds}) = \log\left(\frac{\widehat{p(\text{male})}}{1-\widehat{p(\text{male})}}\right) = −44.45 + 0.66 \cdot \text{height}
$$  

* To get the probability we have to solve in terms $\widehat{p(\text{male})}$
* The [probability]{style="color:darkorange;"} of being `male` given `height`:
$$
\widehat{p(\text{male})} = \frac{\exp(−44.45 + 0.66 \cdot \text{height})}{1+ \exp(−44.45 + 0.66 \cdot \text{height})}
$$
where $\exp()$ is the exponential function $e^x$.

## Let's show this {.smaller}

Let $\widehat{p} = \widehat{p(\text{male})}$, and $\exp(X\widehat{\beta}) = \exp(−44.45 + 0.66 \cdot \text{height})$:

$$
\begin{eqnarray}
\log\left(\frac{\widehat{p}}{1-\widehat{p}}\right) &=& X\widehat{\beta} \\
\exp\left(\log\left(\frac{\widehat{p}}{1-\widehat{p}}\right)\right) &=& \exp(X\widehat{\beta}) \\
\frac{\widehat{p}}{1-\widehat{p}} &=& \exp(X\widehat{\beta})\\ 
\widehat{p} &=& \exp(X\widehat{\beta})\cdot (1-\widehat{p})\\
\widehat{p} &=&\exp(X\widehat{\beta}) - \exp(X\widehat{\beta}) \cdot \widehat{p} \\
\widehat{p} &=& \frac{\exp(X\widehat{\beta})}{1 + \exp(X\widehat{\beta})}
\end{eqnarray}
$$

## Visualizing the model {.smaller}

```{r, fig.align = 'center'}
    ggplot(okcupid, aes(x=height, y=male)) +
      geom_point() +
      geom_line(aes(x = height, y = predict(model,type = "response")), color = "blue")
```
* How to interpret this curve?
* The blue line is $\widehat{p(\text{male})}$, given the `height`.

## Interpreting the coefficients {.smaller}

Our [prediction equation]{style="color:darkorange;"} is:

$$
\log(\text{odds}) = \log\left(\frac{\widehat{p(\text{male})}}{1-\widehat{p(\text{male})}}\right) = −44.45 + 0.66 \cdot \text{height}
$$  

Let’s start with some basic, but [not particularly useful]{style="color:darkorange;"}, interpretations:

* When `height` = 0, we predict that the log odds will be -44.45 , so the probability of male is predicted to be very close to 0%.

* When `height` increases by 1 inch, we predict that the log odds of being male will increase by 0.66.

* Instead of [log odds]{style="color:darkorange;"} is better to have the interpretation in [odds]{style="color:darkorange;"}.

## Interpreting the coefficients {.smaller}

* Let’s rewrite the prediction equation as:

* [Predicted odds]{style="color:darkorange;"} of male, $\exp(−44.45 + 0.66 \cdot \text{height})$.

* [Increasing height by 1]{style="color:darkorange;"} inch will multiply the odds by $\exp(0.66) = 1.94$; i.e., increase the odds by 94%.

* In summary, 
$$(\exp(\widehat{\beta}) - 1)\times 100 = \text{percentage change in odds}.$$

* [Increasing height by 2]{style="color:darkorange;"} inches will multiply the odds by $\exp(2\cdot0.66) = 3.76$; i.e., increase the odds by 276%.

* Odds equal to 1 indicate an one-to-one chance.

## Making predictions {.smaller}

* What is the [probability]{style="color:darkorange;"} of being male given we have a height of 69.

```{r}
#| eval: true
#| echo: true
#| output: true
predict(model, list(height=69), type="response")
```

* Using the probability equation in **R**:

$$
\widehat{p(\text{male})} = \frac{\exp(−44.45 + 0.66 \cdot 69)}{1+ \exp(−44.45 + 0.66 \cdot 69)} = 0.77
$$
```{r}
#| eval: true
#| echo: true
#| output: true
exp(-44.448609 + 0.661904*69)/(1+exp(-44.448609 + 0.661904*69))
```

## Adding more predictors

* Adding another predictor: [can we do better?]{style="color:darkorange;"}

* Just like with a linear regression model, we can [add additional predictors]{style="color:darkorange;"} to the model.

* Our interpretation of the coefficients in multiple logistic regression is [similar to multiple linear regression]{style="color:darkorange;"}, in the sense that each coefficient represents the predicted effect of one $X$ on $Y$, [holding the other]{style="color:darkorange;"} $X$ variables constant.

## How good is our model?

* Unfortunately, the typical root mean squared error, [RSE metric isn’t available for logistic regression]{style="color:darkorchid;"}.
* However, there are [many metrics]{style="color:darkorchid;"} that indicate model fit.
* But: most of these metrics are difficult to interpret, so we’ll focus on [something simpler]{style="color:darkorange;"} to interpret and communicate.


## Accuracy of the model {.smaller}

* We could use our model to [make a prediction]{style="color:darkorange;"} of [sex]{style="color:darkorange;"} based on the probability. 

* Suppose we say that our prediction is:
$$
\text{Prediction} = \begin{cases}
\text{male}, & \text{if $\widehat{P(\text{male})} \geq 0.5$}, \\
\text{female}, & \text{if $\widehat{P(\text{male})} < 0.5$}. \\
\end{cases}
$$

* Given a threshold of 0.5, now we can compute the [fraction of individuals]{style="color:darkorange;"} whose sex we [correctly predicted]{style="color:darkorange;"}.

* For males and females.

* This is known as the [**accuracy**]{style="color:darkorange;"} of the model.

## Accuracy of the model {.smaller}

* We can use the `xtabs` function to get the [**accuracy**]{style="color:darkorange;"}:

* We [add]{style="color:darkorange;"} we the number of [correctly predicted groups]{style="color:darkorange;"} for both male and female, and divide by the total of observations.    

```{r}
#| eval: true
#| echo: true
#| output: true
okcupid = okcupid %>% 
  mutate(predict.sex = ifelse(predict(model, type="response") >= 0.5,"m","f"))
xtabs(~ predict.sex + sex,okcupid) %>%
  addmargins()
```

* Correctly predicted that is female - 19466
* Correctly predicted that is male - 30243
* Total number of individuals in the sample - 59826
* The [**accuracy**]{style="color:darkorange;"} is (19466 + 30243)/59826 = 0.831, or 83%

## Confusion Matrix {.smaller}

* The table from `xtabs` is also called a [Confusion matrix]{style="color:darkorange;"}

|                      |Actual failure | Actual success |
|:--------------------:|:-------------:|:--------------:|
Model predicts failure |True negative  | False negative |
Model predicts success |False positive |  True positive |

* [True positives]{style="color:darkorange;"}: predicting male for someone that is male
* [True negatives]{style="color:darkorange;"}: predicting female for someone that is female 
* [False positives]{style="color:darkorange;"}: predicting male for someone that is female 
* [False negatives]{style="color:darkorange;"}: predicting female for someone that is male
* If we had designated female as 1 and male as 0, these would have switched
* So **Accuracy** = (True negative + True positive)/(Total cases) 


## Accuracy of the model {.smaller}

Suppose that the Amazon is trying to build a model to [predict which costumers buy a certain product]{style="color:darkorchid;"}:

* Suppose that 0.01% of people are costumers of this product (the product is really expensive / high revenue)

* A “null” or “no-brainer” model that predicts that [no one]{style="color:darkorange;"} is a costumer will be 99.99% accurate.

* The revenue coming from our model would be zero.

* But the model could make two different kinds of prediction errors:

* [*False positive*]{style="color:darkorange;"}: predicting someone is a customer when they really are not

* [*False negative*]{style="color:darkorange;"}: predicting someone is not a customer when they really are

* These two measures give us a better idea of the predictive power of our model.

## False positive rate {.smaller}

The false positive rate is the proportion of actual failures where the model predicted success.

```{r}
#| eval: true
#| echo: true
#| output: true
xtabs(~ predict.sex + sex, okcupid) %>%
  addmargins()
```

* [False Positives]{style="color:darkorange;"} - predicting someone is a male when they really are female
* [Actual failure]{style="color:darkorange;"} - number of cases that are female 
* [**False positive rate**]{style="color:darkorange;"} = False positives/ Actual failure
* In our model, the false positive rate is 4623/24089 = 0.19


## False negative rate {.smaller}

The false negative rate is the proportion of actual successes where the model predicted failure.

```{r}
#| eval: true
#| echo: true
#| output: true
xtabs(~ predict.sex + sex, okcupid) %>%
  addmargins()
```

* [False Negatives]{style="color:darkorange;"} - predicting someone is a female when they really are male
* [Actual success]{style="color:darkorange;"} - number of cases that are male 
* [**False negative rate**]{style="color:darkorange;"} = False negatives/ Actual success
* In our model, the [false positive rate]{style="color:darkorange;"} is 5494/35737 = 0.15


## Changing rates  {.smaller}

How do we reduce false [positive/negative]{style="color:darkorange;"} rates?

* Instead of using [50%]{style="color:darkorange;"} as a cutoff probability to decide when to predict success, use a higher (or lower) probability.
* For example, we could have the model predict that someone is male only if $\widehat{p(\text{male})} \geq 0.8$, instead of 0.5:

```{r}
#| eval: true
#| echo: true
#| output: true
okcupid = okcupid %>% 
  mutate(predict.sex = ifelse(predict(model, type="response") >= 0.8,"m","f"))
xtabs(~ predict.sex + sex,okcupid) %>%
  addmargins()
```

* [Accuracy]{style="color:darkorange;"} = (21425+26753)/59826 = 0.76
* [False positive rate]{style="color:darkorange;"} = 2664/24089 = 0.06
* [False negative rate]{style="color:darkorange;"} = 8984/59826 = 0.36


## Prediction Trade off

* We can [decrease]{style="color:darkorange;"} the [false positive rate]{style="color:darkorange;"}, but at the expense of [increasing]{style="color:darkorange;"} the [false negative rate]{style="color:darkorange;"}.

* Or we can [decrease the false negative rate]{style="color:darkorange;"}, but at the expense of increasing the false positive rate.

* We might choose a [cutoff probability other than 50%]{style="color:darkorange;"} based on our assessment of the relative costs of the two different kinds of errors.

## Summary

In a [logistic regression model]{style="color:darkorange;"}, the response variable is binary, taking values of either zero or one.

* The model estimates the [log odds]{style="color:darkorange;"} of the event associated with a response of one.

* The model's effects are interpreted in terms of [odds]{style="color:darkorange;"}.

* Predictions are expressed as [probabilities]{style="color:darkorange;"}.

* The performance of the model is evaluated based on its prediction [accuracy]{style="color:darkorange;"}.






